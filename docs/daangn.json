[
  {
    "creator": "당근",
    "title": "루틴 속 익숙해진 불편함, AI로 새롭게 해결하기까지",
    "link": "https://medium.com/daangn/%EB%A3%A8%ED%8B%B4-%EC%86%8D-%EC%9D%B5%EC%88%99%ED%95%B4%EC%A7%84-%EB%B6%88%ED%8E%B8%ED%95%A8-ai%EB%A1%9C-%EC%83%88%EB%A1%AD%EA%B2%8C-%ED%95%B4%EA%B2%B0%ED%95%98%EA%B8%B0%EA%B9%8C%EC%A7%80-a50d9fa04fd4?source=rss----4505f82a2dbd---4",
    "pubDate": "Mon, 19 May 2025 06:15:20 GMT",
    "content:encoded": "<h3>루틴 속 익숙해진 불편함, AI로 새롭게 해결하기까지 — 당근 AI Show &amp; Tell #4</h3><blockquote>당근은 매주 ‘AI Show &amp; Tell’을 통해 각 팀의 AI 실험을 전사적으로 공유해요. AI를 업무에 어떻게 적용하고 있는지, 그 과정에서 어떤 시행착오와 인사이트가 있었는지 가감 없이 나누죠. 당근은 완벽한 정답을 찾기보다 먼저 과감하게 실행하며, 새로운 시대의 문제 해결 방식을 빠르게 찾아가고 있어요. AI로 만드는 생생한 도전의 순간들, 지금 만나보세요.</blockquote><blockquote>✍️ 이 콘텐츠는 생성형 AI를 활용해 제작한 콘텐츠입니다.</blockquote><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*GuKo9AXXEaVqtWwP9KQgKQ.png\" /><figcaption>해당 이미지는 OpenAI의 이미지 생성 모델인 DALL·E를 활용하여 GPT-4o에서 생성되었습니다.</figcaption></figure><p>계정과 인증을 담당하는 아이덴티티 서비스팀은 최근 AI로 일하는 방식을 새롭게 바꿔냈어요. 이 팀은 플랫폼 조직이라는 특성상, AI를 서비스 전반에 적용할 만한 아이디어를 찾기가 쉽지 않았는데요. 팀원들은 “일단 작게라도 시도하면서 방향을 찾아보자”는 데 뜻을 모았어요. 그렇게 4일간의 해커톤을 기획해, <strong>평소 업무에서 느껴왔던 문제를 각자 AI로 해결해 보기로 했죠.</strong></p><p>짧은 시간 동안 몰입한 결과, 팀원들은 각자의 자리에서 눈에 띄는 변화를 만들었어요. 반복적인 루틴을 간소화하고, 인수인계 과정의 병목을 없애고, 복잡한 기술 이슈도 더 직관적으로 처리할 수 있도록 개선했죠. 지금도 그 결과물들은 실무에 유용하게 쓰이고 있고, 사용성도 계속 고도화되는 중이에요. <strong>익숙한 업무 루틴 속에서 문제를 발견하고 AI로 일의 방식을 바꿔낸 아이덴티티 서비스팀의 사례들을 하나씩 소개해드릴게요.</strong></p><h3>Project 1. PM의 아침 루틴을 30분에서 3분으로, ‘당근 지표 리포터’</h3><p>PM인 Audrey는 매일 아침 사내 지표 플랫폼에서 국가별 가입자 수와 가입 전환율을 확인해요. 수치가 떨어진 항목을 발견하면, 관련 지표를 하나하나 비교하며 원인을 추적했죠. 반복적이고 시간 소모가 큰 아침 루틴을 바꿔보기 위해, Audrey는 <strong>중요한 지표 변화만 자동으로 정리해 주는 슬랙봇 ‘당근 지표 리포터’를 직접 만들어보기로 결심했어요.</strong></p><p>해커톤 첫날부터 Audrey는 바쁘게 움직였어요. 자동화 구조를 익히기 위해, 유튜브 영상 요약 슬랙봇을 먼저 실습 삼아 구현했죠. 이후 n8n을 활용해 실제 당근 지표 리포터를 만들기 시작했어요. 중간중간 막히는 부분이 생길 때마다 유튜브나 검색을 통해 스스로 해결하며 끝까지 밀어붙였어요. 누구도 대신 정리해주지 않는 문제를 혼자 정의하고, 스스로 풀어낸 몰입의 시간이었죠.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*dIuFFoOr6OmU9dVyH5lyWw.png\" /></figure><p>완성된 슬랙봇은 매일 아침 핵심 지표 변화와 특이사항을 자동으로 정리해주고 있어요. Audrey는 이 경험을 통해 일하는 방식을 새롭게 바라보게 됐어요. <strong>생각보다 많은 업무가 일정한 흐름과 패턴을 따르고 있고, 그 구조만 파악하면 대부분 자동화할 수 있겠다는 감이 생긴 거예요. </strong>해커톤 이후 Audrey는 프로세스가 명확한 작업들을 하나씩 자동화하는 계획을 직접 실행에 옮기고 있어요.</p><h3>Project 2. 그동안 쌓인 온콜 이슈를 한눈에, ‘Oncall Summary Bot’</h3><p>아이덴티티 서비스팀에서는 온콜 담당자가 바뀔 때마다, 슬랙에서 지난 이슈를 일일이 다시 찾아봐야 했어요. 해결되지 않은 이슈가 있으면, 새 담당자는 수많은 스레드를 뒤지며 상황을 파악해야 했죠. 시간도 오래 걸릴 뿐 아니라, 중요한 내용을 놓치기 쉬웠죠. Mandy는 AI로 이 과정을 자동화해 보기로 했어요. 반복 작업을 줄이려다 오히려 리소스를 더 많이 쓰는 상황은 피하고 싶었기 때문에, 최대한 가볍고 효율적으로 만드는 게 목표였어요. 그래서 Mandy는 코딩 없이 워크플로우를 자동화할 수 있는 n8n을 활용해 요약봇을 만들었어요.</p><p>그 과정이 쉽지만은 않았어요. 초반엔 결과 포맷이 매번 달라지는 멱등성 문제에 부딪혔죠. Mandy는 최소한의 리소스로 가볍게 구현하려는 목표에 맞춰, <strong>파라미터 조정 없이 정교한 프롬프팅만으로 해결하려 했어요.</strong> 그런데 프롬프트가 길어지며 일부 요청이 누락되는 문제가 생기자, 에이전트를 두 개로 분리했어요. 하나는 자연어 입력을 슬랙 검색 쿼리로 바꾸고, 다른 하나는 해당 쿼리로 메시지를 검색해 요약했죠. <strong>각 에이전트에 입력되는 프롬프트가 간결해지니, 최종 아웃풋도 일관되게 나왔어요.</strong></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ifKVKnoKzXCVFRIjt9vbEA.png\" /></figure><p>지금 이 요약봇은 팀의 온콜 인수인계 과정에 유용하게 쓰이고 있어요. <strong>복잡한 대화 흐름을 한눈에 파악할 수 있게 정리해, 업무 연속성과 효율을 높이고 있죠. </strong>Mandy는 이번 경험을 통해 적은 리소스로도 실용적인 도구를 만들 수 있겠다는 자신감을 얻었어요. 이를 위해선 <strong>구현 초반부터 멱등성을 관리하는 게 핵심이라는 인사이트도 발견했고요.</strong> Mandy는 이번 실행을 계기로 앞으로도 안정적인 자동화 툴을 더 효율적인 방식으로 만들어갈 예정이라고 해요.</p><h3>Project 3. 에러를 일일이 살피던 밤은 이제 끝, ‘에러 박사’</h3><p>아이덴티티 서비스팀은 서버 에러가 발생하면 슬랙에 알림이 오도록 설정했어요. 실시간 대응에는 유용했지만, 긴급하지 않은 이슈에도 민감하게 반응할 수밖에 없었죠. 또 미국, 캐나다, 영국 등 시차가 다른 지역에서도 서비스가 운영되다 보니, 온콜 담당자는 24시간 내내 긴장을 놓을 수 없었고요. <strong>가장 큰 문제는 온콜 담당자가 모든 이슈의 맥락을 알 수는 없다는 점이었어요.</strong> 스택 트레이스를 봐도 그 맥락을 구체적으로 모르면, 다른 팀원들까지 새벽에 함께 대응해야 하거나 에러 해결이 늦어지곤 했죠.</p><p><strong>Brave는 AI가 코드 맥락을 파악해 원인과 해결책까지 제시해 준다면, 부담을 크게 줄일 수 있을 거라 생각했어요.</strong> 그래서 MCP* 도구의 데이터 소스로 Sentry와 GitHub를 연동해 보기로 했죠. 우선 에러 알림이 오면, n8n 기반의 워크플로우가 자동으로 시작돼요. 몇 가지 분기를 거쳐 Sentry에서 에러 이벤트를 가져온 뒤, 메시지에 포함된 코드 위치를 기준으로 GitHub에서 관련 코드를 조회하죠. 이후 LLM이 코드를 분석해 추정 원인과 해결책을 도출하고, 슬랙 알림 댓글에 요약을 남겨요. 처음엔 맥락을 놓치거나 환각이 발생하기도 했지만, 분석 단계를 멀티턴으로 나눠 안정성을 높였어요.</p><blockquote><em>*</em>MCP: AI 모델이 외부 시스템이나 데이터 소스와 연결되어 명령을 실행할 수 있도록 설계된 프로토콜</blockquote><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*AFrj0XuCo8p-Z7uOsjLhJw.png\" /></figure><p><strong>덕분에 팀원들은 모든 코드를 일일이 확인하지 않아도 이슈의 긴급도를 판단할 수 있게 됐어요.</strong> 지금 당장 대응해야 할 문제인지 미리 파악할 수 있으니, 새벽 대응의 피로도가 확실히 줄었어요. 코드 파악에 쓰던 시간을 줄여 전체적인 대응 속도도 빨라졌고요. 이번 경험을 통해 Brave는 AI가 단순한 자동화를 넘어, 업무 맥락을 이해하고 문제 해결까지 도울 수 있겠다는 가능성을 실감했어요. 앞으로도 <strong>CS 대응, 이상행동 탐지 등 더 다양한 업무에 MCP를 확장할 계획이에요.</strong></p><p>이번 세 가지 프로젝트는 <strong>각자의 일상에서 문제를 발견하고, 일하는 방식을 스스로 재설계한 사례였어요.</strong> 구성원들은 이 시도들을 통해 앞으로 AI를 어떻게 활용할지도 더 구체적으로 그려볼 수 있었죠.</p><p>이런 변화는 아이덴티티 서비스팀만의 이야기가 아니에요. 당근 곳곳에서도 자발적으로 시작된 실행들이 또 다른 실행으로 자연스럽게 확장되고 있죠. 이런 환경 속에서 당근이 앞으로 또 어떤 새로운 가능성을 열어갈지 궁금하다면, 다음 이야기도 기대해 주세요.</p><blockquote>AI로 일의 방식을 바꾸는 도전, 함께하고 싶다면?<br>👉 <a href=\"https://bit.ly/4msZx8a\"><strong>당근 채용 공고 바로 가기</strong></a></blockquote><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=a50d9fa04fd4\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/daangn/%EB%A3%A8%ED%8B%B4-%EC%86%8D-%EC%9D%B5%EC%88%99%ED%95%B4%EC%A7%84-%EB%B6%88%ED%8E%B8%ED%95%A8-ai%EB%A1%9C-%EC%83%88%EB%A1%AD%EA%B2%8C-%ED%95%B4%EA%B2%B0%ED%95%98%EA%B8%B0%EA%B9%8C%EC%A7%80-a50d9fa04fd4\">루틴 속 익숙해진 불편함, AI로 새롭게 해결하기까지</a> was originally published in <a href=\"https://medium.com/daangn\">당근 테크 블로그</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "content:encodedSnippet": "루틴 속 익숙해진 불편함, AI로 새롭게 해결하기까지 — 당근 AI Show & Tell #4\n당근은 매주 ‘AI Show & Tell’을 통해 각 팀의 AI 실험을 전사적으로 공유해요. AI를 업무에 어떻게 적용하고 있는지, 그 과정에서 어떤 시행착오와 인사이트가 있었는지 가감 없이 나누죠. 당근은 완벽한 정답을 찾기보다 먼저 과감하게 실행하며, 새로운 시대의 문제 해결 방식을 빠르게 찾아가고 있어요. AI로 만드는 생생한 도전의 순간들, 지금 만나보세요.\n✍️ 이 콘텐츠는 생성형 AI를 활용해 제작한 콘텐츠입니다.\n해당 이미지는 OpenAI의 이미지 생성 모델인 DALL·E를 활용하여 GPT-4o에서 생성되었습니다.\n계정과 인증을 담당하는 아이덴티티 서비스팀은 최근 AI로 일하는 방식을 새롭게 바꿔냈어요. 이 팀은 플랫폼 조직이라는 특성상, AI를 서비스 전반에 적용할 만한 아이디어를 찾기가 쉽지 않았는데요. 팀원들은 “일단 작게라도 시도하면서 방향을 찾아보자”는 데 뜻을 모았어요. 그렇게 4일간의 해커톤을 기획해, 평소 업무에서 느껴왔던 문제를 각자 AI로 해결해 보기로 했죠.\n짧은 시간 동안 몰입한 결과, 팀원들은 각자의 자리에서 눈에 띄는 변화를 만들었어요. 반복적인 루틴을 간소화하고, 인수인계 과정의 병목을 없애고, 복잡한 기술 이슈도 더 직관적으로 처리할 수 있도록 개선했죠. 지금도 그 결과물들은 실무에 유용하게 쓰이고 있고, 사용성도 계속 고도화되는 중이에요. 익숙한 업무 루틴 속에서 문제를 발견하고 AI로 일의 방식을 바꿔낸 아이덴티티 서비스팀의 사례들을 하나씩 소개해드릴게요.\nProject 1. PM의 아침 루틴을 30분에서 3분으로, ‘당근 지표 리포터’\nPM인 Audrey는 매일 아침 사내 지표 플랫폼에서 국가별 가입자 수와 가입 전환율을 확인해요. 수치가 떨어진 항목을 발견하면, 관련 지표를 하나하나 비교하며 원인을 추적했죠. 반복적이고 시간 소모가 큰 아침 루틴을 바꿔보기 위해, Audrey는 중요한 지표 변화만 자동으로 정리해 주는 슬랙봇 ‘당근 지표 리포터’를 직접 만들어보기로 결심했어요.\n해커톤 첫날부터 Audrey는 바쁘게 움직였어요. 자동화 구조를 익히기 위해, 유튜브 영상 요약 슬랙봇을 먼저 실습 삼아 구현했죠. 이후 n8n을 활용해 실제 당근 지표 리포터를 만들기 시작했어요. 중간중간 막히는 부분이 생길 때마다 유튜브나 검색을 통해 스스로 해결하며 끝까지 밀어붙였어요. 누구도 대신 정리해주지 않는 문제를 혼자 정의하고, 스스로 풀어낸 몰입의 시간이었죠.\n\n완성된 슬랙봇은 매일 아침 핵심 지표 변화와 특이사항을 자동으로 정리해주고 있어요. Audrey는 이 경험을 통해 일하는 방식을 새롭게 바라보게 됐어요. 생각보다 많은 업무가 일정한 흐름과 패턴을 따르고 있고, 그 구조만 파악하면 대부분 자동화할 수 있겠다는 감이 생긴 거예요. 해커톤 이후 Audrey는 프로세스가 명확한 작업들을 하나씩 자동화하는 계획을 직접 실행에 옮기고 있어요.\nProject 2. 그동안 쌓인 온콜 이슈를 한눈에, ‘Oncall Summary Bot’\n아이덴티티 서비스팀에서는 온콜 담당자가 바뀔 때마다, 슬랙에서 지난 이슈를 일일이 다시 찾아봐야 했어요. 해결되지 않은 이슈가 있으면, 새 담당자는 수많은 스레드를 뒤지며 상황을 파악해야 했죠. 시간도 오래 걸릴 뿐 아니라, 중요한 내용을 놓치기 쉬웠죠. Mandy는 AI로 이 과정을 자동화해 보기로 했어요. 반복 작업을 줄이려다 오히려 리소스를 더 많이 쓰는 상황은 피하고 싶었기 때문에, 최대한 가볍고 효율적으로 만드는 게 목표였어요. 그래서 Mandy는 코딩 없이 워크플로우를 자동화할 수 있는 n8n을 활용해 요약봇을 만들었어요.\n그 과정이 쉽지만은 않았어요. 초반엔 결과 포맷이 매번 달라지는 멱등성 문제에 부딪혔죠. Mandy는 최소한의 리소스로 가볍게 구현하려는 목표에 맞춰, 파라미터 조정 없이 정교한 프롬프팅만으로 해결하려 했어요. 그런데 프롬프트가 길어지며 일부 요청이 누락되는 문제가 생기자, 에이전트를 두 개로 분리했어요. 하나는 자연어 입력을 슬랙 검색 쿼리로 바꾸고, 다른 하나는 해당 쿼리로 메시지를 검색해 요약했죠. 각 에이전트에 입력되는 프롬프트가 간결해지니, 최종 아웃풋도 일관되게 나왔어요.\n\n지금 이 요약봇은 팀의 온콜 인수인계 과정에 유용하게 쓰이고 있어요. 복잡한 대화 흐름을 한눈에 파악할 수 있게 정리해, 업무 연속성과 효율을 높이고 있죠. Mandy는 이번 경험을 통해 적은 리소스로도 실용적인 도구를 만들 수 있겠다는 자신감을 얻었어요. 이를 위해선 구현 초반부터 멱등성을 관리하는 게 핵심이라는 인사이트도 발견했고요. Mandy는 이번 실행을 계기로 앞으로도 안정적인 자동화 툴을 더 효율적인 방식으로 만들어갈 예정이라고 해요.\nProject 3. 에러를 일일이 살피던 밤은 이제 끝, ‘에러 박사’\n아이덴티티 서비스팀은 서버 에러가 발생하면 슬랙에 알림이 오도록 설정했어요. 실시간 대응에는 유용했지만, 긴급하지 않은 이슈에도 민감하게 반응할 수밖에 없었죠. 또 미국, 캐나다, 영국 등 시차가 다른 지역에서도 서비스가 운영되다 보니, 온콜 담당자는 24시간 내내 긴장을 놓을 수 없었고요. 가장 큰 문제는 온콜 담당자가 모든 이슈의 맥락을 알 수는 없다는 점이었어요. 스택 트레이스를 봐도 그 맥락을 구체적으로 모르면, 다른 팀원들까지 새벽에 함께 대응해야 하거나 에러 해결이 늦어지곤 했죠.\nBrave는 AI가 코드 맥락을 파악해 원인과 해결책까지 제시해 준다면, 부담을 크게 줄일 수 있을 거라 생각했어요. 그래서 MCP* 도구의 데이터 소스로 Sentry와 GitHub를 연동해 보기로 했죠. 우선 에러 알림이 오면, n8n 기반의 워크플로우가 자동으로 시작돼요. 몇 가지 분기를 거쳐 Sentry에서 에러 이벤트를 가져온 뒤, 메시지에 포함된 코드 위치를 기준으로 GitHub에서 관련 코드를 조회하죠. 이후 LLM이 코드를 분석해 추정 원인과 해결책을 도출하고, 슬랙 알림 댓글에 요약을 남겨요. 처음엔 맥락을 놓치거나 환각이 발생하기도 했지만, 분석 단계를 멀티턴으로 나눠 안정성을 높였어요.\n*MCP: AI 모델이 외부 시스템이나 데이터 소스와 연결되어 명령을 실행할 수 있도록 설계된 프로토콜\n\n덕분에 팀원들은 모든 코드를 일일이 확인하지 않아도 이슈의 긴급도를 판단할 수 있게 됐어요. 지금 당장 대응해야 할 문제인지 미리 파악할 수 있으니, 새벽 대응의 피로도가 확실히 줄었어요. 코드 파악에 쓰던 시간을 줄여 전체적인 대응 속도도 빨라졌고요. 이번 경험을 통해 Brave는 AI가 단순한 자동화를 넘어, 업무 맥락을 이해하고 문제 해결까지 도울 수 있겠다는 가능성을 실감했어요. 앞으로도 CS 대응, 이상행동 탐지 등 더 다양한 업무에 MCP를 확장할 계획이에요.\n이번 세 가지 프로젝트는 각자의 일상에서 문제를 발견하고, 일하는 방식을 스스로 재설계한 사례였어요. 구성원들은 이 시도들을 통해 앞으로 AI를 어떻게 활용할지도 더 구체적으로 그려볼 수 있었죠.\n이런 변화는 아이덴티티 서비스팀만의 이야기가 아니에요. 당근 곳곳에서도 자발적으로 시작된 실행들이 또 다른 실행으로 자연스럽게 확장되고 있죠. 이런 환경 속에서 당근이 앞으로 또 어떤 새로운 가능성을 열어갈지 궁금하다면, 다음 이야기도 기대해 주세요.\nAI로 일의 방식을 바꾸는 도전, 함께하고 싶다면?\n👉 당근 채용 공고 바로 가기\n\n루틴 속 익숙해진 불편함, AI로 새롭게 해결하기까지 was originally published in 당근 테크 블로그 on Medium, where people are continuing the conversation by highlighting and responding to this story.",
    "dc:creator": "당근",
    "guid": "https://medium.com/p/a50d9fa04fd4",
    "categories": [
      "automation",
      "ai",
      "ai-tools",
      "hackathons"
    ],
    "isoDate": "2025-05-19T06:15:20.000Z"
  },
  {
    "creator": "당근",
    "title": "지금의 방식이 최선일까? AI로 임팩트를 바꾸는 당근 운영실",
    "link": "https://medium.com/daangn/%EC%A7%80%EA%B8%88%EC%9D%98-%EB%B0%A9%EC%8B%9D%EC%9D%B4-%EC%B5%9C%EC%84%A0%EC%9D%BC%EA%B9%8C-ai%EB%A1%9C-%EC%9E%84%ED%8C%A9%ED%8A%B8%EB%A5%BC-%EB%B0%94%EA%BE%B8%EB%8A%94-%EB%8B%B9%EA%B7%BC-%EC%9A%B4%EC%98%81%EC%8B%A4-289e6c1ba987?source=rss----4505f82a2dbd---4",
    "pubDate": "Tue, 22 Apr 2025 07:01:01 GMT",
    "content:encoded": "<h3>지금의 방식이 최선일까? AI로 임팩트를 바꾸는 당근 운영실 — 당근 AI Show &amp; Tell #3</h3><blockquote>당근은 매주 ‘AI Show &amp; Tell’을 통해 각 팀의 AI 실험을 전사적으로 공유해요. AI를 업무에 어떻게 적용하고 있는지, 그 과정에서 어떤 시행착오와 인사이트가 있었는지 가감 없이 나누죠. 당근은 완벽한 정답을 찾기보다 먼저 과감하게 실행하며, 새로운 시대의 문제 해결 방식을 빠르게 찾아가고 있어요. AI로 만드는 생생한 도전의 순간들, 지금 만나보세요.</blockquote><blockquote>✍️ 이 콘텐츠는 생성형 AI를 활용해 제작한 콘텐츠입니다.</blockquote><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Ql_7q-QpGHVY1Urg7T9tlg.png\" /><figcaption>서비스 운영실 팀 내부 세션 ‘Nextstep for Service Operation’ 발표 자료</figcaption></figure><p>당근 운영실은 최근 AI를 활용해 실행과 학습 속도를 높일 수 있도록 조직 구조와 일하는 방식을 재설계했어요. 그 구체적인 변화는 🔗‘<a href=\"https://medium.com/daangn/%EB%AA%A8%EB%91%90%EA%B0%80-ai-%EB%A1%9C%EC%BC%93%EC%97%90-%EC%98%AC%EB%9D%BC%ED%83%80%EB%8F%84%EB%A1%9D-%EB%8B%B9%EA%B7%BC-%EC%9A%B4%EC%98%81%EC%8B%A4%EC%9D%B4-ai%EB%A1%9C-%EC%9D%BC%ED%95%98%EB%8A%94-%EB%B2%95-b8aaa6713cea\"><strong>모두가 AI 로켓에 올라타도록, 당근 운영실이 AI로 일하는 법</strong></a>’에서 소개해 드렸는데요. 이번에는 ‘당근 AI Show &amp; Tell’ 세션에서 발표된 내용을 바탕으로, 그 전환이 어떤 구체적인 결과물로 이어졌는지 전해드리려 해요.</p><p><strong>이번에 소개할 세 가지 프로젝트는 기존의 문제 해결 방식을 AI를 통해 새롭게 바라본 시도들이에요.</strong> 지금의 방식이 정말 최선인지 근본부터 다시 점검했고, 더 효과적인 해결 방법을 새롭게 기획했죠. 이후 “AI를 활용하면 빠르게 구현할 수 있다”는 확신 아래, 과감하게 실행에 나섰어요. 수개월간 준비하던 프로젝트를 AI 기반 프로덕트로 전면 피봇한 사례부터, 반복 업무를 줄이기 위해 비개발자가 직접 AI 툴을 만들어낸 실험까지 — 그 치열한 몰입의 과정을 지금부터 하나씩 살펴볼게요.</p><h3>Project 1. 누구나 쉽게 조립하는 멀티 AI 에이전트 시스템, ‘KAMP’</h3><p>AI 전환이 본격화되던 시기, 운영개발팀은 여러 차례 논의와 얼라인을 거쳐 팀원 모두가 멀티 AI 에이전트 시스템을 염두에 두고 있다는 점을 확인했어요. 이는 서로 다른 역할의 AI들이 협업해 복잡한 문제를 효율적으로 처리하는 구조인데요. 제재 사유 설명이나 운영 정책 안내처럼 다양한 데이터를 다뤄야 하는 CS 업무에서는 AI 에이전트의 역할을 분리하는 게 응답 정확도와 처리 속도 모두에 유리했어요. 운영개발팀은 서비스별로 필요한 CS 대응 시스템을 누구나 쉽게 구현할 수 있도록, 개발 없이도 에이전트를 조합할 수 있는 플랫폼을 기획했어요.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*UH6a8tPNDfCzqNxa210CLg.png\" /></figure><p>이렇게 탄생한 것이 운영개발팀 Backend Engineer Julius가 개발한 <strong>KAMP(Karrot Agent Management Platform)</strong>예요. KAMP는<strong> ‘프로젝트–에이전트–도구’</strong>라는 세 가지 요소로 구성돼 있어요. 프로젝트는 해결하려는 문제, 에이전트는 LLM 기반으로 역할을 수행하는 AI, 도구는 외부 정보를 조회하거나 특정 행동을 할 수 있도록 해주는 API를 의미하죠.</p><p><strong>이 세 가지를 상황에 맞게 조합하기만 하면, 복잡한 응대 시나리오도 코드 없이 손쉽게 구현할 수 있어요.</strong> 예를 들어 기본적인 사용자 질문에 응대하는 ‘고객지원 상담사’ 에이전트에게는 사용자 정보를 조회할 수 있는 도구를 연결하고, ‘운영정책 전문가’ 에이전트에는 내부 정책 검색 도구를 연결하는 식이에요. 여기에 ‘답변 검토 에이전트’를 추가해 다른 에이전트에 의해 작성된 답변이 사용자의 질문에 충분한 답변이 되는지 검토할 수도 있어요. 이렇게 에이전트는 각자 전문 분야 작업을 수행하고 서로 협업하여 복잡한 사용자의 문제를 해결해요.</p><p>사실 Julius는 한 달 전까지만 해도 룰 베이스 기반의 고객 지원 시스템을 설계 중이었고, 기획과 기본적인 시스템 구조 설계까지 마친 상태였어요. 하지만 AI 전환이 본격화되면서 그 방식이 사용자에게 가장 큰 임팩트를 줄 수 있을지 의문이 생겼고, 고민 끝에 멀티 AI 에이전트 시스템으로 방향을 전환했죠. <strong>이미 많은 설계가 진행된 상황에서 결정을 뒤집는 건 쉽지 않았지만, 오로지 사용자 경험에 집중하며 4일 만에 코어 로직과 UI를 완성했어요.</strong> Julius는 “LLM의 한계를 의심하기보다 빠르게 시도하는 것이 중요하다”는 인사이트를 남겼어요.</p><h3>Project 2. CS 데이터를 가지고 놀 수 있도록, ‘VoC Playground’</h3><p>문의, 신고, 설문, 전화 상담 등 다양한 채널에서 매일 피드백이 들어오지만, 프로덕트 팀이 이 데이터를 직접 활용하기는 어려웠어요. 각 채널별로 데이터는 서로 다른 구조의 DB에 흩어져 있었고, 결국 운영팀이 대신 쿼리를 짜주거나 수작업으로 정리해 전달하는 일이 반복됐어요. 실시간 대응이 중요한 상황에서, 분석 속도는 느려지고 데이터 활용도는 낮아졌어요. 운영개발팀 Backend Engineer Willie는 쌓여만 가는 CS 데이터를 보며, 이 문제를 AI로 해결해 볼 수 있지 않을까 고민했어요.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*guJx5obqZGKzpsViL59xqA.png\" /></figure><p>그 고민의 결과로 누구나 쉽게 CS 데이터를 탐색하고 분석할 수 있는 플랫폼인 <strong>VoC Playground</strong>가 탄생했어요. VoC는 Voice of Customer의 약자로, 사용자가 우리에게 보내는 모든 의견을 뜻하는데요. VoC Playground는 사용자가 보낸 의견 텍스트를 효율적으로 분석할 수 있도록 도와주는 관리 도구예요. <strong>분석하려는 VoC 유형, 정확도 임계값, 필터링 조건 등만 설정하면, AI가 그에 맞춰 데이터를 정리하죠.</strong> 예를 들어 클릭 몇 번과 짧은 프롬프트만으로 ‘의견 남기기’ CS 데이터 중 ‘기능 개선 및 제안’ 관련 데이터만 추출할 수 있어요.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*mQ048lW8NAZgJG68f1SA3A.png\" /></figure><p>이후 필터링된 데이터를 다양한 방식으로 분석해 볼 수 있어요. <strong>원하는 LLM 모델을 선택하고 분석 방향을 프롬프트로 입력하면, AI가 리포트를 자동으로 생성해요. </strong>예를 들어 앞서 추출한 ‘기능 개선 및 제안’ 관련 데이터를 기반으로 서비스별 개선 사항과 액션 아이템을 정리할 수 있어요. <strong>복잡한 쿼리 없이도 누구나 구조화된 인사이트를 도출할 수 있게 된 거예요.</strong> VoC Playground는 앞으로도 VoC 감정 분석, 핵심 키워드 도출, 텍스트 요약 등의 기능이 추가되며 계속 고도화될 예정이에요.</p><p><strong>VoC Playground는 운영개발팀이 반년 넘게 준비하던 VoC 파이프라인을 과감히 접고, 단 일주일 만에 피봇해 만든 결과물이에요.</strong> 처음엔 완성도 높은 분석 기능 구현을 목표로 했지만, AI 전환의 흐름 속에서 빠르고 손쉽게 써볼 수 있는 도구가 더 시급하다는 판단을 내렸죠. 클러스터링, 키워드 추출 등 기존 설계를 폐기하고, 프롬프트 기반 분석 구조로 방향을 바꿨어요. 완성도보다 속도가 중요한 순간, 치열하게 결정하고 빠르게 만들어낸 전환이었어요.</p><h3>Project 3. 수작업 6시간을 3분으로, ‘앱 리뷰 자동 라벨링 툴’</h3><p>운영팀의 Operations Manager인 Sofia는 앱 리뷰 데이터를 직접 관리하며, 매월 리포트 형태로 구성원들에게 공유해 왔어요. 이때 필요한 시각화 자료를 만들기 위해, 매일 수기 라벨링 작업을 병행하고 있었는데요. 앱 리뷰는 텍스트가 정돈돼 있지 않아 단순 키워드 매칭이 어려웠어요. 하나하나 맥락을 판단해 라벨을 붙여야만 했죠. 이 반복 작업은 매월 6시간 이상 소요됐는데, 이 업무를 하는 동안 다른 기획 업무를 병행하기에도 어려웠어요.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*23KjOr-vUP7VAjz_2BH9kA.png\" /></figure><p>문제를 해결하기 위해 Sofia는 Cursor를 활용해 <strong>자동 라벨링 툴</strong>을 완성했어요. 라벨링된 예시 데이터와 라벨링할 대상 데이터를 임베딩하면, 예시 데이터와의 유사도 분석을 통해 앱 리뷰 데이터의 ‘관련 서비스’, ‘유형’, ‘세부 유형’을 자동으로 라벨링해요. <strong>수작업으로 6시간 걸리던 업무는 3분으로 줄었고, 검수 시간을 포함하더라도 1시간 이내로 마칠 수 있게 됐어요.</strong> AI를 실무에 깊이 적용하며, 일하는 방식을 효율적으로 바꾼 거죠.</p><p><strong>특히 놀라운 점은 개발 경험이 거의 없던 Sofia가 LLM과의 대화를 통해 기능을 고도화했다는 사실이에요. </strong>첫 결과물은 키워드 기반으로 맵핑하는 수준이었는데요. “맥락 기준으로 분류할 방법은 없을까?”라는 Sofia의 질문에 Cursor는 OpenAI API 활용을 제안했어요. 그 결과물의 완성도도 부족해서 “예시 데이터를 학습해 줄 수 있을까?”라는 추가 요청을 하게 됐고, 끝내 유사도 기반 임베딩 방식을 도입하여 분류 정확도를 크게 개선할 수 있었어요. <strong>기술력은 부족하더라도 문제를 더 나은 방식으로 해결하고자 한 집요함이 실질적인 결과로 이어진 거예요.</strong></p><p>당근 운영실의 AI 전환은 단순한 기술 도입이 아니라, 실행을 중심에 둔 치열한 전환이었어요. 팀은 문제를 더 잘 풀 수 있다면 수개월간 준비하던 계획도 과감히 접었고, 누구든 자기 자리에서 실무에 적용할 수 있는 도구를 직접 만들며 실행의 밀도를 끌어올렸죠. <strong>개발자와 비개발자 모두가 ‘어떻게든 해내겠다’는 태도로 일에 몰입했고, 그 몰입이 하나둘 실질적인 변화로 이어졌어요.</strong></p><p>이제 이 몰입은 운영실을 넘어 조직 곳곳으로 번지고 있어요. 다음 AI Show &amp; Tell에서는 다른 팀들이 AI를 일의 방식으로 어떻게 받아들이고 있는지 전해드릴게요. 더 나은 문제 해결을 위해 매일 이어지는 치열한 몰입이 조직에 어떤 변화를 만들어내고 있는지 궁금하다면, 그다음 이야기도 기대해 주세요.</p><blockquote>AI로 함께 새로운 변화를 만들어내고 싶다면? <br>👉 <a href=\"https://bit.ly/3RZWwhm\"><strong>당근 채용 공고 바로 가기</strong></a></blockquote><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=289e6c1ba987\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/daangn/%EC%A7%80%EA%B8%88%EC%9D%98-%EB%B0%A9%EC%8B%9D%EC%9D%B4-%EC%B5%9C%EC%84%A0%EC%9D%BC%EA%B9%8C-ai%EB%A1%9C-%EC%9E%84%ED%8C%A9%ED%8A%B8%EB%A5%BC-%EB%B0%94%EA%BE%B8%EB%8A%94-%EB%8B%B9%EA%B7%BC-%EC%9A%B4%EC%98%81%EC%8B%A4-289e6c1ba987\">지금의 방식이 최선일까? AI로 임팩트를 바꾸는 당근 운영실</a> was originally published in <a href=\"https://medium.com/daangn\">당근 테크 블로그</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "content:encodedSnippet": "지금의 방식이 최선일까? AI로 임팩트를 바꾸는 당근 운영실 — 당근 AI Show & Tell #3\n당근은 매주 ‘AI Show & Tell’을 통해 각 팀의 AI 실험을 전사적으로 공유해요. AI를 업무에 어떻게 적용하고 있는지, 그 과정에서 어떤 시행착오와 인사이트가 있었는지 가감 없이 나누죠. 당근은 완벽한 정답을 찾기보다 먼저 과감하게 실행하며, 새로운 시대의 문제 해결 방식을 빠르게 찾아가고 있어요. AI로 만드는 생생한 도전의 순간들, 지금 만나보세요.\n✍️ 이 콘텐츠는 생성형 AI를 활용해 제작한 콘텐츠입니다.\n서비스 운영실 팀 내부 세션 ‘Nextstep for Service Operation’ 발표 자료\n당근 운영실은 최근 AI를 활용해 실행과 학습 속도를 높일 수 있도록 조직 구조와 일하는 방식을 재설계했어요. 그 구체적인 변화는 🔗‘모두가 AI 로켓에 올라타도록, 당근 운영실이 AI로 일하는 법’에서 소개해 드렸는데요. 이번에는 ‘당근 AI Show & Tell’ 세션에서 발표된 내용을 바탕으로, 그 전환이 어떤 구체적인 결과물로 이어졌는지 전해드리려 해요.\n이번에 소개할 세 가지 프로젝트는 기존의 문제 해결 방식을 AI를 통해 새롭게 바라본 시도들이에요. 지금의 방식이 정말 최선인지 근본부터 다시 점검했고, 더 효과적인 해결 방법을 새롭게 기획했죠. 이후 “AI를 활용하면 빠르게 구현할 수 있다”는 확신 아래, 과감하게 실행에 나섰어요. 수개월간 준비하던 프로젝트를 AI 기반 프로덕트로 전면 피봇한 사례부터, 반복 업무를 줄이기 위해 비개발자가 직접 AI 툴을 만들어낸 실험까지 — 그 치열한 몰입의 과정을 지금부터 하나씩 살펴볼게요.\nProject 1. 누구나 쉽게 조립하는 멀티 AI 에이전트 시스템, ‘KAMP’\nAI 전환이 본격화되던 시기, 운영개발팀은 여러 차례 논의와 얼라인을 거쳐 팀원 모두가 멀티 AI 에이전트 시스템을 염두에 두고 있다는 점을 확인했어요. 이는 서로 다른 역할의 AI들이 협업해 복잡한 문제를 효율적으로 처리하는 구조인데요. 제재 사유 설명이나 운영 정책 안내처럼 다양한 데이터를 다뤄야 하는 CS 업무에서는 AI 에이전트의 역할을 분리하는 게 응답 정확도와 처리 속도 모두에 유리했어요. 운영개발팀은 서비스별로 필요한 CS 대응 시스템을 누구나 쉽게 구현할 수 있도록, 개발 없이도 에이전트를 조합할 수 있는 플랫폼을 기획했어요.\n\n이렇게 탄생한 것이 운영개발팀 Backend Engineer Julius가 개발한 KAMP(Karrot Agent Management Platform)예요. KAMP는 ‘프로젝트–에이전트–도구’라는 세 가지 요소로 구성돼 있어요. 프로젝트는 해결하려는 문제, 에이전트는 LLM 기반으로 역할을 수행하는 AI, 도구는 외부 정보를 조회하거나 특정 행동을 할 수 있도록 해주는 API를 의미하죠.\n이 세 가지를 상황에 맞게 조합하기만 하면, 복잡한 응대 시나리오도 코드 없이 손쉽게 구현할 수 있어요. 예를 들어 기본적인 사용자 질문에 응대하는 ‘고객지원 상담사’ 에이전트에게는 사용자 정보를 조회할 수 있는 도구를 연결하고, ‘운영정책 전문가’ 에이전트에는 내부 정책 검색 도구를 연결하는 식이에요. 여기에 ‘답변 검토 에이전트’를 추가해 다른 에이전트에 의해 작성된 답변이 사용자의 질문에 충분한 답변이 되는지 검토할 수도 있어요. 이렇게 에이전트는 각자 전문 분야 작업을 수행하고 서로 협업하여 복잡한 사용자의 문제를 해결해요.\n사실 Julius는 한 달 전까지만 해도 룰 베이스 기반의 고객 지원 시스템을 설계 중이었고, 기획과 기본적인 시스템 구조 설계까지 마친 상태였어요. 하지만 AI 전환이 본격화되면서 그 방식이 사용자에게 가장 큰 임팩트를 줄 수 있을지 의문이 생겼고, 고민 끝에 멀티 AI 에이전트 시스템으로 방향을 전환했죠. 이미 많은 설계가 진행된 상황에서 결정을 뒤집는 건 쉽지 않았지만, 오로지 사용자 경험에 집중하며 4일 만에 코어 로직과 UI를 완성했어요. Julius는 “LLM의 한계를 의심하기보다 빠르게 시도하는 것이 중요하다”는 인사이트를 남겼어요.\nProject 2. CS 데이터를 가지고 놀 수 있도록, ‘VoC Playground’\n문의, 신고, 설문, 전화 상담 등 다양한 채널에서 매일 피드백이 들어오지만, 프로덕트 팀이 이 데이터를 직접 활용하기는 어려웠어요. 각 채널별로 데이터는 서로 다른 구조의 DB에 흩어져 있었고, 결국 운영팀이 대신 쿼리를 짜주거나 수작업으로 정리해 전달하는 일이 반복됐어요. 실시간 대응이 중요한 상황에서, 분석 속도는 느려지고 데이터 활용도는 낮아졌어요. 운영개발팀 Backend Engineer Willie는 쌓여만 가는 CS 데이터를 보며, 이 문제를 AI로 해결해 볼 수 있지 않을까 고민했어요.\n\n그 고민의 결과로 누구나 쉽게 CS 데이터를 탐색하고 분석할 수 있는 플랫폼인 VoC Playground가 탄생했어요. VoC는 Voice of Customer의 약자로, 사용자가 우리에게 보내는 모든 의견을 뜻하는데요. VoC Playground는 사용자가 보낸 의견 텍스트를 효율적으로 분석할 수 있도록 도와주는 관리 도구예요. 분석하려는 VoC 유형, 정확도 임계값, 필터링 조건 등만 설정하면, AI가 그에 맞춰 데이터를 정리하죠. 예를 들어 클릭 몇 번과 짧은 프롬프트만으로 ‘의견 남기기’ CS 데이터 중 ‘기능 개선 및 제안’ 관련 데이터만 추출할 수 있어요.\n\n이후 필터링된 데이터를 다양한 방식으로 분석해 볼 수 있어요. 원하는 LLM 모델을 선택하고 분석 방향을 프롬프트로 입력하면, AI가 리포트를 자동으로 생성해요. 예를 들어 앞서 추출한 ‘기능 개선 및 제안’ 관련 데이터를 기반으로 서비스별 개선 사항과 액션 아이템을 정리할 수 있어요. 복잡한 쿼리 없이도 누구나 구조화된 인사이트를 도출할 수 있게 된 거예요. VoC Playground는 앞으로도 VoC 감정 분석, 핵심 키워드 도출, 텍스트 요약 등의 기능이 추가되며 계속 고도화될 예정이에요.\nVoC Playground는 운영개발팀이 반년 넘게 준비하던 VoC 파이프라인을 과감히 접고, 단 일주일 만에 피봇해 만든 결과물이에요. 처음엔 완성도 높은 분석 기능 구현을 목표로 했지만, AI 전환의 흐름 속에서 빠르고 손쉽게 써볼 수 있는 도구가 더 시급하다는 판단을 내렸죠. 클러스터링, 키워드 추출 등 기존 설계를 폐기하고, 프롬프트 기반 분석 구조로 방향을 바꿨어요. 완성도보다 속도가 중요한 순간, 치열하게 결정하고 빠르게 만들어낸 전환이었어요.\nProject 3. 수작업 6시간을 3분으로, ‘앱 리뷰 자동 라벨링 툴’\n운영팀의 Operations Manager인 Sofia는 앱 리뷰 데이터를 직접 관리하며, 매월 리포트 형태로 구성원들에게 공유해 왔어요. 이때 필요한 시각화 자료를 만들기 위해, 매일 수기 라벨링 작업을 병행하고 있었는데요. 앱 리뷰는 텍스트가 정돈돼 있지 않아 단순 키워드 매칭이 어려웠어요. 하나하나 맥락을 판단해 라벨을 붙여야만 했죠. 이 반복 작업은 매월 6시간 이상 소요됐는데, 이 업무를 하는 동안 다른 기획 업무를 병행하기에도 어려웠어요.\n\n문제를 해결하기 위해 Sofia는 Cursor를 활용해 자동 라벨링 툴을 완성했어요. 라벨링된 예시 데이터와 라벨링할 대상 데이터를 임베딩하면, 예시 데이터와의 유사도 분석을 통해 앱 리뷰 데이터의 ‘관련 서비스’, ‘유형’, ‘세부 유형’을 자동으로 라벨링해요. 수작업으로 6시간 걸리던 업무는 3분으로 줄었고, 검수 시간을 포함하더라도 1시간 이내로 마칠 수 있게 됐어요. AI를 실무에 깊이 적용하며, 일하는 방식을 효율적으로 바꾼 거죠.\n특히 놀라운 점은 개발 경험이 거의 없던 Sofia가 LLM과의 대화를 통해 기능을 고도화했다는 사실이에요. 첫 결과물은 키워드 기반으로 맵핑하는 수준이었는데요. “맥락 기준으로 분류할 방법은 없을까?”라는 Sofia의 질문에 Cursor는 OpenAI API 활용을 제안했어요. 그 결과물의 완성도도 부족해서 “예시 데이터를 학습해 줄 수 있을까?”라는 추가 요청을 하게 됐고, 끝내 유사도 기반 임베딩 방식을 도입하여 분류 정확도를 크게 개선할 수 있었어요. 기술력은 부족하더라도 문제를 더 나은 방식으로 해결하고자 한 집요함이 실질적인 결과로 이어진 거예요.\n당근 운영실의 AI 전환은 단순한 기술 도입이 아니라, 실행을 중심에 둔 치열한 전환이었어요. 팀은 문제를 더 잘 풀 수 있다면 수개월간 준비하던 계획도 과감히 접었고, 누구든 자기 자리에서 실무에 적용할 수 있는 도구를 직접 만들며 실행의 밀도를 끌어올렸죠. 개발자와 비개발자 모두가 ‘어떻게든 해내겠다’는 태도로 일에 몰입했고, 그 몰입이 하나둘 실질적인 변화로 이어졌어요.\n이제 이 몰입은 운영실을 넘어 조직 곳곳으로 번지고 있어요. 다음 AI Show & Tell에서는 다른 팀들이 AI를 일의 방식으로 어떻게 받아들이고 있는지 전해드릴게요. 더 나은 문제 해결을 위해 매일 이어지는 치열한 몰입이 조직에 어떤 변화를 만들어내고 있는지 궁금하다면, 그다음 이야기도 기대해 주세요.\nAI로 함께 새로운 변화를 만들어내고 싶다면? \n👉 당근 채용 공고 바로 가기\n\n지금의 방식이 최선일까? AI로 임팩트를 바꾸는 당근 운영실 was originally published in 당근 테크 블로그 on Medium, where people are continuing the conversation by highlighting and responding to this story.",
    "dc:creator": "당근",
    "guid": "https://medium.com/p/289e6c1ba987",
    "categories": [
      "ai-agent",
      "ai",
      "ai-tools",
      "automation"
    ],
    "isoDate": "2025-04-22T07:01:01.000Z"
  },
  {
    "creator": "Elon park",
    "title": "Cursor와 TDD로 만드는 Swift Macro",
    "link": "https://medium.com/daangn/cursor%EC%99%80-tdd%EB%A1%9C-%EB%A7%8C%EB%93%9C%EB%8A%94-swift-macro-0e4a245caee2?source=rss----4505f82a2dbd---4",
    "pubDate": "Thu, 17 Apr 2025 06:13:12 GMT",
    "content:encoded": "<p>안녕하세요. 모바일실 iOS팀에서 iOS Engineer로 일하고 있는 Elon이에요.</p><p>제가 속한 모바일실은 당근에 전사적으로 필요한 기능을 개발해요. CI/CD, 애널리틱스, 실험 플랫폼, 딥링크 시스템 등을 직접 개발 및 관리하며, 앱 개발에 필요한 플랫폼 엔지니어링을 담당하고 있어요. 이러한 플랫폼 엔지니어링에는 iOS 엔지니어분들의 개발 생산성을 높이기 위한 Swift Macro를 개발하는 것도 포함되는데요.</p><p>이 글에서는 Curosr와 함께 TDD를 통해 Swift Macro를 구현해 보면서, 실제 프로덕션에 적용할 수 있는 신뢰도 높은 코드를 작성하는 방법에 대해 이야기하려고 해요.</p><h3>TDD에 들어가기에 앞서</h3><p>당근 앱의 토대를 쌓아 올리셨던 iOS 엔지니어분들은 모두 당근 초기부터 TDD(Test Driven Development)를 진행했어요. 그래서 단순 View를 제외하면 현재까지도 iOS 당근 앱에선 테스트 코드가 없는 곳을 찾아보기 어렵죠. 기존 개발 환경이 이러하다 보니 iOS 챕터에 합류한 엔지니어분들도 모두 테스트 코드를 작성하는 걸 당연하게 생각해요. 테스트 코드가 없으면 Pull Reqeust를 올리지 않을 정도예요. 전 당근에 입사하기 전까지는 iOS 앱 개발에서는 테스트 코드를 작성해 본 경험이 없었는데요. 그랬던 저도 지금은 테스트 코드를 수월하게 작성해요. Swift Macro와 같이 처음 접하는 기술을 도입할 때도 자연스럽게 TDD로 시작해보고 있죠.</p><p>Swift Macro는 Swift로 작성된 코드를 SwiftSyntax로 파싱한 후, Swift Syntax Tree에서 원하는 코드를 탐색해 가져와 사용하는 방식이에요. 그래서 매크로를 만들기 위해서는 먼저 원하는 형태의 매크로 인터페이스와 매크로 적용 후 생성될 코드 형태를 미리 설계해야 하는데요. 테스트 작성 시 일반적으로 사용하는 패턴인 <em>Given-When-Then</em> 패턴에서 매크로 인터페이스는 <em>Given,</em> 매크로가 적용되어 생성된 코드는 <em>Then</em>에 해당돼요. 따라서 자연스럽게 테스트코드를 먼저 작성하고 구현을 개발하는 TDD 방식에 적합하다고 볼 수 있어요.</p><p>TDD는 아래와 같은 세 단계를 하나의 사이클로 반복하면서 기능을 완성해 나가요. Swift Macro의 경우, 일반적으로 iOS 앱 개발에서 접할 일이 거의 없는 SwiftSyntax API를 사용해야 해요. 그래서 SwiftSyntax 레퍼런스 문서와 Swift AST Explorer 사이트를 오가며 필요한 Syntax를 찾는데 많은 시간을 소비하게 되는데, 이 과정을 Cursor와 함께 한다면 많은 시간을 절약할 수 있어요.</p><ul><li>🔴 Red: 원하는 기능의 테스트 코드를 작성하고 테스트를 실행하여 실패하는 것을 확인해요.</li><li>🟢 Green: 테스트를 통과할 수 있도록 최대한 빠르게 동작할 수 있는 코드를 구현해요.</li><li>🟡 Refactor: 빠르게 작성한 코드에서 중복된 코드 등을 제거하고 유지보수하기 쉽도록 가독성 좋게 리팩토링해요.</li></ul><p>이번 글에서는 예시로 Swift에서 JSON 파싱을 위해 Codable의 CodingKeys 를 자동으로 추가해 주는 Swift 매크로를 만들어 볼게요. 코드 작성은 Cursor에서 진행하고, 컴파일과 테스트 코드 실행은 Xcode를 사용해요.</p><p>💡 아래 예시는 Cursor를 사용하여 TDD를 하는 것을 목적으로 하기 때문에 Swift Macro 패키지를 만들기 위한 모든 내용을 다루지 않아요.</p><ul><li>전체 코드들은 <a href=\"https://github.com/ElonPark/Make-SwiftMacro-using-Cousor-Example\">예제 레포지토리</a>에서 보실 수 있어요.</li></ul><h3>🔴 Red</h3><ol><li>먼저 매크로의 인터페이스를 설계하여 사용될 예시 코드를 정의해요.</li></ol><pre>@CustomCodable<br>struct Person {<br>  let name: String<br>  @CodableKey(&quot;user_age&quot;) let age: Int<br>}</pre><p>2. 실제 매크로가 적용되었을 때 추가될 코드의 모습을 정의해요.</p><pre>struct Person {<br>  let name: String<br>  let age: Int<br><br>  enum CodingKeys: String, CodingKey {<br>    case name<br>    case age = &quot;user_age&quot;<br>  }<br>}</pre><p>3. 해당 코드들을 매크로 패키지에 테스트코드로 정의해요.</p><pre>func testExpansionWithCodableKeyAddsCustomCodingKeys() {<br>  assertMacroExpansion(<br>    &quot;&quot;&quot;<br>    @CustomCodable<br>    struct Person {<br>      let name: String<br>      @CodableKey(&quot;user_age&quot;) let age: Int<br>    }<br>    &quot;&quot;&quot;,<br>    expandedSource: &quot;&quot;&quot;<br>      struct Person {<br>        let name: String<br>        @CodableKey(&quot;user_age&quot;) let age: Int<br><br>        enum CodingKeys: String, CodingKey {<br>          case name<br>          case age = &quot;user_age&quot;<br>        }<br>      }<br>      &quot;&quot;&quot;,<br>    macros: macros,<br>    indentationWidth: .spaces(2)<br>  )<br>}</pre><p>4. 테스트를 실행하여 테스트에 통과하지 못하고 실패하는 것을 확인해요.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Je28RFdhR2h61aOX2-Ae3g.png\" /></figure><h3>🟢 Green</h3><p>테스트에 실패한 것을 확인했으니 동작하는 코드를 구현할 차례예요.</p><p>1. 작성한 테스트 코드를 포함한 프롬프트를 Cursor Composer를 통해 LLM에게 전달하여 실제 구현 코드를 생성해요.</p><blockquote>Tip: SwiftSyntax 버전에 따라 사용할 수 있는 API가 달라요. 따라서 프롬프트 Swift 버전을 명시하여 Swift 버전에 맞는 SwiftSyntax 코드를 생성할 수 있도록 해요.</blockquote><pre>아래와 같은 테스트 코드를 통과하는 Swift Macro를 작성해 줘<br>Swift 버전은 5.10을 사용하고 swift-syntax에 맞게 구현하여야 해<br>Tuple이 필요하다면 struct 또는 enum을 새로 정의하여 사용해 줘<br><br>```swift<br>/* 작성한 테스트 코드 */<br>```</pre><p>2. 생성된 코드를 리뷰하고 문제가 있는 부분을 수정 요청해요. 코드를 커밋해도 될 만큼 코드가 개선되었다고 판단되면, 해당 코드들을 수락하여 패키지에 반영해요.</p><p>3. Xcode에서 패키지를 열고 테스트를 실행해요. LLM이 생성한 코드가 한 번에 테스트를 통과하면 좋겠지만, 대부분은 컴파일 단계에서 실패하는 코드가 생성돼요.</p><ul><li>여기서는 컴파일러 경고와 에러가 하나씩 발생했네요.</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*71T5a3VQtOb5AXlkBiH_KQ.png\" /></figure><ul><li>컴파일러 경고 메시지는 Fix 버튼을 눌러서 간단하게 수정 가능하기 때문에 직접 수정해요.</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*7-Rse83urN4pswD5qqSiGQ.jpeg\" /></figure><p>4. 컴파일러 에러를 수정할 차례예요. 테스트가 통과할 수 있도록 에러를 수정해 달라고 요청해볼게요.</p><pre>다음과 같은 에러와 경고가 발생하였어 테스트가 통과할 수 있게 수정해 줘<br>- 에러 메시지: `CustomCodableMacros/CustomCodableMacro.swift:39:15 Initializer for conditional binding must have Optional type, not &#39;AttributeListSyntax&#39;`</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*CRP8pY6Tetc5EeOyNwUesw.jpeg\" /><figcaption>Cursor에서 실행한 결과</figcaption></figure><p>5. 다시 Xcode로 돌아와서 테스트 코드를 실행해 봅니다. 이번에는 다행히 컴파일에 성공할 수 있도록 코드를 수정해 주었네요. 하지만 아쉽게도 아직 테스트는 통과하지 못했어요.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*3H2fr4WKNF7wLsKdYmmR9g.jpeg\" /></figure><p>6. 다시 한번 에러 메시지를 전달하여 수정을 요청하여 볼게요.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*O0RuJRA_thBWyx5ok46KrA.jpeg\" /></figure><p>7. 수정된 코드를 리뷰하여 문제가 있다면 다시 수정 요청하기를 반복해요. 수정된 코드들에 문제가 없다고 판단되면 적용하고, 다시 Xcode로 돌아와 테스트를 실행해 봅니다. 저는 이번에도 테스트가 실패했는데요.</p><p>오류 메시지를 확인해 보니 생성된 코드와 테스트에서 기대하는 코드의 인덴트가 다르기 때문에 테스트에 통과하지 못했네요.</p><blockquote>Tip: Swift Macro는 매크로가 생성한 코드와 테스트의 결괏값이 정확히 일치하는 경우에만 성공하기 때문에, 문법 상에 문제가 없는 경우에도 테스트에 실패할 수 있어요.</blockquote><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*aWHkHSnnBWG3DH9wwjZvHA.jpeg\" /></figure><p>8. 단순히 인덴트만 수정하면 되기 때문에 이번에는 직접 수정해요.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/896/1*0uqU-NqZ3hIXBG2s49AJBw.jpeg\" /></figure><p>9. 다시 테스트를 실행시켜 테스트에 통과하는 것을 확인해요.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*JxBPjJ8NFEBKix0OciNFaQ.jpeg\" /></figure><h3>🟡 Refactor</h3><p>테스트에 통과하는 것을 확인했으니 이제 리팩토링을 진행할 차례예요.</p><blockquote>Tip: 리팩토링 요청 시에는 원하는 코드 스타일이나 메소드 추출 기법 등 리팩토링 방법들을 프롬프트에 구체적으로 명시하면 더욱 좋은 결과물을 얻을 수 있어요.</blockquote><ol><li>리팩토링 또한 LLM에게 요청하여 진행해 봅니다 😉</li></ol><pre>이제 @CustomCodableMacro.swift를 가독성 좋고 이해기 쉬운 단위로 메소드로 추출하여<br>리팩토링해줘 필요하다면 파일을 여러 개로 분리해도 괜찮아 하지만 기존 테스트는 통과할 수 있도록<br>구현에는 문제가 없어야 해</pre><ul><li>Green 단계에서 진행한 것과 동일하게 수정된 코드를 리뷰해 보고 문제가 없다면 적용해요. 문제가 있다면 해당 내용을 지적하여 다시 리팩토링을 요청해요.</li></ul><p>2. 이제 리팩토링된 코드가 문제가 없는지 다시 한번 테스트를 실행해 봅니다.</p><ul><li>리팩토링 과정에서 이전과 동일하게 인덴트가 스페이스 4칸으로 변경되어 테스트가 실패했네요.</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*MrtNB4tr8N6jDB30YIWSoA.jpeg\" /></figure><p>3. 이번에도 단순 인덴트 차이가 원인이기 때문에 직접 수정해요.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*13m1ECHJEo2uG9EZx8HDzA.jpeg\" /></figure><p>4. 다시 테스트를 실행하여 테스트가 통과하는 것을 확인해요.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*3rYyaWL7IfZz-kXArvBG-w.jpeg\" /></figure><p>5. 필요에 따라 다시 리팩토링을 진행하거나 작업을 마무리해요.</p><blockquote>Tip: LLM은 Context 크기에 제한이 있기 때문에 한 번에 여러 작업을 진행하기보다는 하나씩 작업을 진행하고, 많은 양의 코드를 LLM에 전달하여야 하는 큰 작업이 있다면 여러 개의 작은 단위로 쪼개서 작업하는 것이 좋아요.</blockquote><h3>TDD에 LLM을 활용한다면?</h3><p>이것으로 TDD의 3가지 스텝을 차례대로 진행해 보면서 TDD의 한 사이클을 돌아보았는데요. 이 과정에서 LLM을 사용하여 TDD를 진행했을 때의 두 가지 장점을 찾을 수 있었어요.</p><p>첫 번째 장점은 많은 시간을 절약할 수 있다는 점이에요. Swift Macro를 만들기 위해서는 Green 단계에서 Swift Syntax Tree를 탐색하고 필요한 Syntax를 가져오는 데 시간을 소모해야 하는데요. 이 작업을 LLM이 대신 수행하므로 많은 시간을 절약할 수 있었어요.</p><p>두 번째 장점은 코드의 문제점을 빠르게 파악하고 수정할 수 있다는 점이에요. 테스트를 먼저 작성한 후, 테스트를 통해 LLM이 생성한 코드를 검증하기 때문인데요. 테스트가 통과한 이후 리팩토링을 진행할 때에도 테스트를 통해 문제를 감지하고 바르게 수정할 수 있어요.</p><p>다만, 주의점은 LLM을 활용해 TDD를 하더라도, LLM이 생성한 코드를 리뷰하고 오류를 수정하는 것은 엔지니어의 역할이에요. 따라서 SwiftSyntax와 같이 구현에 사용되는 기술에 대한 지식을 반드시 갖추고 진행하여야 해요.</p><h3>나가며</h3><p>ChatGPT 등장 이후 LLM 기반의 생성형 AI가 빠르게 발전해 나가면서 세상을 바꾸고 있는데요. 당근에서는 LLM을 활용하여 사용자 경험이나 동료의 생산성을 높이는 도구를 함께 만들어 갈 iOS 엔지니어를 찾고 있어요. 저희의 여정에 함께하고 싶으시다면 아래 공고를 통해 지원하실 수 있어요! 😃</p><p><a href=\"https://team.daangn.com/jobs/5282170003/\">Software Engineer, iOS | 당근 팀 채용</a></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=0e4a245caee2\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/daangn/cursor%EC%99%80-tdd%EB%A1%9C-%EB%A7%8C%EB%93%9C%EB%8A%94-swift-macro-0e4a245caee2\">Cursor와 TDD로 만드는 Swift Macro</a> was originally published in <a href=\"https://medium.com/daangn\">당근 테크 블로그</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "content:encodedSnippet": "안녕하세요. 모바일실 iOS팀에서 iOS Engineer로 일하고 있는 Elon이에요.\n제가 속한 모바일실은 당근에 전사적으로 필요한 기능을 개발해요. CI/CD, 애널리틱스, 실험 플랫폼, 딥링크 시스템 등을 직접 개발 및 관리하며, 앱 개발에 필요한 플랫폼 엔지니어링을 담당하고 있어요. 이러한 플랫폼 엔지니어링에는 iOS 엔지니어분들의 개발 생산성을 높이기 위한 Swift Macro를 개발하는 것도 포함되는데요.\n이 글에서는 Curosr와 함께 TDD를 통해 Swift Macro를 구현해 보면서, 실제 프로덕션에 적용할 수 있는 신뢰도 높은 코드를 작성하는 방법에 대해 이야기하려고 해요.\nTDD에 들어가기에 앞서\n당근 앱의 토대를 쌓아 올리셨던 iOS 엔지니어분들은 모두 당근 초기부터 TDD(Test Driven Development)를 진행했어요. 그래서 단순 View를 제외하면 현재까지도 iOS 당근 앱에선 테스트 코드가 없는 곳을 찾아보기 어렵죠. 기존 개발 환경이 이러하다 보니 iOS 챕터에 합류한 엔지니어분들도 모두 테스트 코드를 작성하는 걸 당연하게 생각해요. 테스트 코드가 없으면 Pull Reqeust를 올리지 않을 정도예요. 전 당근에 입사하기 전까지는 iOS 앱 개발에서는 테스트 코드를 작성해 본 경험이 없었는데요. 그랬던 저도 지금은 테스트 코드를 수월하게 작성해요. Swift Macro와 같이 처음 접하는 기술을 도입할 때도 자연스럽게 TDD로 시작해보고 있죠.\nSwift Macro는 Swift로 작성된 코드를 SwiftSyntax로 파싱한 후, Swift Syntax Tree에서 원하는 코드를 탐색해 가져와 사용하는 방식이에요. 그래서 매크로를 만들기 위해서는 먼저 원하는 형태의 매크로 인터페이스와 매크로 적용 후 생성될 코드 형태를 미리 설계해야 하는데요. 테스트 작성 시 일반적으로 사용하는 패턴인 Given-When-Then 패턴에서 매크로 인터페이스는 Given, 매크로가 적용되어 생성된 코드는 Then에 해당돼요. 따라서 자연스럽게 테스트코드를 먼저 작성하고 구현을 개발하는 TDD 방식에 적합하다고 볼 수 있어요.\nTDD는 아래와 같은 세 단계를 하나의 사이클로 반복하면서 기능을 완성해 나가요. Swift Macro의 경우, 일반적으로 iOS 앱 개발에서 접할 일이 거의 없는 SwiftSyntax API를 사용해야 해요. 그래서 SwiftSyntax 레퍼런스 문서와 Swift AST Explorer 사이트를 오가며 필요한 Syntax를 찾는데 많은 시간을 소비하게 되는데, 이 과정을 Cursor와 함께 한다면 많은 시간을 절약할 수 있어요.\n\n🔴 Red: 원하는 기능의 테스트 코드를 작성하고 테스트를 실행하여 실패하는 것을 확인해요.\n🟢 Green: 테스트를 통과할 수 있도록 최대한 빠르게 동작할 수 있는 코드를 구현해요.\n🟡 Refactor: 빠르게 작성한 코드에서 중복된 코드 등을 제거하고 유지보수하기 쉽도록 가독성 좋게 리팩토링해요.\n\n이번 글에서는 예시로 Swift에서 JSON 파싱을 위해 Codable의 CodingKeys 를 자동으로 추가해 주는 Swift 매크로를 만들어 볼게요. 코드 작성은 Cursor에서 진행하고, 컴파일과 테스트 코드 실행은 Xcode를 사용해요.\n💡 아래 예시는 Cursor를 사용하여 TDD를 하는 것을 목적으로 하기 때문에 Swift Macro 패키지를 만들기 위한 모든 내용을 다루지 않아요.\n\n전체 코드들은 예제 레포지토리에서 보실 수 있어요.\n\n🔴 Red\n\n먼저 매크로의 인터페이스를 설계하여 사용될 예시 코드를 정의해요.\n\n@CustomCodable\nstruct Person {\n  let name: String\n  @CodableKey(\"user_age\") let age: Int\n}\n2. 실제 매크로가 적용되었을 때 추가될 코드의 모습을 정의해요.\nstruct Person {\n  let name: String\n  let age: Int\n  enum CodingKeys: String, CodingKey {\n    case name\n    case age = \"user_age\"\n  }\n}\n3. 해당 코드들을 매크로 패키지에 테스트코드로 정의해요.\nfunc testExpansionWithCodableKeyAddsCustomCodingKeys() {\n  assertMacroExpansion(\n    \"\"\"\n    @CustomCodable\n    struct Person {\n      let name: String\n      @CodableKey(\"user_age\") let age: Int\n    }\n    \"\"\",\n    expandedSource: \"\"\"\n      struct Person {\n        let name: String\n        @CodableKey(\"user_age\") let age: Int\n        enum CodingKeys: String, CodingKey {\n          case name\n          case age = \"user_age\"\n        }\n      }\n      \"\"\",\n    macros: macros,\n    indentationWidth: .spaces(2)\n  )\n}\n4. 테스트를 실행하여 테스트에 통과하지 못하고 실패하는 것을 확인해요.\n\n🟢 Green\n테스트에 실패한 것을 확인했으니 동작하는 코드를 구현할 차례예요.\n1. 작성한 테스트 코드를 포함한 프롬프트를 Cursor Composer를 통해 LLM에게 전달하여 실제 구현 코드를 생성해요.\nTip: SwiftSyntax 버전에 따라 사용할 수 있는 API가 달라요. 따라서 프롬프트 Swift 버전을 명시하여 Swift 버전에 맞는 SwiftSyntax 코드를 생성할 수 있도록 해요.\n아래와 같은 테스트 코드를 통과하는 Swift Macro를 작성해 줘\nSwift 버전은 5.10을 사용하고 swift-syntax에 맞게 구현하여야 해\nTuple이 필요하다면 struct 또는 enum을 새로 정의하여 사용해 줘\n```swift\n/* 작성한 테스트 코드 */\n```\n2. 생성된 코드를 리뷰하고 문제가 있는 부분을 수정 요청해요. 코드를 커밋해도 될 만큼 코드가 개선되었다고 판단되면, 해당 코드들을 수락하여 패키지에 반영해요.\n3. Xcode에서 패키지를 열고 테스트를 실행해요. LLM이 생성한 코드가 한 번에 테스트를 통과하면 좋겠지만, 대부분은 컴파일 단계에서 실패하는 코드가 생성돼요.\n\n여기서는 컴파일러 경고와 에러가 하나씩 발생했네요.\n\n컴파일러 경고 메시지는 Fix 버튼을 눌러서 간단하게 수정 가능하기 때문에 직접 수정해요.\n\n4. 컴파일러 에러를 수정할 차례예요. 테스트가 통과할 수 있도록 에러를 수정해 달라고 요청해볼게요.\n다음과 같은 에러와 경고가 발생하였어 테스트가 통과할 수 있게 수정해 줘\n- 에러 메시지: `CustomCodableMacros/CustomCodableMacro.swift:39:15 Initializer for conditional binding must have Optional type, not 'AttributeListSyntax'`\nCursor에서 실행한 결과\n5. 다시 Xcode로 돌아와서 테스트 코드를 실행해 봅니다. 이번에는 다행히 컴파일에 성공할 수 있도록 코드를 수정해 주었네요. 하지만 아쉽게도 아직 테스트는 통과하지 못했어요.\n\n6. 다시 한번 에러 메시지를 전달하여 수정을 요청하여 볼게요.\n\n7. 수정된 코드를 리뷰하여 문제가 있다면 다시 수정 요청하기를 반복해요. 수정된 코드들에 문제가 없다고 판단되면 적용하고, 다시 Xcode로 돌아와 테스트를 실행해 봅니다. 저는 이번에도 테스트가 실패했는데요.\n오류 메시지를 확인해 보니 생성된 코드와 테스트에서 기대하는 코드의 인덴트가 다르기 때문에 테스트에 통과하지 못했네요.\nTip: Swift Macro는 매크로가 생성한 코드와 테스트의 결괏값이 정확히 일치하는 경우에만 성공하기 때문에, 문법 상에 문제가 없는 경우에도 테스트에 실패할 수 있어요.\n\n8. 단순히 인덴트만 수정하면 되기 때문에 이번에는 직접 수정해요.\n\n9. 다시 테스트를 실행시켜 테스트에 통과하는 것을 확인해요.\n\n🟡 Refactor\n테스트에 통과하는 것을 확인했으니 이제 리팩토링을 진행할 차례예요.\nTip: 리팩토링 요청 시에는 원하는 코드 스타일이나 메소드 추출 기법 등 리팩토링 방법들을 프롬프트에 구체적으로 명시하면 더욱 좋은 결과물을 얻을 수 있어요.\n\n리팩토링 또한 LLM에게 요청하여 진행해 봅니다 😉\n\n이제 @CustomCodableMacro.swift를 가독성 좋고 이해기 쉬운 단위로 메소드로 추출하여\n리팩토링해줘 필요하다면 파일을 여러 개로 분리해도 괜찮아 하지만 기존 테스트는 통과할 수 있도록\n구현에는 문제가 없어야 해\n\nGreen 단계에서 진행한 것과 동일하게 수정된 코드를 리뷰해 보고 문제가 없다면 적용해요. 문제가 있다면 해당 내용을 지적하여 다시 리팩토링을 요청해요.\n\n2. 이제 리팩토링된 코드가 문제가 없는지 다시 한번 테스트를 실행해 봅니다.\n\n리팩토링 과정에서 이전과 동일하게 인덴트가 스페이스 4칸으로 변경되어 테스트가 실패했네요.\n\n3. 이번에도 단순 인덴트 차이가 원인이기 때문에 직접 수정해요.\n\n4. 다시 테스트를 실행하여 테스트가 통과하는 것을 확인해요.\n\n5. 필요에 따라 다시 리팩토링을 진행하거나 작업을 마무리해요.\nTip: LLM은 Context 크기에 제한이 있기 때문에 한 번에 여러 작업을 진행하기보다는 하나씩 작업을 진행하고, 많은 양의 코드를 LLM에 전달하여야 하는 큰 작업이 있다면 여러 개의 작은 단위로 쪼개서 작업하는 것이 좋아요.\nTDD에 LLM을 활용한다면?\n이것으로 TDD의 3가지 스텝을 차례대로 진행해 보면서 TDD의 한 사이클을 돌아보았는데요. 이 과정에서 LLM을 사용하여 TDD를 진행했을 때의 두 가지 장점을 찾을 수 있었어요.\n첫 번째 장점은 많은 시간을 절약할 수 있다는 점이에요. Swift Macro를 만들기 위해서는 Green 단계에서 Swift Syntax Tree를 탐색하고 필요한 Syntax를 가져오는 데 시간을 소모해야 하는데요. 이 작업을 LLM이 대신 수행하므로 많은 시간을 절약할 수 있었어요.\n두 번째 장점은 코드의 문제점을 빠르게 파악하고 수정할 수 있다는 점이에요. 테스트를 먼저 작성한 후, 테스트를 통해 LLM이 생성한 코드를 검증하기 때문인데요. 테스트가 통과한 이후 리팩토링을 진행할 때에도 테스트를 통해 문제를 감지하고 바르게 수정할 수 있어요.\n다만, 주의점은 LLM을 활용해 TDD를 하더라도, LLM이 생성한 코드를 리뷰하고 오류를 수정하는 것은 엔지니어의 역할이에요. 따라서 SwiftSyntax와 같이 구현에 사용되는 기술에 대한 지식을 반드시 갖추고 진행하여야 해요.\n나가며\nChatGPT 등장 이후 LLM 기반의 생성형 AI가 빠르게 발전해 나가면서 세상을 바꾸고 있는데요. 당근에서는 LLM을 활용하여 사용자 경험이나 동료의 생산성을 높이는 도구를 함께 만들어 갈 iOS 엔지니어를 찾고 있어요. 저희의 여정에 함께하고 싶으시다면 아래 공고를 통해 지원하실 수 있어요! 😃\nSoftware Engineer, iOS | 당근 팀 채용\n\nCursor와 TDD로 만드는 Swift Macro was originally published in 당근 테크 블로그 on Medium, where people are continuing the conversation by highlighting and responding to this story.",
    "dc:creator": "Elon park",
    "guid": "https://medium.com/p/0e4a245caee2",
    "categories": [
      "tdd",
      "ios",
      "llm",
      "programming"
    ],
    "isoDate": "2025-04-17T06:13:12.000Z"
  },
  {
    "creator": "Hy Lee (sang un)",
    "title": "검색 Indexing 파이프라인 개선기",
    "link": "https://medium.com/daangn/%EA%B2%80%EC%83%89-indexing-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8-%EA%B0%9C%EC%84%A0%EA%B8%B0-c01c64292831?source=rss----4505f82a2dbd---4",
    "pubDate": "Tue, 15 Apr 2025 06:14:54 GMT",
    "content:encoded": "<p>안녕하세요! 검색플랫폼팀의 Backend Engineer 하이(Hy)에요.</p><p>당근에는 중고거래, 동네생활, 동네업체, 채팅 등 다양한 서비스가 있는데요. 검색플랫폼팀은 검색 서비스를 안정적으로 지원하는 플랫폼을 만들며, 다양한 서비스의 검색을 지원하고 있어요. 이를 위해서는 가장 먼저 각 서비스들의 데이터가 필요한데요. 이번 글에서는 각 서비스의 데이터를 검색 엔진에 전달하는 색인 파이프라인을 운영하면서, 어떤 문제가 있었고 어떻게 해결했는지 이야기하려고 해요.</p><p>색인 파이프라인은 1) 실시간성을 보장하기 위한 Online 색인과 2) 사전 변경, 데이터 보정을 위해 하루에 한 번 전체 색인을 하는 Offline 색인을 해요. 이 두 가지를 진행할 때, 기존 파이프라인은 아래와 같은 문제점이 있었어요.</p><h4>생산성</h4><p>데이터를 가져오고 색인하는 로직의 패턴은 대부분 동일하지만, 서비스마다 로직이 따로 존재해서 관리 포인트가 많아져요.</p><h4>의존성</h4><p>당근의 서비스 DB에 직접적으로 붙어서 데이터를 가져오기 때문에, 각 서비스 DB에 강한 의존성을 가지게 됐어요.</p><h4>비용</h4><p>하루에 한 번 OnlineDB의 내용을 전체 Full Scan해야 하기 때문에 색인 파이프라인의 DB의 사이즈가 커야 했어요. 또한 서비스팀에서도 저희를 위한 ReplicaDB를 만들어주어야 해요.</p><h4>가시성</h4><p>어떤 필드가 어떻게 색인이 되는지, 필터링 로직 등 색인 로직이 어떻게 구성됐는지 등을 파악하려면, 늘 코드를 전부 일일이 확인해야 했어요.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*SRARdWW1QM5Ryfk4aQ3I2w.png\" /><figcaption><strong>이전 색인 파이프라인에서 개발해야 하는 부분</strong></figcaption></figure><p><strong>위의 4가지 문제를 해결하기 위해 다음과 같은 목표를 세웠어요.</strong></p><ol><li>설정 기반의 인터페이스로 자동화하여, 누구나 기여할 수 있는 생산성이 높은 시스템을 만들어요.</li><li>외부 서비스 DB에 대한 의존성을 낮춰서, 상태가 변경되어도 안전한 색인 파이프라인을 만들어요.</li><li>하루에 한 번 풀색인(Full Indexing)의 부담을 낮추고 비용을 줄여요.</li><li>이벤트가 쏟아져도 안전한 고가용성의 시스템을 만들어요.</li></ol><h4>1) 설정 기반의 인터페이스로 자동화하여, 누구나 기여할 수 있는 생산성이 높은 시스템을 만들어요.</h4><p>위의 목표를 달성하기 위해서는 동작을 명시하는 설정 Interface가 잘 정의되어야 해요. 또한 아무리 자동화가 목적이라도 안전한 시스템을 갖추기 위해선, 먼저 안전한 스키마를 설계해야 해요. 그래서 아래와 같은 원칙을 세웠어요.</p><ul><li>해당 인터페이스에 명시된 동작만 이루어져야 하며, 이외에 다른 동작을 하는 마술은 부리지 않아야 함</li><li>모든 데이터는 스키마를 가지고 안전하게 처리할 수 있도록 해야 함</li><li>모든 in/out 사이에는 비즈니스로직을 자유롭게 넣을 수 있어야 함</li><li>사용자가 몰라도 되는 core 로직은 데이터 처리 외에는 하지 않아야 함</li></ul><p>먼저 동작을 명시하는 Interface를 정의하기 위해 비교적 접근성이 쉬운 yaml을 사용하였어요. 아래는 데이터를 어떻게 가져오고, 적재하고, 관리할지를 보여주는 Datastore의 Interface 예시예요.</p><pre>version: 1<br>name: {서비스이름}_datastore<br>storage: datastore<br>sources:<br>  - table: {테이블명}_v2<br>    comment: &quot;{설명 주석}&quot;<br>    country: ### 사용되는 나라만 넣어주세요.<br>    primary_key: {primary_key_field}<br>    schema: {DB schema 경로} ### dbschema/{테이블명}.yaml로 만들어주세요 schema의 type은 MySQL 타입기준으로 만들어주시면 됩니다.<br>    scan:<br>      type: time<br>      target_field: {스키마의 변경시간 field이름} ### 데이터 비교 및 최신데이터를 구별할 필드이름을 넣어주세요 예시) updated_at<br>      ttl_interval_ms: {ttl ms}<br>      ### 예시) 온라인 디비에 적재할 TTL 시간을 넣어주세요 예시) 43200000, -1로 하면 영원히 가지고 있게 됩니다.<br>    subscribe_message:<br>      - topic: {kafka_topic} ### 인덱서에서 읽어 들일 카프카 topic을 넣어주세요 예시) event.search.region<br>        transform: {transform name}<br>        ### 해당 메시지를 처리할 transform의 이름을 넣어주세요 검색팀이 아닌 경우 직접 transform로직을 작성해 주셔도 좋고, 비워두시고 검색 플랫폼팀에게 요청을 주시면 검색 팀에서 만들어드립니다.<br>    publish_message:<br>      - topic: services.searchindexer.internal.job_state_change_v1<br>        ## bigquery cdc 로그에 적재할 토픽에 true를 넣어줍니다.<br>        cdc: true<br>    auto_indexing:<br>      - option: simple<br>        generate_policy: always<br>        partition_page_size: 100000<br>        ## 데이터 스토어 형태에서 검색엔진에 데이터를 삭제할 조건을 넣습니다.<br>        active_conditions:<br>          - destroyed_at is null<br>....        <br>    deployments: ### env prod / alpha 배포 설정을 넣어주세요. 지정된 환경변수만 배포가 되어요.<br>      - env: alpha<br>        slack_alarm_channel: &#39;#슬랙채널&#39;<br>    transform_parameter:<br>      - env: alpha<br>        pipeline:<br>          kr:<br>            configs:<br>              - name: flea_market_image_vector_v1<br>                tensorflow_serving_config: tensorflowserving/alpha/kr/flea_market_vector_v1.yaml       offline:<br>    executors:<br>      - env: alpha ### env prod / alpha<br>        back_fill: ### backfill sql 경로를 넣어주세요<br>          sql:<br>            all: backfill/regions.sql<br>        resources: ### 리소스 default는 memory: 512Mi cpu: 500m이에요.<br>          kr:<br>           limits:<br>             memory: 512Mi<br>             cpu: 500m<br>           requests:<br>             memory: 512Mi<br>             cpu: 500m</pre><p>위의 interface를 작성하고 명령어를 치면 시스템에서 코드를 생성해요. <strong>“모든 데이터는 스키마를 가져서 안전하게 처리해야 함”</strong>이라는 원칙을 보장하기 위해 “코드 생성 방식”을 선택했어요. 또한 Online 이벤트와 Offline 데이터를 똑같은 형태로 만들어야 하기 때문에, 시스템에서 Schema를 코드로 만드는 것이 가장 안전하다고 판단했어요.</p><p>아래는 위의 Interface의 Schema를 읽고 생성한 코드 예시예요.</p><pre>// Code generated by indexer/message. DO NOT EDIT.<br>// versions: v1<br>// source: message/schema/.../articles_datastore.yaml<br>// 중고거래 article MySQL 테이블 모델<br><br>package articlesdatastore<br>import (<br>   &quot;github.com/daangn/search-indexer/message/entity&quot;<br>   &quot;github.com/daangn/search-indexer/message/entity/utils&quot;<br>   &quot;time&quot;<br>)<br>var _ entity.IndexerMySQLMessage = &amp;ArticlesV2{}<br>type FleaMarket struct {<br> ID                      int64      `db:&quot;id&quot; json:&quot;id&quot; bigquery:&quot;id&quot; structs:&quot;id&quot;`<br> Title                   *string    `db:&quot;title&quot; json:&quot;title&quot; bigquery:&quot;title&quot; structs:&quot;title&quot;`<br> Content                 *string    `db:&quot;content&quot; json:&quot;content&quot; bigquery:&quot;content&quot; structs:&quot;content&quot;`<br>.....</pre><p>유연한 플랫폼을 만들려면, 엔지니어가 모든 input 데이터를 자유롭게 변환할 수 있어야 해요. 이를 위해 Interface 중간에 엔지니어가 직접 코드를 삽입할 수 있는 Transform 단계를 만들어뒀어요. 외부에서 받아온 메시지는 Transform의 로직을 거쳐 변환되고, 그 후 OnlineStorage와 상태 변경 이벤트를 통해 OfflineStorage에 적재돼요. Transform을 구현하는 구체적인 방식은 아래와 같아요.</p><pre>type DataLoaderMessageTransform interface {<br>    // GetName transform 이름 : schema yaml transform 필드에 들어갈 이름<br>    GetName() string<br><br>    // Transform 변환 로직이 들어갈 곳 return 값이 DataStore에 적재됨<br>    // kafka message를 파싱해 다시 DB에서 값을 가져오거나<br>    // message안에 데이터가 있다면 변환해서 생성된 MySQL Schema모델에 채워서 리턴해준다.<br>    MessageTransform(ctx context.Context, <br>                     parameter entity.TransformParameter,<br>                     msg *kafka.Message) (entity.IndexerMySQLMessage, error)<br><br>    // CustomClearSQL Custom 하여 지우는 로직을 넣는 SQL<br>    // custom 이 필요 없이 나 자신을 Clear 하는 것이라면 retrun &quot;&quot; 하면 된다<br>    // 릴레이션되어 1:N관계 일 때 1에 릴레이션되어 커스텀 SQL을 넣고 싶을 때 사용하면 된다.<br>    CustomClearSQL() string<br>}</pre><p>SearchEngine에 적재하는 Indexing Inteface도 이와 비슷한 형태로 만들 수 있어요. 추가적으로, 코드를 생성시키면 Airflow에 Offline 색인 파이프라인이 자동으로 생기도록 하였어요.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*juJHipwXCiF8xBUrBEwDpw.png\" /></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*xB642YdttfLuZcGm55Ihzw.png\" /><figcaption>자동으로 생성된 Airflow Dag</figcaption></figure><h4>2) 외부 서비스 DB와 의존성을 낮춰서, 안전한 색인 파이프라인을 만들어요</h4><p>대부분 서비스의 DB는 MySQL, MongoDB 같은 OnlineDB 일 거예요. OnlineDB는 실시간으로 변할 수 있는 상태이기 때문에, 직접 외부 서비스의 OnlineDB를 바라보는 것은 장애 포인트가 될 수 있어요. 또 풀색인 시에 모든 데이터를 읽어오기 위한 비용도 크고 유지하기가 어려워요. 이를 해결하고자 저희는 Offline Storage를 사용하기로 결정하였어요.</p><p>당근은 OfflineStorage로 BigQuery를 사용하고 있고, 모든 서비스의 데이터가 BigQuery에 잘 적재되어 있는데요. 이는 서비스에서 직접적으로 사용되는 데이터이기 때문에 DB의 신뢰성과 성능, 그리고 비용을 잘 관리해야 했어요.</p><p>먼저, 위의 datastore inteface에 명시된 backfill로 가져온 데이터로 빈 데이터를 채워주었어요. 그리고 WAL(Write-Ahead Log)과 같은 형태의 상태 변경 메시지를 발행하여, 데이터를 OfflineStorage에 적재했어요. 이때 적재된 데이터를 가장 우선순위가 높은 데이터로 활용하였어요.</p><blockquote><em>“상태 변경 이벤트”는 특정 요인에서 순서, 시간 등 이 잘못될 수 있기 때문에 이벤트 메시지에는 ID만 발행해요. 그다음 CDCStreaming 애플리케이션에서 ID를 기준으로 데이터를 조회한 후, Schema에 맞춘 변환 로직을 거쳐서 OfflineStorage에 저장해요.</em></blockquote><p>프로세스는 다음과 같아요.</p><ul><li>먼저 외부 데이터를 Backfill하여 빈데이터를 채워줘요.</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*HAdsVSlKSqUt-SusfKC5bg.png\" /></figure><ul><li>Scan 하는 필드를 기준으로 Streaming 데이터를 비교하여 더 큰 값을 우선순위로 정해요. 그 후 ID 기준의 유니크한 Row를 가지는 형태로 머지하고, Offline 색인 테이블에 Overwrite 해요.</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*i4j0MxrKSi3R-vRfPXFtNQ.png\" /></figure><ul><li>위의 프로세스로 전체 데이터를 채워주면 Backfill시에 잘못된 데이터가 들어와도, 저희의 데이터로 overwrite하기 때문에 안전한 데이터 테이블을 만들어 색인할 수 있었어요.</li><li>마지막으로 OfflineStorage의 최신 데이터 이후의 데이터도 함께 처리해줘야 하는데요. 해당 데이터는 색인 파이프라인에서 사용하는 OnlineDB에서 가져와 실시간성까지 보장할 수 있었어요.</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*yzjKo8UjpZ-Th9z5bENvyQ.png\" /></figure><h4>3) 하루에 한 번 풀색인의 부담을 낮추고 비용을 줄여요.</h4><p>하루에 한 번 전체 데이터를 가져와 색인하는 과정에서 <strong>“어떻게 OffineStorage에서 적은 비용으로 빠르게 데이터를 가져올 수 있을까?”</strong> 하는 고민을 가지게 됐어요. BigQuery 같은 OfflineStorage는 파일 시스템이기 때문에 특정한 범위를 가져오는 SQL을 작성하더라도 FullScan을 발생시켜요. 해당 문제를 해결하기 위해 색인 파이프라인에서 데이터를 읽기 전, 데이터를 파티셔닝하고 복제 테이블을 만드는 작업을 하였어요.</p><ul><li>Offline 색인 시작 전 data table에서 partition 룰을 통하여 partition을 나눈 임시 테이블을 생성해요.</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*9s2xtKIZvFotROKqOGL7gg.png\" /></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*pg0SkpOqZZHz0yT07Xu_ZQ.png\" /><figcaption><strong>실제 airflow log</strong></figcaption></figure><ul><li>색인 파이프라인에서 각 분산된 Processor별로 할당된 파티션을 읽고, Processor 내부의 Task들이 데이터를 검색엔진에 색인해요.</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*VvovzE1EmkHszFuAsuKHnA.png\" /></figure><p>위와 같이 작업을 하고 하니 하루에 수억 건의 데이터를 색인하는데, 1~2시간 정도 끝나는 성능을 보였어요. 게다가 1~2만 원 정도의 비용밖에 들지 않았고요.</p><p>실제로 Spark 같은 오픈소스가 이 문제를 어떻게 해결하는지 들여다보니 위에서 작업한 형태와 크게 다르지 않다는 것을 확인했어요. 이를 통해 현재 접근 방식에 대한 신뢰와 확신을 좀 더 가질 수 있었어요.</p><h4>4) 이벤트가 쏟아져도 안전한 고가용 시스템을 만들어요.</h4><p>색인 파이프라인을 오픈된 형태로 유지하려면, 무수히 많은 카프카 이벤트에도 안전한 시스템을 만들어야 해요. 실제로 내부 서비스 중 어떤 서비스는 초당 수천 건의 이벤트를 발행해요. 이러한 상황에서 비용을 최소화하고 성능을 극대화할 방법을 찾아야 했어요.</p><p>대부분의 서비스들은 초당 많아야 100건 이내의 이벤트가 발생하기 때문에, 각 이벤트를 OnlineDB에 적재해도 큰 문제가 없어요. 다만, 초당 수천 건 이상의 데이터를 밀어 넣는 서비스의 경우, 혹은 해당 파이프라인에서 embedding vector를 생성하거나 모델 inference를 하는 Latency가 긴 서비스의 경우, KafkaLag가 생기거나 DB의 부하가 급격히 증가해 다른 서비스에도 영향을 줄 수 있어요.</p><p>해당 문제를 해결하기 위해 Streaming Tumbling TimeWindow을 만들어 Streaming Batch 처리를 가능하게 하였어요. 검색엔진에 적재할 때도 동일하게 Streaming Batch 처리를 해요.</p><p>Indexer Pipeline의 TimeWindow 동작 방식은 Streaming OpenSource들을 모방하여 Local 머신마다 StateStore를 가지고 처리할 수 있도록 구현했어요.</p><pre>streams := kafka.NewStream(topic, config.KafkaConsumer).<br>\t\tWindowTime(windowTime).<br>\t\tTransform(func(ctx context.Context, msg *kafka2.Message) (interface{}, interface{}, error) {<br>\t\t\t\treturn msg.key, msg.Value, nil<br>\t\t\t}, consumeFailProcess).<br>\t\tProcess(func(ctx context.Context, window *TimeWindow) error {<br>\t\t...<br>\t\t\treturn nil<br>\t\t}, consumeFailProcess)</pre><p>아래와 같이 설정에 time window 기간을 명시하고, 아래 “flea_market_vector_v1”라는 이름으로 DataLoaderBatchStreamingTransform를 구현했어요. 이 로직을 transform에 넣어주면 TumblingWindow 형태로 데이터를 모아서 batch 처리를 할 수 있어요.</p><pre>subscribe_message:<br>      - topic: indexing.fleamarket.flea_market_article.v1<br>        transform: flea_market_vector_v1<br>        window_time_milliseconds: 2000</pre><pre>type DataLoaderBatchStreamingTransform interface {<br>    // GetName transform 이름 : schema yaml transform 필드에 들어갈 이름<br>    GetName() string<br>    // Preprocess batch transform전 단일 문서에 대해 전처리를 수행합니다.<br>    Preprocess(ctx context.Context,<br>               parameter entity.TransformParameter,<br>               msg *kafka.Message) (entity.IndexerMessage, error)<br>    // BatchTransform bulk로 변환 로직이 들어갈 곳 return 값이 DataStore에 적재됨<br>    BatchTransform(ctx context.Context,<br>                   parameter entity.TransformParameter,<br>                   data []entity.IndexerMessage) ([]entity.IndexerMySQLMessage, error)<br>}</pre><p>해당 작업을 통해 1.5core의 pod 8대 만으로도 초당 수만 건의 이벤트를 무리 없이 처리할 수 있었어요.</p><p>이외에도 최근까지 Transform을 자동으로 생성해 주는 옵션, Vector Emebdding, LLM, Model Inference, 수많은 PR 테스트와 모니터링 등 검색 서비스를 안정적으로 지원하기 위해 다양한 작업을 지속하고 있어요.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*0w3ElC3X2PvwGmFCjBEzDg.png\" /><figcaption>Model Inference 등 기능이 추가되고 있는 Indexer pipeline</figcaption></figure><h4>마무리</h4><p>지금까지 당근의 검색플랫폼팀에서 색인 파이프라인의 안정성과 생산성을 높이기 위해 고민하고 개선했던 과정들을 소개했어요.</p><p>서비스가 성장할수록 더 많은 데이터를 효율적으로 처리할 수 있는 구조가 필요해지고, 동시에 서비스 간 의존성을 낮추고 가시성을 높여야 하는데요. 앞으로도 저희 검색플랫폼팀은 더욱 편리하고 신뢰할 수 있는 검색 환경을 사용자와 개발자 모두에게 제공하기 위해 다양한 시도를 계속할 예정이에요. 당근의 검색 플랫폼이 어떤 모습으로 발전해 나갈지 많은 관심 부탁드리며, 함께 고민하고 성장할 멋진 동료들을 언제나 환영합니다!</p><p>긴 글 읽어주셔서 감사합니다. 😊</p><p>검색인프라 채용: <a href=\"https://about.daangn.com/jobs/5688517003/\">https://about.daangn.com/jobs/5688517003/</a></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c01c64292831\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/daangn/%EA%B2%80%EC%83%89-indexing-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8-%EA%B0%9C%EC%84%A0%EA%B8%B0-c01c64292831\">검색 Indexing 파이프라인 개선기</a> was originally published in <a href=\"https://medium.com/daangn\">당근 테크 블로그</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "content:encodedSnippet": "안녕하세요! 검색플랫폼팀의 Backend Engineer 하이(Hy)에요.\n당근에는 중고거래, 동네생활, 동네업체, 채팅 등 다양한 서비스가 있는데요. 검색플랫폼팀은 검색 서비스를 안정적으로 지원하는 플랫폼을 만들며, 다양한 서비스의 검색을 지원하고 있어요. 이를 위해서는 가장 먼저 각 서비스들의 데이터가 필요한데요. 이번 글에서는 각 서비스의 데이터를 검색 엔진에 전달하는 색인 파이프라인을 운영하면서, 어떤 문제가 있었고 어떻게 해결했는지 이야기하려고 해요.\n색인 파이프라인은 1) 실시간성을 보장하기 위한 Online 색인과 2) 사전 변경, 데이터 보정을 위해 하루에 한 번 전체 색인을 하는 Offline 색인을 해요. 이 두 가지를 진행할 때, 기존 파이프라인은 아래와 같은 문제점이 있었어요.\n생산성\n데이터를 가져오고 색인하는 로직의 패턴은 대부분 동일하지만, 서비스마다 로직이 따로 존재해서 관리 포인트가 많아져요.\n의존성\n당근의 서비스 DB에 직접적으로 붙어서 데이터를 가져오기 때문에, 각 서비스 DB에 강한 의존성을 가지게 됐어요.\n비용\n하루에 한 번 OnlineDB의 내용을 전체 Full Scan해야 하기 때문에 색인 파이프라인의 DB의 사이즈가 커야 했어요. 또한 서비스팀에서도 저희를 위한 ReplicaDB를 만들어주어야 해요.\n가시성\n어떤 필드가 어떻게 색인이 되는지, 필터링 로직 등 색인 로직이 어떻게 구성됐는지 등을 파악하려면, 늘 코드를 전부 일일이 확인해야 했어요.\n이전 색인 파이프라인에서 개발해야 하는 부분\n위의 4가지 문제를 해결하기 위해 다음과 같은 목표를 세웠어요.\n\n설정 기반의 인터페이스로 자동화하여, 누구나 기여할 수 있는 생산성이 높은 시스템을 만들어요.\n외부 서비스 DB에 대한 의존성을 낮춰서, 상태가 변경되어도 안전한 색인 파이프라인을 만들어요.\n하루에 한 번 풀색인(Full Indexing)의 부담을 낮추고 비용을 줄여요.\n이벤트가 쏟아져도 안전한 고가용성의 시스템을 만들어요.\n\n1) 설정 기반의 인터페이스로 자동화하여, 누구나 기여할 수 있는 생산성이 높은 시스템을 만들어요.\n위의 목표를 달성하기 위해서는 동작을 명시하는 설정 Interface가 잘 정의되어야 해요. 또한 아무리 자동화가 목적이라도 안전한 시스템을 갖추기 위해선, 먼저 안전한 스키마를 설계해야 해요. 그래서 아래와 같은 원칙을 세웠어요.\n\n해당 인터페이스에 명시된 동작만 이루어져야 하며, 이외에 다른 동작을 하는 마술은 부리지 않아야 함\n모든 데이터는 스키마를 가지고 안전하게 처리할 수 있도록 해야 함\n모든 in/out 사이에는 비즈니스로직을 자유롭게 넣을 수 있어야 함\n사용자가 몰라도 되는 core 로직은 데이터 처리 외에는 하지 않아야 함\n\n먼저 동작을 명시하는 Interface를 정의하기 위해 비교적 접근성이 쉬운 yaml을 사용하였어요. 아래는 데이터를 어떻게 가져오고, 적재하고, 관리할지를 보여주는 Datastore의 Interface 예시예요.\nversion: 1\nname: {서비스이름}_datastore\nstorage: datastore\nsources:\n  - table: {테이블명}_v2\n    comment: \"{설명 주석}\"\n    country: ### 사용되는 나라만 넣어주세요.\n    primary_key: {primary_key_field}\n    schema: {DB schema 경로} ### dbschema/{테이블명}.yaml로 만들어주세요 schema의 type은 MySQL 타입기준으로 만들어주시면 됩니다.\n    scan:\n      type: time\n      target_field: {스키마의 변경시간 field이름} ### 데이터 비교 및 최신데이터를 구별할 필드이름을 넣어주세요 예시) updated_at\n      ttl_interval_ms: {ttl ms}\n      ### 예시) 온라인 디비에 적재할 TTL 시간을 넣어주세요 예시) 43200000, -1로 하면 영원히 가지고 있게 됩니다.\n    subscribe_message:\n      - topic: {kafka_topic} ### 인덱서에서 읽어 들일 카프카 topic을 넣어주세요 예시) event.search.region\n        transform: {transform name}\n        ### 해당 메시지를 처리할 transform의 이름을 넣어주세요 검색팀이 아닌 경우 직접 transform로직을 작성해 주셔도 좋고, 비워두시고 검색 플랫폼팀에게 요청을 주시면 검색 팀에서 만들어드립니다.\n    publish_message:\n      - topic: services.searchindexer.internal.job_state_change_v1\n        ## bigquery cdc 로그에 적재할 토픽에 true를 넣어줍니다.\n        cdc: true\n    auto_indexing:\n      - option: simple\n        generate_policy: always\n        partition_page_size: 100000\n        ## 데이터 스토어 형태에서 검색엔진에 데이터를 삭제할 조건을 넣습니다.\n        active_conditions:\n          - destroyed_at is null\n....        \n    deployments: ### env prod / alpha 배포 설정을 넣어주세요. 지정된 환경변수만 배포가 되어요.\n      - env: alpha\n        slack_alarm_channel: '#슬랙채널'\n    transform_parameter:\n      - env: alpha\n        pipeline:\n          kr:\n            configs:\n              - name: flea_market_image_vector_v1\n                tensorflow_serving_config: tensorflowserving/alpha/kr/flea_market_vector_v1.yaml       offline:\n    executors:\n      - env: alpha ### env prod / alpha\n        back_fill: ### backfill sql 경로를 넣어주세요\n          sql:\n            all: backfill/regions.sql\n        resources: ### 리소스 default는 memory: 512Mi cpu: 500m이에요.\n          kr:\n           limits:\n             memory: 512Mi\n             cpu: 500m\n           requests:\n             memory: 512Mi\n             cpu: 500m\n위의 interface를 작성하고 명령어를 치면 시스템에서 코드를 생성해요. “모든 데이터는 스키마를 가져서 안전하게 처리해야 함”이라는 원칙을 보장하기 위해 “코드 생성 방식”을 선택했어요. 또한 Online 이벤트와 Offline 데이터를 똑같은 형태로 만들어야 하기 때문에, 시스템에서 Schema를 코드로 만드는 것이 가장 안전하다고 판단했어요.\n아래는 위의 Interface의 Schema를 읽고 생성한 코드 예시예요.\n// Code generated by indexer/message. DO NOT EDIT.\n// versions: v1\n// source: message/schema/.../articles_datastore.yaml\n// 중고거래 article MySQL 테이블 모델\npackage articlesdatastore\nimport (\n   \"github.com/daangn/search-indexer/message/entity\"\n   \"github.com/daangn/search-indexer/message/entity/utils\"\n   \"time\"\n)var _ entity.IndexerMySQLMessage = &ArticlesV2{}\ntype FleaMarket struct {\n ID                      int64      `db:\"id\" json:\"id\" bigquery:\"id\" structs:\"id\"`\n Title                   *string    `db:\"title\" json:\"title\" bigquery:\"title\" structs:\"title\"`\n Content                 *string    `db:\"content\" json:\"content\" bigquery:\"content\" structs:\"content\"`\n.....\n유연한 플랫폼을 만들려면, 엔지니어가 모든 input 데이터를 자유롭게 변환할 수 있어야 해요. 이를 위해 Interface 중간에 엔지니어가 직접 코드를 삽입할 수 있는 Transform 단계를 만들어뒀어요. 외부에서 받아온 메시지는 Transform의 로직을 거쳐 변환되고, 그 후 OnlineStorage와 상태 변경 이벤트를 통해 OfflineStorage에 적재돼요. Transform을 구현하는 구체적인 방식은 아래와 같아요.\ntype DataLoaderMessageTransform interface {\n    // GetName transform 이름 : schema yaml transform 필드에 들어갈 이름\n    GetName() string\n    // Transform 변환 로직이 들어갈 곳 return 값이 DataStore에 적재됨\n    // kafka message를 파싱해 다시 DB에서 값을 가져오거나\n    // message안에 데이터가 있다면 변환해서 생성된 MySQL Schema모델에 채워서 리턴해준다.\n    MessageTransform(ctx context.Context, \n                     parameter entity.TransformParameter,\n                     msg *kafka.Message) (entity.IndexerMySQLMessage, error)\n    // CustomClearSQL Custom 하여 지우는 로직을 넣는 SQL\n    // custom 이 필요 없이 나 자신을 Clear 하는 것이라면 retrun \"\" 하면 된다\n    // 릴레이션되어 1:N관계 일 때 1에 릴레이션되어 커스텀 SQL을 넣고 싶을 때 사용하면 된다.\n    CustomClearSQL() string\n}\nSearchEngine에 적재하는 Indexing Inteface도 이와 비슷한 형태로 만들 수 있어요. 추가적으로, 코드를 생성시키면 Airflow에 Offline 색인 파이프라인이 자동으로 생기도록 하였어요.\n자동으로 생성된 Airflow Dag\n2) 외부 서비스 DB와 의존성을 낮춰서, 안전한 색인 파이프라인을 만들어요\n대부분 서비스의 DB는 MySQL, MongoDB 같은 OnlineDB 일 거예요. OnlineDB는 실시간으로 변할 수 있는 상태이기 때문에, 직접 외부 서비스의 OnlineDB를 바라보는 것은 장애 포인트가 될 수 있어요. 또 풀색인 시에 모든 데이터를 읽어오기 위한 비용도 크고 유지하기가 어려워요. 이를 해결하고자 저희는 Offline Storage를 사용하기로 결정하였어요.\n당근은 OfflineStorage로 BigQuery를 사용하고 있고, 모든 서비스의 데이터가 BigQuery에 잘 적재되어 있는데요. 이는 서비스에서 직접적으로 사용되는 데이터이기 때문에 DB의 신뢰성과 성능, 그리고 비용을 잘 관리해야 했어요.\n먼저, 위의 datastore inteface에 명시된 backfill로 가져온 데이터로 빈 데이터를 채워주었어요. 그리고 WAL(Write-Ahead Log)과 같은 형태의 상태 변경 메시지를 발행하여, 데이터를 OfflineStorage에 적재했어요. 이때 적재된 데이터를 가장 우선순위가 높은 데이터로 활용하였어요.\n“상태 변경 이벤트”는 특정 요인에서 순서, 시간 등 이 잘못될 수 있기 때문에 이벤트 메시지에는 ID만 발행해요. 그다음 CDCStreaming 애플리케이션에서 ID를 기준으로 데이터를 조회한 후, Schema에 맞춘 변환 로직을 거쳐서 OfflineStorage에 저장해요.\n프로세스는 다음과 같아요.\n\n먼저 외부 데이터를 Backfill하여 빈데이터를 채워줘요.\n\nScan 하는 필드를 기준으로 Streaming 데이터를 비교하여 더 큰 값을 우선순위로 정해요. 그 후 ID 기준의 유니크한 Row를 가지는 형태로 머지하고, Offline 색인 테이블에 Overwrite 해요.\n\n위의 프로세스로 전체 데이터를 채워주면 Backfill시에 잘못된 데이터가 들어와도, 저희의 데이터로 overwrite하기 때문에 안전한 데이터 테이블을 만들어 색인할 수 있었어요.\n마지막으로 OfflineStorage의 최신 데이터 이후의 데이터도 함께 처리해줘야 하는데요. 해당 데이터는 색인 파이프라인에서 사용하는 OnlineDB에서 가져와 실시간성까지 보장할 수 있었어요.\n\n3) 하루에 한 번 풀색인의 부담을 낮추고 비용을 줄여요.\n하루에 한 번 전체 데이터를 가져와 색인하는 과정에서 “어떻게 OffineStorage에서 적은 비용으로 빠르게 데이터를 가져올 수 있을까?” 하는 고민을 가지게 됐어요. BigQuery 같은 OfflineStorage는 파일 시스템이기 때문에 특정한 범위를 가져오는 SQL을 작성하더라도 FullScan을 발생시켜요. 해당 문제를 해결하기 위해 색인 파이프라인에서 데이터를 읽기 전, 데이터를 파티셔닝하고 복제 테이블을 만드는 작업을 하였어요.\n\nOffline 색인 시작 전 data table에서 partition 룰을 통하여 partition을 나눈 임시 테이블을 생성해요.\n실제 airflow log\n색인 파이프라인에서 각 분산된 Processor별로 할당된 파티션을 읽고, Processor 내부의 Task들이 데이터를 검색엔진에 색인해요.\n\n위와 같이 작업을 하고 하니 하루에 수억 건의 데이터를 색인하는데, 1~2시간 정도 끝나는 성능을 보였어요. 게다가 1~2만 원 정도의 비용밖에 들지 않았고요.\n실제로 Spark 같은 오픈소스가 이 문제를 어떻게 해결하는지 들여다보니 위에서 작업한 형태와 크게 다르지 않다는 것을 확인했어요. 이를 통해 현재 접근 방식에 대한 신뢰와 확신을 좀 더 가질 수 있었어요.\n4) 이벤트가 쏟아져도 안전한 고가용 시스템을 만들어요.\n색인 파이프라인을 오픈된 형태로 유지하려면, 무수히 많은 카프카 이벤트에도 안전한 시스템을 만들어야 해요. 실제로 내부 서비스 중 어떤 서비스는 초당 수천 건의 이벤트를 발행해요. 이러한 상황에서 비용을 최소화하고 성능을 극대화할 방법을 찾아야 했어요.\n대부분의 서비스들은 초당 많아야 100건 이내의 이벤트가 발생하기 때문에, 각 이벤트를 OnlineDB에 적재해도 큰 문제가 없어요. 다만, 초당 수천 건 이상의 데이터를 밀어 넣는 서비스의 경우, 혹은 해당 파이프라인에서 embedding vector를 생성하거나 모델 inference를 하는 Latency가 긴 서비스의 경우, KafkaLag가 생기거나 DB의 부하가 급격히 증가해 다른 서비스에도 영향을 줄 수 있어요.\n해당 문제를 해결하기 위해 Streaming Tumbling TimeWindow을 만들어 Streaming Batch 처리를 가능하게 하였어요. 검색엔진에 적재할 때도 동일하게 Streaming Batch 처리를 해요.\nIndexer Pipeline의 TimeWindow 동작 방식은 Streaming OpenSource들을 모방하여 Local 머신마다 StateStore를 가지고 처리할 수 있도록 구현했어요.\nstreams := kafka.NewStream(topic, config.KafkaConsumer).\n\t\tWindowTime(windowTime).\n\t\tTransform(func(ctx context.Context, msg *kafka2.Message) (interface{}, interface{}, error) {\n\t\t\t\treturn msg.key, msg.Value, nil\n\t\t\t}, consumeFailProcess).\n\t\tProcess(func(ctx context.Context, window *TimeWindow) error {\n\t\t...\n\t\t\treturn nil\n\t\t}, consumeFailProcess)\n아래와 같이 설정에 time window 기간을 명시하고, 아래 “flea_market_vector_v1”라는 이름으로 DataLoaderBatchStreamingTransform를 구현했어요. 이 로직을 transform에 넣어주면 TumblingWindow 형태로 데이터를 모아서 batch 처리를 할 수 있어요.\nsubscribe_message:\n      - topic: indexing.fleamarket.flea_market_article.v1\n        transform: flea_market_vector_v1\n        window_time_milliseconds: 2000\ntype DataLoaderBatchStreamingTransform interface {\n    // GetName transform 이름 : schema yaml transform 필드에 들어갈 이름\n    GetName() string\n    // Preprocess batch transform전 단일 문서에 대해 전처리를 수행합니다.\n    Preprocess(ctx context.Context,\n               parameter entity.TransformParameter,\n               msg *kafka.Message) (entity.IndexerMessage, error)\n    // BatchTransform bulk로 변환 로직이 들어갈 곳 return 값이 DataStore에 적재됨\n    BatchTransform(ctx context.Context,\n                   parameter entity.TransformParameter,\n                   data []entity.IndexerMessage) ([]entity.IndexerMySQLMessage, error)\n}\n해당 작업을 통해 1.5core의 pod 8대 만으로도 초당 수만 건의 이벤트를 무리 없이 처리할 수 있었어요.\n이외에도 최근까지 Transform을 자동으로 생성해 주는 옵션, Vector Emebdding, LLM, Model Inference, 수많은 PR 테스트와 모니터링 등 검색 서비스를 안정적으로 지원하기 위해 다양한 작업을 지속하고 있어요.\nModel Inference 등 기능이 추가되고 있는 Indexer pipeline\n마무리\n지금까지 당근의 검색플랫폼팀에서 색인 파이프라인의 안정성과 생산성을 높이기 위해 고민하고 개선했던 과정들을 소개했어요.\n서비스가 성장할수록 더 많은 데이터를 효율적으로 처리할 수 있는 구조가 필요해지고, 동시에 서비스 간 의존성을 낮추고 가시성을 높여야 하는데요. 앞으로도 저희 검색플랫폼팀은 더욱 편리하고 신뢰할 수 있는 검색 환경을 사용자와 개발자 모두에게 제공하기 위해 다양한 시도를 계속할 예정이에요. 당근의 검색 플랫폼이 어떤 모습으로 발전해 나갈지 많은 관심 부탁드리며, 함께 고민하고 성장할 멋진 동료들을 언제나 환영합니다!\n긴 글 읽어주셔서 감사합니다. 😊\n검색인프라 채용: https://about.daangn.com/jobs/5688517003/\n\n검색 Indexing 파이프라인 개선기 was originally published in 당근 테크 블로그 on Medium, where people are continuing the conversation by highlighting and responding to this story.",
    "dc:creator": "Hy Lee (sang un)",
    "guid": "https://medium.com/p/c01c64292831",
    "categories": [
      "indexing",
      "platform-engineering",
      "search",
      "pipeline"
    ],
    "isoDate": "2025-04-15T06:14:54.000Z"
  },
  {
    "creator": "당근",
    "title": "모두가 AI 로켓에 올라타도록, 당근 운영실이 AI로 일하는 법",
    "link": "https://medium.com/daangn/%EB%AA%A8%EB%91%90%EA%B0%80-ai-%EB%A1%9C%EC%BC%93%EC%97%90-%EC%98%AC%EB%9D%BC%ED%83%80%EB%8F%84%EB%A1%9D-%EB%8B%B9%EA%B7%BC-%EC%9A%B4%EC%98%81%EC%8B%A4%EC%9D%B4-ai%EB%A1%9C-%EC%9D%BC%ED%95%98%EB%8A%94-%EB%B2%95-b8aaa6713cea?source=rss----4505f82a2dbd---4",
    "pubDate": "Fri, 11 Apr 2025 06:11:35 GMT",
    "content:encoded": "<h3>모두가 AI 로켓에 올라타도록, 당근 운영실이 AI로 일하는 법 — 당근 AI Show &amp; Tell #2</h3><blockquote>당근은 매주 ‘AI Show &amp; Tell’을 통해 각 팀의 AI 실험을 전사적으로 공유해요. AI를 업무에 어떻게 적용하고 있는지, 그 과정에서 어떤 시행착오와 인사이트가 있었는지 가감 없이 나누죠. 당근은 완벽한 정답을 찾기보다 먼저 과감하게 실행하며, 새로운 시대의 문제 해결 방식을 빠르게 찾아가고 있어요. AI로 만드는 생생한 도전의 순간들, 지금 만나보세요.</blockquote><blockquote>✍️ 이 콘텐츠는 생성형 AI를 활용해 제작한 콘텐츠입니다.</blockquote><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*_KoBP5ulvvwWDBSKxnKSNw.png\" /><figcaption>서비스 운영실 팀 내부 세션 ‘Nextstep for Service Operation’ 발표 자료</figcaption></figure><p>단순히 팀에 AI를 도입한다고 해서 곧바로 혁신을 만들어낼 수 있을까요? 기술은 어디까지나 수단일 뿐이에요. 새로운 기술을 ‘무엇을 위해’, ‘어떻게’ 활용할지에 대한 체계가 뒷받침되지 않으면, 변화는 새로운 기술을 한두 번 시도해 보는 수준에서 그칠 수 있어요. AI로 유의미한 성과를 지속적으로 만들기 위해선 문제 해결 방식과 협업 구조까지 조직 전체가 AI에 맞게 바뀌어야 해요.</p><p>최근 당근 AI Show &amp; Tell 세션에서 서비스 운영실 리더 Brent와 운영실 ML Engineer 리더 Aio는 <strong>“운영실이 AI로 일하는 방식”</strong>을 공유했어요. 운영실은 사용자 접점의 최전선에서 사용자 문제 해결에 집요하게 몰입하는 팀이에요. 그렇기 때문에 AI를 도입할 때도 어떻게 해야 더 빠르고 정확하게 사용자 문제를 해결할 수 있을지 고민하며, <strong>팀의 실행과 학습 속도를 극대화할 수 있도록 조직 구조와 일하는 방식을 재설계했어요.</strong></p><p>이 글에서는 당근 운영실이 AI 전환을 위해 어떻게 실행 문화를 세우고, 조직 구조를 실험적으로 바꿔 나갔는지, 그리고 어떻게 직군의 경계를 넘어 팀 모두가 함께 몰입할 수 있었는지 그 과정을 전하려고 해요.</p><h3>Part 1. 기반을 다지다 — 방향과 실행 문화의 세팅</h3><p>운영실의 AI 전환은 팀의 비전을 함께 점검하며 목표를 명확히 설정하는 데서 시작됐어요. “사용자 문제를 해결해 사용자 만족으로 연결하겠다”라는 운영실의 목표는 예나 지금이나 같고, AI 시대에도 변하지 않을 거란 점을 다시 확인했는데요. <strong>다만 AI라는 강력한 수단을 잘 활용하면, 이전과는 비교할 수 없을 만큼 빠르게 목표를 성취할 수 있다는 데 모두가 공감했어요. </strong>그렇게 팀 전체가 AI로 과감하게 시도하며 그 구체적인 방법을 찾아가 보자는 데 뜻을 모았죠.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*i-gDJU17vSYKqvzRe4kySA.png\" /></figure><p><strong>특히 운영실 리더들은 한 달 동안, 많게는 하루에 한 번씩, 최소 일주일에 한 번은 모여 실행 전략을 구체화했어요. </strong>AI 프로젝트의 방향과 분기별 OKR, 북미 대상 제품 전략까지 논의하며, 운영실의 다양한 프로덕트 중 어떤 영역을 AI로 혁신할 수 있을지 리스트업하고 우선순위를 재정비했죠. 이 치열한 정렬 과정 덕분에 팀원들은 방향을 잃지 않고, 함께 한 방향을 바라볼 수 있게 됐어요.</p><p><strong>이후 구성원들이 그 방향으로 힘 있게 나아갈 수 있도록, 구성원들에게 심리적 안정감을 주려 했어요.</strong> AI는 빠르게 실험하며 최적화하는 과정이 중요한데, ‘실패하면 어쩌지’라는 두려움이 앞서면 시작조차 어렵거든요. 그래서 운영실에서는 “틀려도 된다”, “실패에서도 배울 수 있다”, “실행 자체가 의미 있다”는 말들을 슬랙과 회의에서 반복하며, 시도를 격려하는 분위기를 일관되게 만들어갔어요. 그러자 회고 자리에서 “AI로 과감한 실행을 해보고 싶어 졌다”라는 피드백이 나올 정도로 실행에 대한 기대감이 팀 안에 자리잡기 시작했어요.</p><h3>Part 2. 구조를 바꾸다 — 워킹그룹으로의 조직적 피봇</h3><p>이런 문화적 기반 위에서 운영실은 실제 일하는 구조를 바꿨어요. 기존에는 운영실 내 다양한 파트가 기능적으로 나뉘어 있었는데요. ‘사용자가 겪는 문제를 빠르게 해결하자’는 목표는 같았지만, 한 파트는 시나리오 기반의 문제 해결을, 다른 파트는 챗봇과 FAQ 모델 활용을 고려하는 등 접근 방식이 달랐어요. 문제 해결에 대한 관점이 분산되다 보니, 조율을 위한 커뮤니케이션 비용이 커졌어요. 특히 반복적인 실행과 빠른 정렬이 필요한 AI 기술을 도입할 때, 이런 구조는 실행 속도를 늦추는 장애물이 됐죠.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*eeVHadH-8CmKzTvvXFAw4Q.png\" /><figcaption>다양한 파트의 팀원들이 섞여 프로젝트별로 결성된 워킹그룹</figcaption></figure><p>문제를 해결하기 위해 운영실은 익숙했던 파트 구조를 내려놓고 워킹그룹 체계를 도입했어요. 동일한 문제를 푸는 사람들을 파트와 무관하게 한 그룹으로 묶고, 실제 오피스 자리까지 서로 가깝게 재배치했죠. 그러자 분산되어 있던 AI 기능도 하나의 목표 아래 빠르게 실험할 수 있게 됐어요. <strong>각 워킹그룹은 하나의 AI 프로덕트를 담당해 집중하고, 매주 월요일마다 데모를 팀 내부에 공유하며 개선점을 빠르게 정리했어요.</strong> 실행과 피드백을 빠르게 오가는 사이클을 통해 시도는 반복되고 학습은 가속화됐어요.</p><p><strong>이 실행 루틴을 뒷받침하기 위해</strong> <strong>운영실은 팀의 리소스를 과감히 재배치했어요. </strong>팀 내 자원이 한정된 상황에서 구성원들이 실행에 몰입할 수 있도록 한 거죠. 예를 들어 AI 효과가 상대적으로 적은 기능성 프로젝트는 최소 동작 상태로만 유지하고, VOC 파이프라인 프로젝트*처럼 AI 적용 효과가 큰 영역은 완전 자동화 중심으로 피봇했어요. 이렇게 무엇을 멈추고 어디에 몰입할지를 명확히 정리한 덕에 실행의 밀도를 크게 높일 수 있었어요.</p><blockquote><em>VOC 파이프라인 프로젝트: 챗봇 기반으로 문제를 해결하고 VOC를 프로덕트 팀에 연결하는 프로젝트</em></blockquote><p>물론 이 과정에서 중요한 기존 프로젝트들을 놓치지 않는 것도 필요했어요. 기존에 운영실이 진행해 오던 핵심 프로젝트들은 종료 일정을 명확히 설정해 병행 중이고, 완성 이후에는 인력을 워킹그룹 체계로 재배치할 예정이에요. 지금 가장 중요한 문제 해결에 집중할 수 있도록 과거의 업무와 새로운 시도를 병렬로 정렬한 전략이었는데요. 기존의 기반을 유지하면서도 새로운 시도에 치열하게 몰입한 운영 방식은, 실행 중심 AI 전환의 좋은 사례가 되었어요.</p><h3>Part 3. 몰입을 확산하다 — 직군을 뛰어넘는 AI 몰입과 실행</h3><p>그다음 단계는 기술의 문턱을 허무는 일이었어요. 운영실은 AI 전환의 속도를 높이기 위해, 개발자 중심의 실행에 머물지 않겠다는 원칙을 세웠어요. AI가 특정 직군의 도구로 한정되면 팀 전체의 몰입도는 낮아질 수밖에 없다고 본 거죠. <strong>그래서 비개발자 구성원들도 Cursor 같은 LLM 도구를 직접 실무에 활용할 수 있도록 환경을 설계했어요. </strong>예를 들어 중고거래 게시글의 가품 여부를 판단하는 실험에서는, 운영 매니저가 엔지니어와 함께 LLM에 테이블 스키마만 제공한 뒤, SQL 쿼리 생성을 요청하고 결과를 시각화하는 실습을 진행했어요.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*znuPSSrlCJSFItUCLIjPmQ.png\" /></figure><p>이런 경험을 통해 문제 해결에 AI가 유용하다는 체감이 생기자, 비개발자 구성원들의 태도도 빠르게 바뀌었어요. ‘AI가 할 수 있을까?’라는 의심은 ‘이건 내가 AI로 풀어야 할 문제다’라는 책임감으로 전환됐죠. 그러자 자발적인 학습 문화도 빠르게 자리 잡았어요. 퇴근 후에도 커서로 실습을 해보거나, 서로 사용법을 나누는 분위기가 형성됐죠. 리더는 “개발자도 아닌 우리도 할 수 있다”라고 독려했고, 실제로 Cursor뿐 아니라 다양한 LLM 도구를 활용한 크고 작은 실험이 자연스럽게 퍼졌어요.</p><p><strong>결국 운영실은 ‘AI를 도입한 팀’이 아니라 ‘AI로 일하는 팀’으로 변화해 갔어요. </strong>기술 역량에 상관없이 모두가 문제를 정의하고, 실행하고, 다시 정렬하는 리듬을 갖게 되었죠. AI는 특정 직군의 언어가 아닌, 문제를 함께 푸는 공동의 언어가 되었고요. 이 변화는 모두가 AI 전환의 로켓에 올라탔기 때문에 가능했고, ‘누구나 실행할 수 있다’는 믿음이 만든 결과였어요.</p><p>당근 운영실의 AI 전환은 단순한 기술 도입이 아니었어요. <strong>문화에서 시작해 구조를 바꾸고, 실행을 모두의 일상으로 확장한 과정이었어요. </strong>팀의 비전을 다시 정비하고, 실행을 주저하지 않는 문화를 함께 설계하며, 문제 중심으로 일하는 구조를 과감히 재편했어요. 여기에 직군을 가리지 않는 몰입과 학습, 빠른 실행과 피드백의 루틴이 더해지며, 문제 해결 방식도 계속해서 고도화됐어요.</p><p>운영실의 전환은 끝이 아니라 시작이에요. 이제 이 몰입의 문화가 다른 팀들로도 확산되고 있어요. 다음 AI Show &amp; Tell에서도 이 몰입의 문화가 어떻게 더 다양한 직군과 팀으로 확산되고, 전사적으로 어떤 실행의 연결고리를 만들어가고 있는지를 보여드리도록 할게요. 실행이 문화를 만들고, 문화가 더 큰 실행을 낳는 이 전환의 흐름을 함께 지켜봐 주세요.</p><blockquote>당근에서 함께 AI 전환에 몰입하고 싶다면?<br>👉 <a href=\"https://about.daangn.com/jobs/\"><strong>당근 채용 공고 바로 가기</strong></a></blockquote><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b8aaa6713cea\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/daangn/%EB%AA%A8%EB%91%90%EA%B0%80-ai-%EB%A1%9C%EC%BC%93%EC%97%90-%EC%98%AC%EB%9D%BC%ED%83%80%EB%8F%84%EB%A1%9D-%EB%8B%B9%EA%B7%BC-%EC%9A%B4%EC%98%81%EC%8B%A4%EC%9D%B4-ai%EB%A1%9C-%EC%9D%BC%ED%95%98%EB%8A%94-%EB%B2%95-b8aaa6713cea\">모두가 AI 로켓에 올라타도록, 당근 운영실이 AI로 일하는 법</a> was originally published in <a href=\"https://medium.com/daangn\">당근 테크 블로그</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "content:encodedSnippet": "모두가 AI 로켓에 올라타도록, 당근 운영실이 AI로 일하는 법 — 당근 AI Show & Tell #2\n당근은 매주 ‘AI Show & Tell’을 통해 각 팀의 AI 실험을 전사적으로 공유해요. AI를 업무에 어떻게 적용하고 있는지, 그 과정에서 어떤 시행착오와 인사이트가 있었는지 가감 없이 나누죠. 당근은 완벽한 정답을 찾기보다 먼저 과감하게 실행하며, 새로운 시대의 문제 해결 방식을 빠르게 찾아가고 있어요. AI로 만드는 생생한 도전의 순간들, 지금 만나보세요.\n✍️ 이 콘텐츠는 생성형 AI를 활용해 제작한 콘텐츠입니다.\n서비스 운영실 팀 내부 세션 ‘Nextstep for Service Operation’ 발표 자료\n단순히 팀에 AI를 도입한다고 해서 곧바로 혁신을 만들어낼 수 있을까요? 기술은 어디까지나 수단일 뿐이에요. 새로운 기술을 ‘무엇을 위해’, ‘어떻게’ 활용할지에 대한 체계가 뒷받침되지 않으면, 변화는 새로운 기술을 한두 번 시도해 보는 수준에서 그칠 수 있어요. AI로 유의미한 성과를 지속적으로 만들기 위해선 문제 해결 방식과 협업 구조까지 조직 전체가 AI에 맞게 바뀌어야 해요.\n최근 당근 AI Show & Tell 세션에서 서비스 운영실 리더 Brent와 운영실 ML Engineer 리더 Aio는 “운영실이 AI로 일하는 방식”을 공유했어요. 운영실은 사용자 접점의 최전선에서 사용자 문제 해결에 집요하게 몰입하는 팀이에요. 그렇기 때문에 AI를 도입할 때도 어떻게 해야 더 빠르고 정확하게 사용자 문제를 해결할 수 있을지 고민하며, 팀의 실행과 학습 속도를 극대화할 수 있도록 조직 구조와 일하는 방식을 재설계했어요.\n이 글에서는 당근 운영실이 AI 전환을 위해 어떻게 실행 문화를 세우고, 조직 구조를 실험적으로 바꿔 나갔는지, 그리고 어떻게 직군의 경계를 넘어 팀 모두가 함께 몰입할 수 있었는지 그 과정을 전하려고 해요.\nPart 1. 기반을 다지다 — 방향과 실행 문화의 세팅\n운영실의 AI 전환은 팀의 비전을 함께 점검하며 목표를 명확히 설정하는 데서 시작됐어요. “사용자 문제를 해결해 사용자 만족으로 연결하겠다”라는 운영실의 목표는 예나 지금이나 같고, AI 시대에도 변하지 않을 거란 점을 다시 확인했는데요. 다만 AI라는 강력한 수단을 잘 활용하면, 이전과는 비교할 수 없을 만큼 빠르게 목표를 성취할 수 있다는 데 모두가 공감했어요. 그렇게 팀 전체가 AI로 과감하게 시도하며 그 구체적인 방법을 찾아가 보자는 데 뜻을 모았죠.\n\n특히 운영실 리더들은 한 달 동안, 많게는 하루에 한 번씩, 최소 일주일에 한 번은 모여 실행 전략을 구체화했어요. AI 프로젝트의 방향과 분기별 OKR, 북미 대상 제품 전략까지 논의하며, 운영실의 다양한 프로덕트 중 어떤 영역을 AI로 혁신할 수 있을지 리스트업하고 우선순위를 재정비했죠. 이 치열한 정렬 과정 덕분에 팀원들은 방향을 잃지 않고, 함께 한 방향을 바라볼 수 있게 됐어요.\n이후 구성원들이 그 방향으로 힘 있게 나아갈 수 있도록, 구성원들에게 심리적 안정감을 주려 했어요. AI는 빠르게 실험하며 최적화하는 과정이 중요한데, ‘실패하면 어쩌지’라는 두려움이 앞서면 시작조차 어렵거든요. 그래서 운영실에서는 “틀려도 된다”, “실패에서도 배울 수 있다”, “실행 자체가 의미 있다”는 말들을 슬랙과 회의에서 반복하며, 시도를 격려하는 분위기를 일관되게 만들어갔어요. 그러자 회고 자리에서 “AI로 과감한 실행을 해보고 싶어 졌다”라는 피드백이 나올 정도로 실행에 대한 기대감이 팀 안에 자리잡기 시작했어요.\nPart 2. 구조를 바꾸다 — 워킹그룹으로의 조직적 피봇\n이런 문화적 기반 위에서 운영실은 실제 일하는 구조를 바꿨어요. 기존에는 운영실 내 다양한 파트가 기능적으로 나뉘어 있었는데요. ‘사용자가 겪는 문제를 빠르게 해결하자’는 목표는 같았지만, 한 파트는 시나리오 기반의 문제 해결을, 다른 파트는 챗봇과 FAQ 모델 활용을 고려하는 등 접근 방식이 달랐어요. 문제 해결에 대한 관점이 분산되다 보니, 조율을 위한 커뮤니케이션 비용이 커졌어요. 특히 반복적인 실행과 빠른 정렬이 필요한 AI 기술을 도입할 때, 이런 구조는 실행 속도를 늦추는 장애물이 됐죠.\n다양한 파트의 팀원들이 섞여 프로젝트별로 결성된 워킹그룹\n문제를 해결하기 위해 운영실은 익숙했던 파트 구조를 내려놓고 워킹그룹 체계를 도입했어요. 동일한 문제를 푸는 사람들을 파트와 무관하게 한 그룹으로 묶고, 실제 오피스 자리까지 서로 가깝게 재배치했죠. 그러자 분산되어 있던 AI 기능도 하나의 목표 아래 빠르게 실험할 수 있게 됐어요. 각 워킹그룹은 하나의 AI 프로덕트를 담당해 집중하고, 매주 월요일마다 데모를 팀 내부에 공유하며 개선점을 빠르게 정리했어요. 실행과 피드백을 빠르게 오가는 사이클을 통해 시도는 반복되고 학습은 가속화됐어요.\n이 실행 루틴을 뒷받침하기 위해 운영실은 팀의 리소스를 과감히 재배치했어요. 팀 내 자원이 한정된 상황에서 구성원들이 실행에 몰입할 수 있도록 한 거죠. 예를 들어 AI 효과가 상대적으로 적은 기능성 프로젝트는 최소 동작 상태로만 유지하고, VOC 파이프라인 프로젝트*처럼 AI 적용 효과가 큰 영역은 완전 자동화 중심으로 피봇했어요. 이렇게 무엇을 멈추고 어디에 몰입할지를 명확히 정리한 덕에 실행의 밀도를 크게 높일 수 있었어요.\nVOC 파이프라인 프로젝트: 챗봇 기반으로 문제를 해결하고 VOC를 프로덕트 팀에 연결하는 프로젝트\n물론 이 과정에서 중요한 기존 프로젝트들을 놓치지 않는 것도 필요했어요. 기존에 운영실이 진행해 오던 핵심 프로젝트들은 종료 일정을 명확히 설정해 병행 중이고, 완성 이후에는 인력을 워킹그룹 체계로 재배치할 예정이에요. 지금 가장 중요한 문제 해결에 집중할 수 있도록 과거의 업무와 새로운 시도를 병렬로 정렬한 전략이었는데요. 기존의 기반을 유지하면서도 새로운 시도에 치열하게 몰입한 운영 방식은, 실행 중심 AI 전환의 좋은 사례가 되었어요.\nPart 3. 몰입을 확산하다 — 직군을 뛰어넘는 AI 몰입과 실행\n그다음 단계는 기술의 문턱을 허무는 일이었어요. 운영실은 AI 전환의 속도를 높이기 위해, 개발자 중심의 실행에 머물지 않겠다는 원칙을 세웠어요. AI가 특정 직군의 도구로 한정되면 팀 전체의 몰입도는 낮아질 수밖에 없다고 본 거죠. 그래서 비개발자 구성원들도 Cursor 같은 LLM 도구를 직접 실무에 활용할 수 있도록 환경을 설계했어요. 예를 들어 중고거래 게시글의 가품 여부를 판단하는 실험에서는, 운영 매니저가 엔지니어와 함께 LLM에 테이블 스키마만 제공한 뒤, SQL 쿼리 생성을 요청하고 결과를 시각화하는 실습을 진행했어요.\n\n이런 경험을 통해 문제 해결에 AI가 유용하다는 체감이 생기자, 비개발자 구성원들의 태도도 빠르게 바뀌었어요. ‘AI가 할 수 있을까?’라는 의심은 ‘이건 내가 AI로 풀어야 할 문제다’라는 책임감으로 전환됐죠. 그러자 자발적인 학습 문화도 빠르게 자리 잡았어요. 퇴근 후에도 커서로 실습을 해보거나, 서로 사용법을 나누는 분위기가 형성됐죠. 리더는 “개발자도 아닌 우리도 할 수 있다”라고 독려했고, 실제로 Cursor뿐 아니라 다양한 LLM 도구를 활용한 크고 작은 실험이 자연스럽게 퍼졌어요.\n결국 운영실은 ‘AI를 도입한 팀’이 아니라 ‘AI로 일하는 팀’으로 변화해 갔어요. 기술 역량에 상관없이 모두가 문제를 정의하고, 실행하고, 다시 정렬하는 리듬을 갖게 되었죠. AI는 특정 직군의 언어가 아닌, 문제를 함께 푸는 공동의 언어가 되었고요. 이 변화는 모두가 AI 전환의 로켓에 올라탔기 때문에 가능했고, ‘누구나 실행할 수 있다’는 믿음이 만든 결과였어요.\n당근 운영실의 AI 전환은 단순한 기술 도입이 아니었어요. 문화에서 시작해 구조를 바꾸고, 실행을 모두의 일상으로 확장한 과정이었어요. 팀의 비전을 다시 정비하고, 실행을 주저하지 않는 문화를 함께 설계하며, 문제 중심으로 일하는 구조를 과감히 재편했어요. 여기에 직군을 가리지 않는 몰입과 학습, 빠른 실행과 피드백의 루틴이 더해지며, 문제 해결 방식도 계속해서 고도화됐어요.\n운영실의 전환은 끝이 아니라 시작이에요. 이제 이 몰입의 문화가 다른 팀들로도 확산되고 있어요. 다음 AI Show & Tell에서도 이 몰입의 문화가 어떻게 더 다양한 직군과 팀으로 확산되고, 전사적으로 어떤 실행의 연결고리를 만들어가고 있는지를 보여드리도록 할게요. 실행이 문화를 만들고, 문화가 더 큰 실행을 낳는 이 전환의 흐름을 함께 지켜봐 주세요.\n당근에서 함께 AI 전환에 몰입하고 싶다면?\n👉 당근 채용 공고 바로 가기\n\n모두가 AI 로켓에 올라타도록, 당근 운영실이 AI로 일하는 법 was originally published in 당근 테크 블로그 on Medium, where people are continuing the conversation by highlighting and responding to this story.",
    "dc:creator": "당근",
    "guid": "https://medium.com/p/b8aaa6713cea",
    "categories": [
      "ai",
      "working",
      "culture",
      "team-building"
    ],
    "isoDate": "2025-04-11T06:11:35.000Z"
  },
  {
    "creator": "identity16",
    "title": "의존성 그래프를 활용한 프로젝트 시각화 — 사이드 이펙트 한눈에 파악하기",
    "link": "https://medium.com/daangn/%EC%9D%98%EC%A1%B4%EC%84%B1-%EA%B7%B8%EB%9E%98%ED%94%84%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%9C-%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8-%EC%8B%9C%EA%B0%81%ED%99%94-%EC%82%AC%EC%9D%B4%EB%93%9C-%EC%9D%B4%ED%8E%99%ED%8A%B8-%ED%95%9C%EB%88%88%EC%97%90-%ED%8C%8C%EC%95%85%ED%95%98%EA%B8%B0-eec17d5aabb2?source=rss----4505f82a2dbd---4",
    "pubDate": "Thu, 10 Apr 2025 06:26:29 GMT",
    "content:encoded": "<h3>의존성 그래프를 활용한 프로젝트 시각화 — 사이드 이펙트 한눈에 파악하기</h3><p>안녕하세요! 당근의 동네 지도 탭에서 확인할 수 있는 동네 가게 화면과 동네 사장님들이 가게를 관리하기 위한 비즈니스 도구를 만드는 로컬 비즈니스의 Frontend Engineer 준(Joon)이에요.</p><p>혹시 ‘줄줄이 고구마’라는 말을 들어보신 적 있으신가요? 고구마 하나를 캐니, 그 뿌리에 달린 다른 고구마들이 몇 십 개나 딸려 나왔다는 건데요. 고구마가 70개나 달린 줄줄이 고구마 사진을 뉴스에서 처음 봤을 때, 저는 제가 작업하던 코드가 생각나더라고요.</p><p>많은 엔지니어들이 그러하듯, 저도 개발에 착수하기 전에 일정을 산정하는데요. 정작 작업하다 보면 예상치 못한 사이드 이펙트가 ‘하나 캐니 줄줄줄!’ 나오는 상황이 발생하곤 하죠. 그럴 때마다 ‘고구마 뿌리 뽑듯이 계속 나와서요.. 🥺’라고 공유드리며 일정을 미룬 적이 한두 번이\u001c아니에요. 이러면 팀 차원의 일정이 미뤄지기 때문에 그 해결 방법을 고민하기 시작했어요.</p><p>결과적으로 저희 팀은 의존성 그래프를 활용해 프로젝트를 시각화함으로써 코드 파악에 소모되는 시간을 단축했어요. 덕분에 팀에서는 불필요한 리소스 소모 없이 더 빠른 실행이 가능해졌는데요. 이 글에서는 그 구체적인 과정을 소개해 드리려고 해요. 본격적인 작업에 들어가기 전 기존 코드 파악에 시간을 많이 쏟는 게 고민이시라면, 저희의 사례가 업무의 생산성을 높이는 데 큰 도움이 될 수 있을 거예요.</p><h3>문제를 정의해 보자</h3><p>해결에 앞서 5 whys 기법을 통해 문제의 본질적인 원인을 찾아보려고 했어요.</p><p><strong>Q1: 사이드 이펙트를 왜 뒤늦게 발견하는가?</strong></p><p>A: 일정 산정 시점에는 보이지 않았는데, 코드 작업할 때 눈에 띄는 경우가 많았어요.</p><p><strong>Q2: 일정 산정 시점에는 사이드 이펙트를 찾지 못하는가?</strong></p><p>A: 현실적으로 작업 전에 관련된 모든 파일을 다 뒤져보지 못하기 때문이에요.</p><p><strong>Q3: 왜 작업 관련 파일을 다 뒤져보지 못하는가?</strong></p><p>A: 비즈프로필 웹뷰에는 144개의 페이지, 4648개의 파일이 존재해요. 중간중간 재사용되는 파일을 다 뒤지다 보면, 수작업으로 파악하기 어려울 정도로 경우의 수가 커져요.</p><p>5개의 why를 던지지는 않았지만 이쯤에서 저는 본질적인 원인을 파악했다고 판단했어요. <strong>‘너무 많은 파일들의 의존 관계를 다 뒤져보기 힘들다’</strong>는 부분이 해결되면, 예상치 못한 ‘줄줄이 고구마’를 개발 착수 전에 미리 찾아낼 수 있겠다고 생각했죠. 이걸 문제로 정의하고 해결해 보기로 했어요.</p><h3>어떻게 해결하는 게 좋을까?</h3><p>앞에서 정의한 문제는 다음과 같아요.</p><blockquote><strong><em>‘너무 많은 파일들의 의존 관계를 다 뒤져보기 힘들다’</em></strong></blockquote><p>이 문제는 두 가지 방식으로 접근할 수 있을 것 같아요.</p><ol><li><strong>설계 잘하기</strong> : 파일 간의 관계를 명확하게 드러낼 수 있는 폴더 구조 혹은 아키텍처를 정립한다.</li><li><strong>시각화 잘하기</strong> : 현재 파일이 어떤 파일을 import 하고 있는지, 어떤 파일에 import 되고 있는지 한눈에 볼 수 있도록 시각화한다.</li></ol><p>1번은 의존 관계의 복잡도 자체를 근본적으로 낮출 수 있는 방법이에요. 그러나 빠르게 프로덕트를 실험하고 기능을 추가하고자 하는 상황에서 한 가지 구조를 학습시키고 따르게 하는 것은 많은 설득과 시간이 필요할 거예요. 따라서 1번은 장기적으로 가져가야 할 전략이라 판단했고, 당장 지금의 문제를 완화할 수 있는 방안으로 시각화를 잘해보자는 결론에 다다랐어요.</p><p>JS/TS 환경에서 사용할 수 있는 의존성 시각화 도구로 크게 두 가지 정도를 발견했어요.</p><ul><li><a href=\"https://github.com/pahen/madge\">Madge</a></li><li><a href=\"https://github.com/sverweij/dependency-cruiser/tree/main\">Dependency Cruiser</a></li></ul><p>저는 이 중에서 조금 더 문서화가 잘 되어 있는 Dependency Cruiser를 사용하기로 했어요. 이 도구를 사용하면 다음과 같은 의존성 그래프를 그릴 수 있어요.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*sK4g4seXlehCtKRUF2EM7Q.png\" /></figure><p>참고로, 이 글에서는 도구의 사용법은 다루지 않을 예정이에요. 어떤 도구를 사용하든 파일 구조에 따라 정규표현식을 각기 다르게 써주어야 하고, 보고 싶은 정보가 사람, 팀, 상황마다 다를 수 있어요. 그래서 툴보다는 의존성 그래프 활용 사례를 중심으로 소개해 드릴 예정이에요.</p><blockquote><em>Tip: Cursor의 Docs에 추가하거나 GPT에 링크를 넣고 물어보면, 훨씬 빠르게 원하는 옵션을 찾을 수 있어요.</em></blockquote><h3>의존성 그래프를 적용해 보자</h3><p>개발을 하다 보면 코드를 파악해야 하는 상황이 정말 많이 벌어지는데요. 의존성 그래프가 어떻게 코드 파악을 용이하게 만드는지 DashBoard, Profile, Settings 3개의 페이지를 가진 예제 프로젝트를 기준으로 설명드려볼게요.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*IE9QROkgKtmjo_jUXA_Dsw.png\" /><figcaption>예제 프로젝트 — DashBoard 페이지</figcaption></figure><h3>Bottom-Up: 이 파일의 사이드 이펙트를 빠르게 알고 싶어.</h3><p>협업을 하다 보면 종종 디자이너 분들에게서 이런 요청을 받을 때가 있어요.</p><blockquote><em>👩‍🎨(디자이너) : 공통 컴포넌트를 디자인시스템으로 만들기 위해 파악 중이에요. 혹시 카드 컴포넌트가 어디 어디 쓰이는지 알 수 있을까요?</em></blockquote><p>혹은 본격적으로 개발하기 전에 수정해야 할 파일의 사이드 이펙트를 파악해야 하죠.</p><blockquote><em>🧑‍💻(엔지니어) : 이번 작업은 카드 컴포넌트 리팩토링이 수반될 것 같은데, 이 작업이 어느 파일들에 영향을 줄까?</em></blockquote><p>의존성 그래프를 도입하기 전과 후의 작업 과정을 한번 비교해 볼게요.</p><p><strong>Before: 파일을 하나하나 타고 올라가서 파악했어요</strong></p><p>기존에는 위와 같은 요청을 받으면 일단 에디터에서 카드 컴포넌트 파일(Card.tsx)을 여는 걸로 시작했어요. 그 후 Cmd(혹은 Ctrl) 버튼을 누른 채 컴포넌트명을 누르면, 아래 이미지와 같이 이 컴포넌트가 어디에서 import 되는지 목록을 볼 수 있죠.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*YQkER8vjEgBIiMuLpneX1Q.png\" /></figure><p>여기를 보면 다음과 같은 파일들이 보이네요.</p><ul><li>components/.../UserProfile.tsx</li><li>components/.../ActivityList.tsx</li><li>components/.../StatCard.tsx</li><li>components/.../NotificationSettings.tsx</li><li>components/.../UserProfileCard.tsx</li><li>pages/Dashboard.tsx</li><li>pages/Settings.tsx</li></ul><p>일단 pages/Dashboard.tsx, pages/Settings.tsx 두 페이지에서 쓰인다는 사실은 알았으니 나머지 컴포넌트들은 어느 페이지에서 쓰이는지 봐야 해요. UserProfile.tsx를 import하고 있는 파일 목록 안에, 그 파일들을 import 하고 있는 파일 목록을 또 파악하고, 그 과정 끝에 최종적으로 import 하고 있는 페이지 컴포넌트를 확인해야 하죠.</p><p>이 과정을 ActivityList.tsx, StatCard.tsx, NotificationSettings.tsx, UserProfileCard.tsx 각각에 대하여 재귀적으로 다 확인해 본 다음에야 비로소 카드 컴포넌트가 사용되는 영향 범위를 전부 알 수 있어요. 수백, 수천 개의 파일이 서로 의존하고 있는 프로젝트에선 길게는 수시간 이상 걸리기도 하는 과정이에요.</p><p><strong>After: 의존성 그래프를 그려 한눈에 파악해요</strong></p><p>다음 옵션들과 함께 Dependency Cruiser 스크립트 실행하면 이런 그래프를 그릴 수 있어요.</p><ul><li>depcruise src : 타겟 경로로 src를 넣어 전체 소스 코드에 대한 의존성 그래프를 그려요.</li><li>--reaches 옵션으로, 카드 컴포넌트에 도달하는 의존성만 필터링해서 원하는 정보만 남겼어요.</li><li>--highlight 옵션으로, pages와 카드 컴포넌트를 한눈에 알아볼 수 있게 색을 입혔어요.</li><li>--include-only 옵션으로, src 경로에서만 의존성을 파악하도록 했어요. (node_modules 제외)</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*CDSiyltcDCQRHA3iBYtwgQ.png\" /></figure><p>이렇게 생성된 의존성 그래프를 보면, Card 컴포넌트가 사용되고 있는 페이지를 한눈에 파악할 수 있어요. 이제는 스크립트 한번 실행하는 것만으로 파일을 하나하나 찾아볼 시간이 절약되었고, 덤으로 DashLayout, SettingsLayout는 아무 페이지에서도 안 쓰고 있다는 것을 발견했네요.</p><p>또한 실무에서 협업할 때도 확실한 이점을 느꼈는데요. 본 작업 전에 코드 리팩토링 작업이 필요하다는 사실을 비개발 직군 팀원들에게 설득할 때, 복잡하게 그려진 의존성 그래프를 보여주면 문제를 더 직관적으로 이해시킬 수 있게 됐어요.</p><h3>Top-Down: 이 컴포넌트가 어떤 파일에 영향받는지 궁금해.</h3><p>앞에서는 특정 파일이 <strong>‘어디에 사용되는지’</strong>에 대한 궁금증을 해소하기 위해 의존성 그래프의 도움을 받았어요. 이번에는 특정 파일이 <strong>‘무엇을 사용하는지’</strong> 궁금한 경우에 대해서 이야기해보려고 해요.</p><p>대표적으로 프로젝트에서 처음 수정해 보는 페이지를 마주치게 된다면, 다음과 같은 생각이 자연스럽게 따라 나오는 것 같아요.</p><blockquote><em>👨‍💻(엔지니어) : DashBoard 페이지는 처음 건드려보는데, 얼마나 복잡한 페이지일까?</em></blockquote><p>DashBoard 페이지를 작업하기 전에 예상 소요 시간을 파악하기 위해서는 주요 파일들을 한번 훑어보는 과정이 반드시 필요하죠. 의존성 그래프를 도입하기 전후로 이 과정에 어떤 변화가 있었는지 소개드려볼게요.</p><p><strong>Before: 코드를 보고 눈대중으로 중요한 부분만 파악했어요</strong></p><p>일단 pages/Dashboard.tsx 에 들어가서 코드를 훑어본 다음, 중요해 보이는 컴포넌트나 훅이 있다면 해당 파일로 가서 세부 구현을 살펴봐요.</p><p>그렇게 들어간 파일에서도 핵심 로직을 집중적으로 보고, 나머지 코드에서도 중요해 보이는 파일이 있다면 또 넘어가서 보는 것을 반복했어요. 이 페이지가 가진 복잡도가 어느 정도인지 눈대중으로 감을 얻어갔죠.</p><p>이 과정에서 느낀 불편함은 다음과 같아요.</p><ul><li><strong>시간 소모</strong> : 복잡한 페이지일수록 수십 개의 컴포넌트를 일일이 확인해야 하는데, 많게는 몇 시간까지 소요돼요. 게다가 중간중간 질문을 받거나 회의를 다녀오면 집중이 끊기기 때문에 더 많은 시간을 필요로 해요.</li><li><strong>중복 파악 / 누락 가능성</strong> : 중첩된 컴포넌트 구조에서는 일부를 놓치거나 봤던 파일을 또 보기 쉬워요. 또한 추상화로 인해 숨겨진 내부 동작을 모르고 어림짐작으로 넘어간다면, 작업할 때 뒤늦게 발견되는 코드로 인해 일정에 변수가 생기기도 해요.</li><li><strong>일정 산정의 근거 미약 :</strong> 이렇게 파악한 결론은 ‘복잡한 것 같다는 감’이기 때문에, “이 페이지는 다른 데보다 조금 더 복잡해서 일정이 더 필요할 거 같아요” 정도의 두루뭉술한 소통이 일어날 수 있어요. 일정이 지연됐을 때 “막상 작업하다 보니 생각보다 더 복잡해서 일정이 조금 더 필요해요”라고 변명하는 모습은 제가 생각하는 좋은 엔지니어의 모습은 아니었어요.</li></ul><p><strong>After: 하위 의존성을 시각화해, 파일들의 복잡도를 한눈에 파악해요</strong></p><p>Dependency Cruiser는 기본적으로 Top-Down 시각화를 지원해요.</p><p>다음 옵션들과 함께 Dependency Cruiser 스크립트 실행하면 이런 그래프를 그릴 수 있어요.</p><ul><li>depcruise src/pages/Dashboard.tsx : 타겟 경로로 Dashboard 파일을 지정하여 Dashboard 페이지 하위 의존성만 그리도록 했어요.</li><li>--highlight : 대시보드 페이지만 강조하도록 했어요.</li><li>--collapse : node_modules에 있는 라이브러리는 최상위 폴더만 보이도록 설정했어요.</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*cCGdtW12S0FjpnnC-I2hYg.png\" /></figure><p>이렇게 Top-Down 방향으로 하위 의존성을 시각화해 보면, Dashboard 페이지 하위에 어떤 파일들이 연관되어 있는지와 얼마나 복잡하게 의존하고 있는지를 한눈에 볼 수 있어요.</p><p>의존성 그래프로 인해 앞서 말했던 불편함들이 이렇게 해결돼요.</p><ul><li><strong>시간 절약 :</strong> Dashboard 페이지 하위의 파일들을 한눈에 보고, 내가 모르는 부분과 이미 아는 부분을 빠르게 솎아낼 수 있어요. 파악할 코드를 추려내기 위해 위에서부터 하나하나 파일을 타고 내려갈 필요 없이 스크립트를 돌리는 몇 초만 소요하면 돼요.</li><li><strong>중복 파악 / 누락 가능성 감소 :</strong> 파일과 코드를 일일이 텍스트로 보지 않고 관련 파일을 펼쳐놓고 보기 때문에, 봤던 파일을 또 보거나 못 보고 넘어갈 확률이 줄어요.</li><li><strong>일정 산정의 근거로 활용 :</strong> 개발자만 느끼는 ‘복잡도’라는 개념을 시각적으로 보여주면 비개발자도 쉽게 이해할 수 있어요. 왜 A 페이지를 수정하는 게 왜 B 페이지보다 오래 걸리는지 직관적으로 설명할 수 있어요. 그리고 리팩토링은 복잡도를 낮춰 생산성을 올리는 행위라는 걸 AS-IS / TO-BE로 비교하며 보여줄 수도 있어요.</li></ul><h3>Code Review: Pull Request에 의존성 그래프 추가</h3><p>이번에는 코드리뷰 시점에 의존성 그래프를 적용한 시도를 소개해볼게요.</p><p>Github Actions를 이용해 Pull Request에 포함된 File Changes를 기준으로 영향이 가는 범위를 시각화하는 스크립트를 추가했어요. 이 작업으로 인해 저장소에 기여자의 PR이 올라오면, 다음과 같은 시각화가 자동으로 달리게 돼요.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*AN7g7lw-smjkyDo333EDag.png\" /><figcaption>실제 Pull Request에 생성된 의존성 그래프</figcaption></figure><p>기존에는 다른 개발자의 코드를 리뷰할 때 파일 하나, 코드 한 줄이라는 텍스트에 집중했다면, 의존성 시각화를 통해 이 사람이 <strong>얼마나 큰 작업을 했는지,</strong> 수정한 파일의 <strong>사이드 이펙트가 얼마나 큰지를 알 수 있게 되었어요.</strong></p><p>또한 의존성 그래프가 있으면 리뷰어는 다음과 같은 코드리뷰를 통해서 코드베이스 전체의 복잡도까지 관리할 수 있게 돼요.</p><blockquote><em>🧑‍💻(리뷰어) : X 파일 수정하셨는데, 작업과 관련 없는 A/B/C 페이지에도 영향이 가는 것 같아요. 예상치 못한 사이드 이펙트가 없도록 리팩토링 먼저 하고 변경사항 적용하면 어떨까요?</em></blockquote><h3>Team: 프로젝트 전체 구조 한눈에 훑어보기</h3><p>프로젝트의 신규 기여자를 온보딩할 때, 일반적으로 코드를 살펴보는 시간을 충분히 가지게 하죠.</p><p>매일 많은 변경이 일어나는 거대한 프로젝트를 제대로 이해하려면, 그 어떤 문서보다 실제로 돌아가는 코드를 확인하는 게 중요해요. 하지만 수천 개의 파일이 담긴 프로젝트 폴더를 열어보는 순간 어디서부터 봐야 할지 막막해지죠.</p><p>이런 막막함을 느껴본 적이 있으시다면 프로젝트 전체를 의존성 그래프로 그려서 지도처럼 표현해 보는 방식을 소개드려요. 다음과 같은 그림이나 스크립트를 기여 문서에 추가한다면, 처음 기여하는 사람들이 조금 더 원활하게 온보딩할 수 있어요.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*sK4g4seXlehCtKRUF2EM7Q.png\" /><figcaption>예제 프로젝트의 전체 의존성 그래프</figcaption></figure><p>기본적으로 depcruise src 로 전체 소스코드를 대상으로 그래프를 그릴 수 있고, Depedency Cruiser 옵션을 더 다양하게 알아보고 건드리면서 필요한 수준으로 조정하시는 걸 추천드려요.</p><ul><li>--output-type ddot로 폴더 단위로 그래프를 그려 폴더 구조만 표현할 수 있어요.</li><li>특정 범위의 파일만 보고 싶다면 --include-only로 원하는 영역만 시각화할 수 있어요.</li><li>Rule을 사용하면 순환 참조나 폴더 구조에 대한 규칙을 Lint처럼 걸어두고, 이를 위반한 경우에는 의존성 그래프에 화살표를 강조하는 등의 안내를 줄 수 있어요.</li></ul><h3>결론</h3><p>의존성 그래프를 활용해 프로젝트 시각화한 후, 코드를 파악하느라 정처 없이 여기저기 파일을 열어보며 돌아다니는 시간이 거의 사라졌어요.</p><p>사이드 프로젝트가 아닌 회사 프로젝트에서, 대부분의 개발자는 관리하고 있는 코드의 첫 번째 기여자가 아닌 경우가 많아요. 따라서 코드를 작성하는 것보다 파악하는 시간이 훨씬 길어질 수 있어요.</p><p>특히나 제가 당근에서 마주친 코드들은 엔지니어링과 프로덕트 경험 양면에서의 치열한 고민 흔적들이 많이 묻어있었어요. 지금도 옆에서 다양한 고민을 함께 나누고 시도하는 동료들이 있기 때문에 파악할 코드는 더더욱 빠르게 늘어나고 있고요.</p><p>Copilot, Cursor 등의 AI 도구들이 발전하면서 코드 작성에 대한 생산성은 빠르게 개선되고 있다고 생각하는데요. 이번 글을 읽으신 분들께서는 의존성 시각화라는 도구를 통해 코드를 파악하는 데에 드는 시간까지 줄여서, 더 빠른 실행과 좋은 프로덕트를 만들어가는 데 조금이라도 도움이 되셨으면 좋겠습니다!</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=eec17d5aabb2\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/daangn/%EC%9D%98%EC%A1%B4%EC%84%B1-%EA%B7%B8%EB%9E%98%ED%94%84%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%9C-%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8-%EC%8B%9C%EA%B0%81%ED%99%94-%EC%82%AC%EC%9D%B4%EB%93%9C-%EC%9D%B4%ED%8E%99%ED%8A%B8-%ED%95%9C%EB%88%88%EC%97%90-%ED%8C%8C%EC%95%85%ED%95%98%EA%B8%B0-eec17d5aabb2\">의존성 그래프를 활용한 프로젝트 시각화 — 사이드 이펙트 한눈에 파악하기</a> was originally published in <a href=\"https://medium.com/daangn\">당근 테크 블로그</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "content:encodedSnippet": "의존성 그래프를 활용한 프로젝트 시각화 — 사이드 이펙트 한눈에 파악하기\n안녕하세요! 당근의 동네 지도 탭에서 확인할 수 있는 동네 가게 화면과 동네 사장님들이 가게를 관리하기 위한 비즈니스 도구를 만드는 로컬 비즈니스의 Frontend Engineer 준(Joon)이에요.\n혹시 ‘줄줄이 고구마’라는 말을 들어보신 적 있으신가요? 고구마 하나를 캐니, 그 뿌리에 달린 다른 고구마들이 몇 십 개나 딸려 나왔다는 건데요. 고구마가 70개나 달린 줄줄이 고구마 사진을 뉴스에서 처음 봤을 때, 저는 제가 작업하던 코드가 생각나더라고요.\n많은 엔지니어들이 그러하듯, 저도 개발에 착수하기 전에 일정을 산정하는데요. 정작 작업하다 보면 예상치 못한 사이드 이펙트가 ‘하나 캐니 줄줄줄!’ 나오는 상황이 발생하곤 하죠. 그럴 때마다 ‘고구마 뿌리 뽑듯이 계속 나와서요.. 🥺’라고 공유드리며 일정을 미룬 적이 한두 번이\u001c아니에요. 이러면 팀 차원의 일정이 미뤄지기 때문에 그 해결 방법을 고민하기 시작했어요.\n결과적으로 저희 팀은 의존성 그래프를 활용해 프로젝트를 시각화함으로써 코드 파악에 소모되는 시간을 단축했어요. 덕분에 팀에서는 불필요한 리소스 소모 없이 더 빠른 실행이 가능해졌는데요. 이 글에서는 그 구체적인 과정을 소개해 드리려고 해요. 본격적인 작업에 들어가기 전 기존 코드 파악에 시간을 많이 쏟는 게 고민이시라면, 저희의 사례가 업무의 생산성을 높이는 데 큰 도움이 될 수 있을 거예요.\n문제를 정의해 보자\n해결에 앞서 5 whys 기법을 통해 문제의 본질적인 원인을 찾아보려고 했어요.\nQ1: 사이드 이펙트를 왜 뒤늦게 발견하는가?\nA: 일정 산정 시점에는 보이지 않았는데, 코드 작업할 때 눈에 띄는 경우가 많았어요.\nQ2: 일정 산정 시점에는 사이드 이펙트를 찾지 못하는가?\nA: 현실적으로 작업 전에 관련된 모든 파일을 다 뒤져보지 못하기 때문이에요.\nQ3: 왜 작업 관련 파일을 다 뒤져보지 못하는가?\nA: 비즈프로필 웹뷰에는 144개의 페이지, 4648개의 파일이 존재해요. 중간중간 재사용되는 파일을 다 뒤지다 보면, 수작업으로 파악하기 어려울 정도로 경우의 수가 커져요.\n5개의 why를 던지지는 않았지만 이쯤에서 저는 본질적인 원인을 파악했다고 판단했어요. ‘너무 많은 파일들의 의존 관계를 다 뒤져보기 힘들다’는 부분이 해결되면, 예상치 못한 ‘줄줄이 고구마’를 개발 착수 전에 미리 찾아낼 수 있겠다고 생각했죠. 이걸 문제로 정의하고 해결해 보기로 했어요.\n어떻게 해결하는 게 좋을까?\n앞에서 정의한 문제는 다음과 같아요.\n‘너무 많은 파일들의 의존 관계를 다 뒤져보기 힘들다’\n이 문제는 두 가지 방식으로 접근할 수 있을 것 같아요.\n\n설계 잘하기 : 파일 간의 관계를 명확하게 드러낼 수 있는 폴더 구조 혹은 아키텍처를 정립한다.\n시각화 잘하기 : 현재 파일이 어떤 파일을 import 하고 있는지, 어떤 파일에 import 되고 있는지 한눈에 볼 수 있도록 시각화한다.\n\n1번은 의존 관계의 복잡도 자체를 근본적으로 낮출 수 있는 방법이에요. 그러나 빠르게 프로덕트를 실험하고 기능을 추가하고자 하는 상황에서 한 가지 구조를 학습시키고 따르게 하는 것은 많은 설득과 시간이 필요할 거예요. 따라서 1번은 장기적으로 가져가야 할 전략이라 판단했고, 당장 지금의 문제를 완화할 수 있는 방안으로 시각화를 잘해보자는 결론에 다다랐어요.\nJS/TS 환경에서 사용할 수 있는 의존성 시각화 도구로 크게 두 가지 정도를 발견했어요.\n\nMadge\nDependency Cruiser\n\n저는 이 중에서 조금 더 문서화가 잘 되어 있는 Dependency Cruiser를 사용하기로 했어요. 이 도구를 사용하면 다음과 같은 의존성 그래프를 그릴 수 있어요.\n\n참고로, 이 글에서는 도구의 사용법은 다루지 않을 예정이에요. 어떤 도구를 사용하든 파일 구조에 따라 정규표현식을 각기 다르게 써주어야 하고, 보고 싶은 정보가 사람, 팀, 상황마다 다를 수 있어요. 그래서 툴보다는 의존성 그래프 활용 사례를 중심으로 소개해 드릴 예정이에요.\nTip: Cursor의 Docs에 추가하거나 GPT에 링크를 넣고 물어보면, 훨씬 빠르게 원하는 옵션을 찾을 수 있어요.\n의존성 그래프를 적용해 보자\n개발을 하다 보면 코드를 파악해야 하는 상황이 정말 많이 벌어지는데요. 의존성 그래프가 어떻게 코드 파악을 용이하게 만드는지 DashBoard, Profile, Settings 3개의 페이지를 가진 예제 프로젝트를 기준으로 설명드려볼게요.\n예제 프로젝트 — DashBoard 페이지\nBottom-Up: 이 파일의 사이드 이펙트를 빠르게 알고 싶어.\n협업을 하다 보면 종종 디자이너 분들에게서 이런 요청을 받을 때가 있어요.\n👩‍🎨(디자이너) : 공통 컴포넌트를 디자인시스템으로 만들기 위해 파악 중이에요. 혹시 카드 컴포넌트가 어디 어디 쓰이는지 알 수 있을까요?\n혹은 본격적으로 개발하기 전에 수정해야 할 파일의 사이드 이펙트를 파악해야 하죠.\n🧑‍💻(엔지니어) : 이번 작업은 카드 컴포넌트 리팩토링이 수반될 것 같은데, 이 작업이 어느 파일들에 영향을 줄까?\n의존성 그래프를 도입하기 전과 후의 작업 과정을 한번 비교해 볼게요.\nBefore: 파일을 하나하나 타고 올라가서 파악했어요\n기존에는 위와 같은 요청을 받으면 일단 에디터에서 카드 컴포넌트 파일(Card.tsx)을 여는 걸로 시작했어요. 그 후 Cmd(혹은 Ctrl) 버튼을 누른 채 컴포넌트명을 누르면, 아래 이미지와 같이 이 컴포넌트가 어디에서 import 되는지 목록을 볼 수 있죠.\n\n여기를 보면 다음과 같은 파일들이 보이네요.\n\ncomponents/.../UserProfile.tsx\ncomponents/.../ActivityList.tsx\ncomponents/.../StatCard.tsx\ncomponents/.../NotificationSettings.tsx\ncomponents/.../UserProfileCard.tsx\npages/Dashboard.tsx\npages/Settings.tsx\n\n일단 pages/Dashboard.tsx, pages/Settings.tsx 두 페이지에서 쓰인다는 사실은 알았으니 나머지 컴포넌트들은 어느 페이지에서 쓰이는지 봐야 해요. UserProfile.tsx를 import하고 있는 파일 목록 안에, 그 파일들을 import 하고 있는 파일 목록을 또 파악하고, 그 과정 끝에 최종적으로 import 하고 있는 페이지 컴포넌트를 확인해야 하죠.\n이 과정을 ActivityList.tsx, StatCard.tsx, NotificationSettings.tsx, UserProfileCard.tsx 각각에 대하여 재귀적으로 다 확인해 본 다음에야 비로소 카드 컴포넌트가 사용되는 영향 범위를 전부 알 수 있어요. 수백, 수천 개의 파일이 서로 의존하고 있는 프로젝트에선 길게는 수시간 이상 걸리기도 하는 과정이에요.\nAfter: 의존성 그래프를 그려 한눈에 파악해요\n다음 옵션들과 함께 Dependency Cruiser 스크립트 실행하면 이런 그래프를 그릴 수 있어요.\n\ndepcruise src : 타겟 경로로 src를 넣어 전체 소스 코드에 대한 의존성 그래프를 그려요.\n--reaches 옵션으로, 카드 컴포넌트에 도달하는 의존성만 필터링해서 원하는 정보만 남겼어요.\n--highlight 옵션으로, pages와 카드 컴포넌트를 한눈에 알아볼 수 있게 색을 입혔어요.\n--include-only 옵션으로, src 경로에서만 의존성을 파악하도록 했어요. (node_modules 제외)\n\n이렇게 생성된 의존성 그래프를 보면, Card 컴포넌트가 사용되고 있는 페이지를 한눈에 파악할 수 있어요. 이제는 스크립트 한번 실행하는 것만으로 파일을 하나하나 찾아볼 시간이 절약되었고, 덤으로 DashLayout, SettingsLayout는 아무 페이지에서도 안 쓰고 있다는 것을 발견했네요.\n또한 실무에서 협업할 때도 확실한 이점을 느꼈는데요. 본 작업 전에 코드 리팩토링 작업이 필요하다는 사실을 비개발 직군 팀원들에게 설득할 때, 복잡하게 그려진 의존성 그래프를 보여주면 문제를 더 직관적으로 이해시킬 수 있게 됐어요.\nTop-Down: 이 컴포넌트가 어떤 파일에 영향받는지 궁금해.\n앞에서는 특정 파일이 ‘어디에 사용되는지’에 대한 궁금증을 해소하기 위해 의존성 그래프의 도움을 받았어요. 이번에는 특정 파일이 ‘무엇을 사용하는지’ 궁금한 경우에 대해서 이야기해보려고 해요.\n대표적으로 프로젝트에서 처음 수정해 보는 페이지를 마주치게 된다면, 다음과 같은 생각이 자연스럽게 따라 나오는 것 같아요.\n👨‍💻(엔지니어) : DashBoard 페이지는 처음 건드려보는데, 얼마나 복잡한 페이지일까?\nDashBoard 페이지를 작업하기 전에 예상 소요 시간을 파악하기 위해서는 주요 파일들을 한번 훑어보는 과정이 반드시 필요하죠. 의존성 그래프를 도입하기 전후로 이 과정에 어떤 변화가 있었는지 소개드려볼게요.\nBefore: 코드를 보고 눈대중으로 중요한 부분만 파악했어요\n일단 pages/Dashboard.tsx 에 들어가서 코드를 훑어본 다음, 중요해 보이는 컴포넌트나 훅이 있다면 해당 파일로 가서 세부 구현을 살펴봐요.\n그렇게 들어간 파일에서도 핵심 로직을 집중적으로 보고, 나머지 코드에서도 중요해 보이는 파일이 있다면 또 넘어가서 보는 것을 반복했어요. 이 페이지가 가진 복잡도가 어느 정도인지 눈대중으로 감을 얻어갔죠.\n이 과정에서 느낀 불편함은 다음과 같아요.\n\n시간 소모 : 복잡한 페이지일수록 수십 개의 컴포넌트를 일일이 확인해야 하는데, 많게는 몇 시간까지 소요돼요. 게다가 중간중간 질문을 받거나 회의를 다녀오면 집중이 끊기기 때문에 더 많은 시간을 필요로 해요.\n중복 파악 / 누락 가능성 : 중첩된 컴포넌트 구조에서는 일부를 놓치거나 봤던 파일을 또 보기 쉬워요. 또한 추상화로 인해 숨겨진 내부 동작을 모르고 어림짐작으로 넘어간다면, 작업할 때 뒤늦게 발견되는 코드로 인해 일정에 변수가 생기기도 해요.\n일정 산정의 근거 미약 : 이렇게 파악한 결론은 ‘복잡한 것 같다는 감’이기 때문에, “이 페이지는 다른 데보다 조금 더 복잡해서 일정이 더 필요할 거 같아요” 정도의 두루뭉술한 소통이 일어날 수 있어요. 일정이 지연됐을 때 “막상 작업하다 보니 생각보다 더 복잡해서 일정이 조금 더 필요해요”라고 변명하는 모습은 제가 생각하는 좋은 엔지니어의 모습은 아니었어요.\n\nAfter: 하위 의존성을 시각화해, 파일들의 복잡도를 한눈에 파악해요\nDependency Cruiser는 기본적으로 Top-Down 시각화를 지원해요.\n다음 옵션들과 함께 Dependency Cruiser 스크립트 실행하면 이런 그래프를 그릴 수 있어요.\n\ndepcruise src/pages/Dashboard.tsx : 타겟 경로로 Dashboard 파일을 지정하여 Dashboard 페이지 하위 의존성만 그리도록 했어요.\n--highlight : 대시보드 페이지만 강조하도록 했어요.\n--collapse : node_modules에 있는 라이브러리는 최상위 폴더만 보이도록 설정했어요.\n\n이렇게 Top-Down 방향으로 하위 의존성을 시각화해 보면, Dashboard 페이지 하위에 어떤 파일들이 연관되어 있는지와 얼마나 복잡하게 의존하고 있는지를 한눈에 볼 수 있어요.\n의존성 그래프로 인해 앞서 말했던 불편함들이 이렇게 해결돼요.\n\n시간 절약 : Dashboard 페이지 하위의 파일들을 한눈에 보고, 내가 모르는 부분과 이미 아는 부분을 빠르게 솎아낼 수 있어요. 파악할 코드를 추려내기 위해 위에서부터 하나하나 파일을 타고 내려갈 필요 없이 스크립트를 돌리는 몇 초만 소요하면 돼요.\n중복 파악 / 누락 가능성 감소 : 파일과 코드를 일일이 텍스트로 보지 않고 관련 파일을 펼쳐놓고 보기 때문에, 봤던 파일을 또 보거나 못 보고 넘어갈 확률이 줄어요.\n일정 산정의 근거로 활용 : 개발자만 느끼는 ‘복잡도’라는 개념을 시각적으로 보여주면 비개발자도 쉽게 이해할 수 있어요. 왜 A 페이지를 수정하는 게 왜 B 페이지보다 오래 걸리는지 직관적으로 설명할 수 있어요. 그리고 리팩토링은 복잡도를 낮춰 생산성을 올리는 행위라는 걸 AS-IS / TO-BE로 비교하며 보여줄 수도 있어요.\n\nCode Review: Pull Request에 의존성 그래프 추가\n이번에는 코드리뷰 시점에 의존성 그래프를 적용한 시도를 소개해볼게요.\nGithub Actions를 이용해 Pull Request에 포함된 File Changes를 기준으로 영향이 가는 범위를 시각화하는 스크립트를 추가했어요. 이 작업으로 인해 저장소에 기여자의 PR이 올라오면, 다음과 같은 시각화가 자동으로 달리게 돼요.\n실제 Pull Request에 생성된 의존성 그래프\n기존에는 다른 개발자의 코드를 리뷰할 때 파일 하나, 코드 한 줄이라는 텍스트에 집중했다면, 의존성 시각화를 통해 이 사람이 얼마나 큰 작업을 했는지, 수정한 파일의 사이드 이펙트가 얼마나 큰지를 알 수 있게 되었어요.\n또한 의존성 그래프가 있으면 리뷰어는 다음과 같은 코드리뷰를 통해서 코드베이스 전체의 복잡도까지 관리할 수 있게 돼요.\n🧑‍💻(리뷰어) : X 파일 수정하셨는데, 작업과 관련 없는 A/B/C 페이지에도 영향이 가는 것 같아요. 예상치 못한 사이드 이펙트가 없도록 리팩토링 먼저 하고 변경사항 적용하면 어떨까요?\nTeam: 프로젝트 전체 구조 한눈에 훑어보기\n프로젝트의 신규 기여자를 온보딩할 때, 일반적으로 코드를 살펴보는 시간을 충분히 가지게 하죠.\n매일 많은 변경이 일어나는 거대한 프로젝트를 제대로 이해하려면, 그 어떤 문서보다 실제로 돌아가는 코드를 확인하는 게 중요해요. 하지만 수천 개의 파일이 담긴 프로젝트 폴더를 열어보는 순간 어디서부터 봐야 할지 막막해지죠.\n이런 막막함을 느껴본 적이 있으시다면 프로젝트 전체를 의존성 그래프로 그려서 지도처럼 표현해 보는 방식을 소개드려요. 다음과 같은 그림이나 스크립트를 기여 문서에 추가한다면, 처음 기여하는 사람들이 조금 더 원활하게 온보딩할 수 있어요.\n예제 프로젝트의 전체 의존성 그래프\n기본적으로 depcruise src 로 전체 소스코드를 대상으로 그래프를 그릴 수 있고, Depedency Cruiser 옵션을 더 다양하게 알아보고 건드리면서 필요한 수준으로 조정하시는 걸 추천드려요.\n\n--output-type ddot로 폴더 단위로 그래프를 그려 폴더 구조만 표현할 수 있어요.\n특정 범위의 파일만 보고 싶다면 --include-only로 원하는 영역만 시각화할 수 있어요.\nRule을 사용하면 순환 참조나 폴더 구조에 대한 규칙을 Lint처럼 걸어두고, 이를 위반한 경우에는 의존성 그래프에 화살표를 강조하는 등의 안내를 줄 수 있어요.\n\n결론\n의존성 그래프를 활용해 프로젝트 시각화한 후, 코드를 파악하느라 정처 없이 여기저기 파일을 열어보며 돌아다니는 시간이 거의 사라졌어요.\n사이드 프로젝트가 아닌 회사 프로젝트에서, 대부분의 개발자는 관리하고 있는 코드의 첫 번째 기여자가 아닌 경우가 많아요. 따라서 코드를 작성하는 것보다 파악하는 시간이 훨씬 길어질 수 있어요.\n특히나 제가 당근에서 마주친 코드들은 엔지니어링과 프로덕트 경험 양면에서의 치열한 고민 흔적들이 많이 묻어있었어요. 지금도 옆에서 다양한 고민을 함께 나누고 시도하는 동료들이 있기 때문에 파악할 코드는 더더욱 빠르게 늘어나고 있고요.\nCopilot, Cursor 등의 AI 도구들이 발전하면서 코드 작성에 대한 생산성은 빠르게 개선되고 있다고 생각하는데요. 이번 글을 읽으신 분들께서는 의존성 시각화라는 도구를 통해 코드를 파악하는 데에 드는 시간까지 줄여서, 더 빠른 실행과 좋은 프로덕트를 만들어가는 데 조금이라도 도움이 되셨으면 좋겠습니다!\n\n의존성 그래프를 활용한 프로젝트 시각화 — 사이드 이펙트 한눈에 파악하기 was originally published in 당근 테크 블로그 on Medium, where people are continuing the conversation by highlighting and responding to this story.",
    "dc:creator": "identity16",
    "guid": "https://medium.com/p/eec17d5aabb2",
    "categories": [
      "programming",
      "dependency-graph",
      "productivity"
    ],
    "isoDate": "2025-04-10T06:26:29.000Z"
  },
  {
    "creator": "Lebron J",
    "title": "Feed-Entity: 당근 피드의 심장",
    "link": "https://medium.com/daangn/feed-entity-%EB%8B%B9%EA%B7%BC-%ED%94%BC%EB%93%9C%EC%9D%98-%EC%8B%AC%EC%9E%A5-e2ba0a7f57fa?source=rss----4505f82a2dbd---4",
    "pubDate": "Thu, 03 Apr 2025 06:05:58 GMT",
    "content:encoded": "<p>안녕하세요. 저는 당근 피드인프라팀에서 Software Engineer로 일하고 있는 Lebron이라고 해요.</p><p>피드인프라팀은 하루에 수백만 명의 사용자들이 당근 앱을 열었을 때 가장 먼저 마주하게 되는 피드 경험을 담당해요. 각 사용자의 관심사에 맞는 맞춤형 콘텐츠를 적절한 위치에 제공하기 위해 복잡한 피드 시스템을 운영하며, 대규모 트래픽을 안정적으로 처리하기 위한 인프라를 구축하고 있어요.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*NzOkY-yE5l8JKcV0-LmT2g.png\" /><figcaption>중고차, 부동산 등 다양한 콘텐츠가 있는 피드</figcaption></figure><p>위 이미지와 같이 당근의 피드에는 다양한 콘텐츠가 존재하며, 이들은 여러 맥락으로 연결되어 서빙돼요.</p><p>피드인프라팀은 다양한 콘텐츠를 서빙하면서 “어떻게 하면 더 일관성 있게 데이터를 저장하고 활용할 수 있을까?”라는 고민을 여러 번 마주했어요. Feed-Entity는 바로 그 고민에서 시작한 프로젝트예요. Feed-Entity는 단순히 데이터 구조를 표준화하는 것을 넘어서, 당근의 피드 시스템이 더욱 확장 가능하고 유연한 형태로 발전하도록 했어요. 덕분에 피드에 새로운 서비스를 더 빠르게 통합하고, 사용자들에게 더 다양하고 풍부한 콘텐츠를 제공할 수 있었어요.</p><p>지금부터 Feed-Entity가 어떻게 탄생했는지, 그리고 이를 통해 어떤 문제들을 해결했는지 이야기해 드릴게요.</p><h3>Feed-Entity의 탄생</h3><p>Feed-Entity가 등장하기 전, 피드 시스템은 다소 분산된 형태로 운영되었어요. 당근 내 여러 서비스(중고거래, 중고차, 당근알바 등)가 각자의 콘텐츠를 피드에 노출시키기 위해 독립적인 저장소를 운영하거나, 피드 시스템과의 직접적인 연동을 통해 데이터를 제공했어요.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*T8r-PqOv2jkZDszdjhtWeA.png\" /><figcaption>Feed-Entity 등장 이전 피드 시스템</figcaption></figure><p>예를 들어 중고거래 서비스는 자체 데이터베이스에 상품 정보를 저장하고 있었고, 알바 서비스는 별도의 저장소에 구인구직 정보를 보관했어요. 피드 시스템은 여러 저장소에서 데이터를 가져와 사용자에게 보여주는 역할을 했어요.</p><p>이런 구조는 초기에는 단순하고 직관적이었어요. 그러나 당근의 서비스가 다양해지고 규모가 커지면서 여러 가지 한계점을 드러내기 시작했어요. 각 서비스마다 데이터 구조와 저장 방식이 달랐고, 새로운 서비스를 피드에 추가할 때마다 많은 통합 작업이 필요했어요.</p><p>이러한 문제들을 해결하기 위해 우리는 Feed-Entity라는 개념을 도입했어요. Feed-Entity를 통해 이루고자 했던 목표들은 다음과 같아요.</p><ul><li>데이터 구조의 표준화: 각 서비스마다 달랐던 데이터 구조와 통신 방식을 표준화하여 시스템 간 일관된 인터페이스를 제공하고 싶었어요.</li><li>시스템 확장성 개선: 새로운 서비스를 피드에 더 쉽고 빠르게 추가할 수 있게 만들고 싶었어요.</li><li>데이터 일관성 확보: 다양한 콘텐츠를 더 일관성 있게 다루고 싶었어요.</li><li>통합 관리 시스템 구축: 중고거래, 알바, 중고차, 부동산, 동네생활 등 모든 서비스의 콘텐츠를 한 곳에서 관리하고 싶었어요.</li><li>사용자 경험 향상: 결과적으로 사용자들에게 더 다양하고 풍부한 콘텐츠를 보여주고 싶었어요.</li></ul><h3>Feed-Entity: Single Source of Truth for Feed System</h3><p>Feed-Entity는 피드에서 노출 가능한, 당근 내에서 발행할 수 있는 가장 작은 단위의 콘텐츠로, 피드 시스템 안에서의 Single Source of Truth(SSOT)로 정의한 단위를 의미해요.</p><ul><li>콘텐츠 자체를 나타내는 요소들만 Feed-Entity에 정의해요.</li><li>중고거래 게시글, 동네생활* 게시글, 동네생활 댓글, 비즈프로필** 등이 Feed-Entity가 될 수 있어요.</li><li>사진, 텍스트, 관심 수나 채팅 수와 같은 속성은 Feed-Entity가 될 수 없어요.</li></ul><blockquote><em>💡</em> *동네생활: 동네에 대한 이야기와 정보를 나눌 수 있는 커뮤니티<br><em>💡</em> **비즈프로필: 동네 업체에 대한 정보를 담고 있는 프로필</blockquote><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/950/1*6525aP0EqMml9NtYlf7WLA.png\" /><figcaption>Feed-Entity로 변환되는 예시</figcaption></figure><p>위 그림처럼 다양한 콘텐츠(중고거래, 알바, 부동산 등)를 Feed-Entity라는 가공된 형태로 변환하여 저장하게 돼요. Feed-Entity는 기본적으로 다음과 같은 구성 요소를 가져요.</p><ul><li>ID: Feed-Entity를 고유하게 구분할 수 있는 식별자예요.</li><li>타입: 중고거래, 알바, 동네생활 등 어떤 종류의 콘텐츠인지를 나타내요.</li><li>소스 정보: SourceContent*의 출처와 관련된 정보를 담아요.</li><li>생성자: 콘텐츠를 작성한 사용자에 대한 정보를 담아요.</li><li>생성/수정/노출 시간: 콘텐츠가 언제 만들어지고 수정되었는지에 대한 정보를 나타내요.</li><li>상태: Feed-Entity가 생성되었는지, 노출 중인지, 미노출 상태인지와 같은 상태 정보를 나타내요.</li><li>Entity: 변환된 콘텐츠 정보를 담아요.</li></ul><p>Feed-Entity는 이러한 표준화된 구조를 통해 다양한 서비스의 콘텐츠를 일관되게 관리할 수 있게 했어요. 덕분에 새로운 서비스를 피드에 통합하는 과정이 훨씬 간단해졌고, 개발자들은 데이터 구조보다는 비즈니스 로직에 더 집중할 수 있게 되었어요.</p><blockquote><em>💡</em> *SourceContent: Feed-Entity로 변환하기 전의 원본 콘텐츠 데이터</blockquote><h3>Feed-Entity Data Pipeline</h3><p>Feed-Entity의 데이터 파이프라인은 콘텐츠 제공자(Content Provider)로부터 데이터를 수집하고, 이를 Feed-Entity 형태로 변환하여 저장하는 과정을 담당해요. 이 파이프라인은 크게 데이터 수집, 변환, 저장의 단계로 구성돼요. 각 단계는 모듈화되어 있어 새로운 콘텐츠 타입이 추가되더라도 전체 시스템을 수정하지 않고 해당 모듈만 추가하면 되는 유연한 구조를 가져요.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*n4QAs9mfEbTMeJCFn2JRnQ.png\" /><figcaption>Feed-Entity 데이터 파이프라인</figcaption></figure><p>위 이미지는 Feed-Entity의 데이터 파이프라인을 상세하게 보여주고 있어요. 이 파이프라인은 크게 네 단계로 구성되어 있는데, 각 단계별로 자세히 살펴보겠습니다.</p><h4>1. 데이터 수집</h4><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*eB7HXxELvL4h7pWuA6UyHw.png\" /><figcaption>데이터 수집 흐름</figcaption></figure><p>첫 번째 단계는 데이터 수집 과정이에요. 중고거래, 알바, 동네생활, 부동산 등 여러 서비스에서 생성된 원본 데이터가 Feed-Entity 시스템으로 유입됩니다. 각 서비스마다 데이터 구조와 형식이 다르기 때문에, 이 단계에서는 다양한 형태의 데이터를 처리할 수 있는 유연한 수집 메커니즘이 필요해요.</p><p>Feed-Entity 시스템은 이러한 유연한 수집 메커니즘을 위해 데이터 유형별로 별도의 큐를 운영해요. 각 큐에서 데이터를 처리할 때는 멱등성(Idempotency)을 보장하도록 설계했어요. 덕분에 같은 SourceContent가 여러 번 전달되더라도 단 한 번만 처리돼요. 또한 실시간 데이터 스트림을 통해 새로운 콘텐츠나 업데이트된 콘텐츠를 지속적으로 수집하여 사용자에게 항상 최신 정보를 제공할 수 있어요.</p><h4>2. 데이터 변환</h4><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*cnMvdXJSPcMOHPKcHB3_7Q.png\" /><figcaption>데이터 변환 흐름</figcaption></figure><p>두 번째 단계는 수집된 다양한 형태의 데이터를 표준화된 Feed-Entity 형식으로 변환하는 과정이에요. 이 과정에서 각 콘텐츠 타입에 맞는 변환 로직이 적용돼요. 예를 들어 중고거래 게시글은 상품 정보, 가격, 위치 등의 필드를 가지는 반면, 동네생활 게시글은 본문 내용, 카테고리, 댓글 수 등 다른 구조를 가져요. 변환 단계에서는 이러한 다양한 구조의 데이터들을 공통 필드(ID, 타입, 생성 시간 등)를 가진 Feed-Entity라는 통일된 형식으로 래핑하면서도, 각 콘텐츠의 고유한 특성은 Entity 필드 내부에 보존하여 변환해요.</p><p>변환된 Feed-Entity 데이터는 별도의 저장소에 저장되어 피드 시스템 전체에서 일관되게 접근할 수 있게 돼요. 모듈화된 시스템 구조 덕분에 새로운 콘텐츠 타입이 추가되더라도, 전체 시스템을 수정하지 않고 해당 모듈만 추가하면 되는 유연한 구조를 갖추고 있어요. 이는 시스템의 확장성을 크게 향상시키고, 새로운 서비스를 빠르게 통합할 수 있게 해주는 핵심 장점이에요.</p><h4>3. 데이터 검증 및 DLQ 처리</h4><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*tICI53T-E8vdH5iFyAvzHg.png\" /><figcaption>데이터 검증 및 DLQ 처리 흐름</figcaption></figure><p>세 번째 단계는 데이터 검증 과정이에요. SourceContent나 Feed-Entity 데이터가 검증에 실패할 경우, 해당 데이터는 Dead Letter Queue(DLQ)로 이동하게 돼요. DLQ는 처리에 실패한 메시지를 저장해 두는 특별한 큐로, 이를 통해 데이터 손실 없이 나중에 다시 처리할 수 있게 해요.</p><p>예를 들어 원본 데이터의 형식이 변경되었거나 필수 필드가 누락되었을 때, 데이터 변환 과정에서 오류가 발생할 수 있어요. 이런 경우 해당 데이터는 DLQ로 이동하고, 개발자들은 오류의 원인을 분석한 후 필요한 수정을 거쳐 데이터를 다시 처리할 수 있어요. 이러한 메커니즘을 통해 시스템의 안정성을 높이고, 데이터 무결성을 보장했어요.</p><p>또한 DLQ 시스템은 문제가 발생한 데이터를 자동으로 모니터링하고 알림을 보내는 기능도 포함하고 있어요. 이를 통해 개발팀이 빠르게 문제를 인지하고 대응할 수 있게 되었어요. 이는 데이터 파이프라인의 신뢰성을 높이고, 장애 상황에서도 데이터 손실을 최소화할 수 있게 했어요.</p><h4>4. 메시지 큐 프로듀스 및 서비스 간 연동</h4><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Fopd_bQJ5-GsKENZalG9Zw.png\" /><figcaption>메시지 생성 및 서비스 간 연동 흐름</figcaption></figure><p>마지막 단계는 변환된 Feed-Entity 데이터를 별도의 메시지 큐에 프로듀스(Produce)하는 과정이에요. 이를 통해 피드 시스템 외의 다른 서비스들도 이 표준화된 데이터를 활용할 수 있게 되었어요. 다양한 내부 서비스들이 Feed-Entity를 구독(Subscribe)하여 필요한 정보를 실시간으로 받아볼 수 있게 되었어요. 이런 구조는 서비스 간 데이터 일관성을 유지하고, 각 서비스가 독립적으로 발전할 수 있는 기반이 되었어요.</p><p>또한 처음 Feed-Entity 시스템을 도입할 때는 콜드 스타트(Cold Start) 문제를 해결하기 위한 백필(Backfill) 기능도 중요했어요. 백필은 기존에 존재하던 콘텐츠들을 새로운 Feed-Entity 형식으로 변환하여 시스템에 채워 넣는 과정이에요. 이 과정을 통해 새 시스템을 도입하는 순간부터 충분한 양의 데이터를 확보하고, 사용자들에게 풍부한 콘텐츠를 제공할 수 있었어요.</p><p>이러한 메시지 큐 기반의 아키텍처는 시스템 간 느슨한 결합(Loose Coupling)을 가능하게 하여, 각 서비스가 독립적으로 개발, 배포, 확장될 수 있게 해요. 또한 비동기 처리 방식을 통해 시스템의 부하를 분산시키고, 전체 시스템의 안정성과 확장성을 높이는 데 크게 기여해요.</p><h3>Feed-Entity Serving Flow</h3><p>Feed-Entity를 잘 저장한 것만큼 이를 효율적으로 서빙하는 것도 정말 중요해요. 특히 피드는 쓰기보다 읽기 트래픽이 훨씬 많다는 특성이 있어서, 읽기에 최적화된 구조로 데이터를 저장하고 적절한 캐싱 전략을 설계하는 것이 필수예요.</p><p>피드인프라팀에서는 Feed-Entity 데이터를 효율적으로 서빙하기 위해 지역별 특성을 고려한 Redis 캐싱 전략을 구현했어요. Feed-Entity는 지역성이 강한 특징이 있기 때문에, 지역별로 다른 캐시를 구성하여 데이터 접근 속도를 크게 향상시켰어요. 사용자가 특정 지역의 피드를 요청하면, 해당 지역에 최적화된 캐시에서 데이터를 빠르게 가져와요.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ZBMVSOxXD8_2sl5S81LjDw.png\" /><figcaption>Feed-Entity 서빙 흐름</figcaption></figure><p>Feed-Entity 서빙 시스템은 효율적인 데이터 접근을 위해 다단계 저장 및 조회 구조를 갖추고 있어요. 이 시스템은 크게 Redis 캐싱 레이어와 데이터베이스 저장소의 두 계층으로 구성되어 있어요.</p><h4>데이터 저장 프로세스</h4><p>Feed-Entity가 생성되거나 업데이트되면, 다음과 같은 과정을 거쳐요:</p><ul><li><strong>데이터베이스 저장:</strong> 모든 Feed-Entity는 우선 영구 저장소인 데이터베이스에 저장됩니다. 이는 데이터의 영구성과 내구성을 보장하기 위함이에요.</li><li><strong>인덱스 생성:</strong> 저장과 동시에 해당 Feed-Entity의 메타데이터와 지역 정보를 포함한 인덱스가 생성됩니다. 이 인덱스는 빠른 검색과 필터링을 위한 핵심 자료구조예요.</li><li><strong>레디스 캐시 업데이트:</strong> 생성된 인덱스는 지역별로 구분된 Redis 캐시에 저장됩니다. 각 지역마다 별도의 캐시 버킷을 유지하여 지역 기반 쿼리의 성능을 최적화했어요.</li></ul><h4>데이터 조회 프로세스</h4><p>사용자가 특정 지역의 피드를 요청하면, 시스템은 다음과 같은 단계로 데이터를 효율적으로 조회해요:</p><ul><li><strong>캐시 조회 (Cache Hit):</strong> 먼저 해당 지역의 Redis 캐시에서 인덱스를 조회합니다. 이때 인덱스에는 Feed-Entity ID와 기본 메타데이터만 포함되어 있어 메모리 사용을 최소화하면서도 빠른 응답 속도를 제공해요.</li><li><strong>데이터베이스 조회 최소화:</strong> 캐시에서 인덱스를 찾으면(Cache Hit), 해당 인덱스를 기반으로 필요한 Feed-Entity만 선택적으로 캐시 혹은 데이터베이스에서 가져옵니다. 이를 통해 전체 데이터베이스 스캔 없이도 필요한 데이터만 효율적으로 접근할 수 있어요.</li><li><strong>캐시 미스 처리 (Cache Miss):</strong> 만약 캐시에서 해당 지역의 인덱스를 찾지 못하면(Cache Miss), 시스템은 데이터베이스를 직접 조회하여 필요한 Feed-Entity를 검색해요.</li></ul><h4>성능 최적화</h4><p>이러한 다층 구조는 여러 성능 이점을 가져왔어요:</p><ul><li><strong>읽기 트래픽 최적화:</strong> 피드 시스템은 특성상 쓰기보다 읽기 작업이 압도적으로 많은데, 인덱스 기반 캐싱을 통해 읽기 성능을 크게 향상시켰어요.</li><li><strong>지역별 부하 분산:</strong> 지역별로 별도의 캐시를 관리함으로써 특정 지역의 트래픽이 증가하더라도 다른 지역의 성능에 영향을 미치지 않도록 했어요.</li></ul><p>이렇게 설계된 Feed-Entity 서빙 시스템은 수백만 사용자의 지역 기반 피드 요청을 매우 낮은 지연 시간(p99 기준 평균 20ms 이하)으로 처리해요. 특히 사용자가 많은 대도시 지역에서도 안정적인 성능을 유지하며 사용자 경험을 크게 개선했어요.</p><h3>Feed-Entity의 장점과 효과</h3><p>Feed-Entity를 도입하면서 저희 팀은 여러 가지 중요한 개선 효과를 얻을 수 있었어요. 앞서 Feed-Entity를 통해 이루고 싶은 목표를 다섯 가지 측면에서 정리했었는데요. 각 측면에서 경험한 구체적인 장점들은 아래와 같아요.</p><ul><li><strong>데이터 구조의 표준화:</strong> 이전에는 서로 다른 형식의 데이터를 처리하기 위해 각 서비스마다 별도의 로직이 필요했지만, Feed-Entity를 통해 모든 콘텐츠를 일관된 형식으로 관리하면서 개발 복잡성이 크게 줄어들었어요.</li><li><strong>시스템 확장성 개선:</strong> 기존에는 새로운 콘텐츠 타입을 추가할 때마다 피드 시스템 전체를 수정해야 했지만, Feed-Entity 도입 후에는 해당 콘텐츠에 대한 변환 모듈만 추가하면 되는 구조가 되었어요. 이런 유연한 아키텍처 덕분에 빠르게 변화하는 비즈니스 요구사항에 신속하게 대응할 수 있게 되었어요.</li><li><strong>데이터 일관성 확보:</strong> DLQ 처리 메커니즘을 통해 데이터 손실 없이 오류 상황을 효과적으로 관리하고, 표준화된 스키마를 통해 데이터 품질을 일관되게 유지할 수 있어요. 이는 서비스 안정성 향상과 운영 부담 감소로 이어졌어요.</li><li><strong>통합 관리 시스템 구축:</strong> 서비스 간 연동이 훨씬 용이해졌어요. Feed-Entity를 통해 이제는 한 곳에서 모든 콘텐츠를 관리할 수 있어요. 새로운 서비스를 추가하거나 기존 서비스를 수정할 때, 표준화된 인터페이스 덕분에 전체 시스템에 미치는 영향이 최소화되었어요. 이런 구조적 이점은 개발 속도와 효율성을 크게 향상시켰고, 팀 간 협업도 더 원활하게 만들었어요.</li><li><strong>사용자 경험 향상:</strong> 다양한 콘텐츠 타입(중고거래, 알바, 부동산, 동네생활 등)을 통합적으로 관리하게 되면서, 사용자에게 더 풍부하고 개인화된 피드 경험을 제공할 수 있어요. 사용자의 관심사와 위치에 기반한 맞춤형 콘텐츠를 보여줄 수 있게 되었어요.</li></ul><p>이러한 여러 장점들이 모여 피드 시스템은 Feed-Entity를 통해 더 안정적이고, 확장 가능한 시스템을 구축할 수 있었어요.</p><h3>Feed-Entity NEXT</h3><p>Feed-Entity는 현재 당근의 피드 시스템에서 핵심적인 역할을 담당하고 있지만, 아직 해결해야 할 여러 과제가 있어요. 지속적인 사용자 증가와 함께 콘텐츠의 다양성도 늘어나면서, 데이터 처리 효율성과 확장성에 대한 새로운 요구사항이 계속해서 등장하고 있어요. 또한 사용자별 맞춤형 경험을 더욱 세밀하게 제공하기 위해서는 Feed-Entity 시스템의 고도화가 필요한 상황이에요. 이러한 도전 과제들을 해결하기 위해 저희 팀은 다음과 같은 방향으로 개선을 계획하고 있어요.</p><ul><li><strong>복합적 인덱싱 전략:</strong> 현재는 단순히 최신순 지역별 큐만 가지고 있지만, 이것만으로는 다양한 요구를 충족시키기에 부족해요. 향후에는 거리순, 카테고리별, 인기도별, 최신순 등 다양한 인덱싱 전략을 구현하여 멀티 큐 시스템을 운영할 필요성이 있어요. 이를 통해 사용자가 원하는 방식으로 콘텐츠를 빠르게 탐색할 수 있게 될 거예요. 특히 지리적 거리에 따른 인덱싱은 하이퍼로컬 서비스인 당근의 특성을 더욱 강화할 수 있는 중요한 요소예요. 또한 카테고리별 인덱싱은 사용자가 관심 있는 특정 영역의 콘텐츠만 효율적으로 찾을 수 있게 도와줄 거예요. 이러한 복합적 인덱싱 전략을 구현함으로써 Feed-Entity의 서빙 속도와 정확도를 크게 향상시킬 수 있을 것으로 기대해요.</li><li><strong>추천 모델학습 활용:</strong> Feed-Entity는 아직 추천 시스템 전반에 적용되지 않아요. 이를 추천 모델 학습과 서빙에 활용한다면, 사용자별 선호도 예측의 정확도를 높이고 개인화된 피드 경험을 제공할 수 있을 것이에요. 또한 통일된 데이터 구조를 기반으로 ML 파이프라인을 구축하면 모델 업데이트와 학습 프로세스를 더욱 효율적으로 관리할 수 있을 거라 기대돼요.</li><li><strong>탐색 시스템의 SSOT 확장:</strong> 현재 Feed-Entity는 해시태그 시스템과 추천 알림에서 SSOT(Single Source of Truth)로 성공적으로 활용되고 있어요. 이러한 성공을 바탕으로 더 많은 시스템으로 Feed-Entity의 활용 범위를 확장할 계획이에요. 이를 통해 전체 서비스에서 일관된 데이터 구조와 높은 신뢰성을 확보할 수 있을 것으로 기대돼요.</li></ul><p>이러한 과제들을 해결하여 Feed-Entity를 추천 전반에 활용할 수 있게 된다면, 당근의 피드 시스템은 한 단계 더 발전된 형태로 사용자들에게 가치 있는 경험을 제공할 수 있을 것이라고 믿어요.</p><h3>마치며…</h3><p>이렇게 피드라는 복잡한 문제를 해결하기 위해 Feed-Entity라는 개념을 도입하고 발전시켜 온 피드인프라팀. 저희는 항상 더 나은 기술과 사용자 경험을 위해 끊임없이 도전하고 있어요. 혹시 이런 기술적 도전에 함께하고 싶으신가요? 당근 피드인프라팀에서는 수백만 명의 사용자들에게 하이퍼로컬 가치 있는 정보를 효율적으로 전달하기 위한 기술을 개발하고 다양한 실험에 도전할 열정적인 동료를 찾고 있어요.</p><p>당근 피드인프라팀과 함께 하이퍼로컬 피드 경험을 만들어 갈 동료를 기다립니다.</p><p>당근마켓 채용 페이지: <a href=\"https://team.daangn.com/jobs/\">https://team.daangn.com/jobs/</a></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e2ba0a7f57fa\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/daangn/feed-entity-%EB%8B%B9%EA%B7%BC-%ED%94%BC%EB%93%9C%EC%9D%98-%EC%8B%AC%EC%9E%A5-e2ba0a7f57fa\">Feed-Entity: 당근 피드의 심장</a> was originally published in <a href=\"https://medium.com/daangn\">당근 테크 블로그</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "content:encodedSnippet": "안녕하세요. 저는 당근 피드인프라팀에서 Software Engineer로 일하고 있는 Lebron이라고 해요.\n피드인프라팀은 하루에 수백만 명의 사용자들이 당근 앱을 열었을 때 가장 먼저 마주하게 되는 피드 경험을 담당해요. 각 사용자의 관심사에 맞는 맞춤형 콘텐츠를 적절한 위치에 제공하기 위해 복잡한 피드 시스템을 운영하며, 대규모 트래픽을 안정적으로 처리하기 위한 인프라를 구축하고 있어요.\n중고차, 부동산 등 다양한 콘텐츠가 있는 피드\n위 이미지와 같이 당근의 피드에는 다양한 콘텐츠가 존재하며, 이들은 여러 맥락으로 연결되어 서빙돼요.\n피드인프라팀은 다양한 콘텐츠를 서빙하면서 “어떻게 하면 더 일관성 있게 데이터를 저장하고 활용할 수 있을까?”라는 고민을 여러 번 마주했어요. Feed-Entity는 바로 그 고민에서 시작한 프로젝트예요. Feed-Entity는 단순히 데이터 구조를 표준화하는 것을 넘어서, 당근의 피드 시스템이 더욱 확장 가능하고 유연한 형태로 발전하도록 했어요. 덕분에 피드에 새로운 서비스를 더 빠르게 통합하고, 사용자들에게 더 다양하고 풍부한 콘텐츠를 제공할 수 있었어요.\n지금부터 Feed-Entity가 어떻게 탄생했는지, 그리고 이를 통해 어떤 문제들을 해결했는지 이야기해 드릴게요.\nFeed-Entity의 탄생\nFeed-Entity가 등장하기 전, 피드 시스템은 다소 분산된 형태로 운영되었어요. 당근 내 여러 서비스(중고거래, 중고차, 당근알바 등)가 각자의 콘텐츠를 피드에 노출시키기 위해 독립적인 저장소를 운영하거나, 피드 시스템과의 직접적인 연동을 통해 데이터를 제공했어요.\nFeed-Entity 등장 이전 피드 시스템\n예를 들어 중고거래 서비스는 자체 데이터베이스에 상품 정보를 저장하고 있었고, 알바 서비스는 별도의 저장소에 구인구직 정보를 보관했어요. 피드 시스템은 여러 저장소에서 데이터를 가져와 사용자에게 보여주는 역할을 했어요.\n이런 구조는 초기에는 단순하고 직관적이었어요. 그러나 당근의 서비스가 다양해지고 규모가 커지면서 여러 가지 한계점을 드러내기 시작했어요. 각 서비스마다 데이터 구조와 저장 방식이 달랐고, 새로운 서비스를 피드에 추가할 때마다 많은 통합 작업이 필요했어요.\n이러한 문제들을 해결하기 위해 우리는 Feed-Entity라는 개념을 도입했어요. Feed-Entity를 통해 이루고자 했던 목표들은 다음과 같아요.\n\n데이터 구조의 표준화: 각 서비스마다 달랐던 데이터 구조와 통신 방식을 표준화하여 시스템 간 일관된 인터페이스를 제공하고 싶었어요.\n시스템 확장성 개선: 새로운 서비스를 피드에 더 쉽고 빠르게 추가할 수 있게 만들고 싶었어요.\n데이터 일관성 확보: 다양한 콘텐츠를 더 일관성 있게 다루고 싶었어요.\n통합 관리 시스템 구축: 중고거래, 알바, 중고차, 부동산, 동네생활 등 모든 서비스의 콘텐츠를 한 곳에서 관리하고 싶었어요.\n사용자 경험 향상: 결과적으로 사용자들에게 더 다양하고 풍부한 콘텐츠를 보여주고 싶었어요.\n\nFeed-Entity: Single Source of Truth for Feed System\nFeed-Entity는 피드에서 노출 가능한, 당근 내에서 발행할 수 있는 가장 작은 단위의 콘텐츠로, 피드 시스템 안에서의 Single Source of Truth(SSOT)로 정의한 단위를 의미해요.\n\n콘텐츠 자체를 나타내는 요소들만 Feed-Entity에 정의해요.\n중고거래 게시글, 동네생활* 게시글, 동네생활 댓글, 비즈프로필** 등이 Feed-Entity가 될 수 있어요.\n사진, 텍스트, 관심 수나 채팅 수와 같은 속성은 Feed-Entity가 될 수 없어요.\n\n💡 *동네생활: 동네에 대한 이야기와 정보를 나눌 수 있는 커뮤니티\n💡 **비즈프로필: 동네 업체에 대한 정보를 담고 있는 프로필\nFeed-Entity로 변환되는 예시\n위 그림처럼 다양한 콘텐츠(중고거래, 알바, 부동산 등)를 Feed-Entity라는 가공된 형태로 변환하여 저장하게 돼요. Feed-Entity는 기본적으로 다음과 같은 구성 요소를 가져요.\n\nID: Feed-Entity를 고유하게 구분할 수 있는 식별자예요.\n타입: 중고거래, 알바, 동네생활 등 어떤 종류의 콘텐츠인지를 나타내요.\n소스 정보: SourceContent*의 출처와 관련된 정보를 담아요.\n생성자: 콘텐츠를 작성한 사용자에 대한 정보를 담아요.\n생성/수정/노출 시간: 콘텐츠가 언제 만들어지고 수정되었는지에 대한 정보를 나타내요.\n상태: Feed-Entity가 생성되었는지, 노출 중인지, 미노출 상태인지와 같은 상태 정보를 나타내요.\nEntity: 변환된 콘텐츠 정보를 담아요.\n\nFeed-Entity는 이러한 표준화된 구조를 통해 다양한 서비스의 콘텐츠를 일관되게 관리할 수 있게 했어요. 덕분에 새로운 서비스를 피드에 통합하는 과정이 훨씬 간단해졌고, 개발자들은 데이터 구조보다는 비즈니스 로직에 더 집중할 수 있게 되었어요.\n💡 *SourceContent: Feed-Entity로 변환하기 전의 원본 콘텐츠 데이터\nFeed-Entity Data Pipeline\nFeed-Entity의 데이터 파이프라인은 콘텐츠 제공자(Content Provider)로부터 데이터를 수집하고, 이를 Feed-Entity 형태로 변환하여 저장하는 과정을 담당해요. 이 파이프라인은 크게 데이터 수집, 변환, 저장의 단계로 구성돼요. 각 단계는 모듈화되어 있어 새로운 콘텐츠 타입이 추가되더라도 전체 시스템을 수정하지 않고 해당 모듈만 추가하면 되는 유연한 구조를 가져요.\nFeed-Entity 데이터 파이프라인\n위 이미지는 Feed-Entity의 데이터 파이프라인을 상세하게 보여주고 있어요. 이 파이프라인은 크게 네 단계로 구성되어 있는데, 각 단계별로 자세히 살펴보겠습니다.\n1. 데이터 수집\n데이터 수집 흐름\n첫 번째 단계는 데이터 수집 과정이에요. 중고거래, 알바, 동네생활, 부동산 등 여러 서비스에서 생성된 원본 데이터가 Feed-Entity 시스템으로 유입됩니다. 각 서비스마다 데이터 구조와 형식이 다르기 때문에, 이 단계에서는 다양한 형태의 데이터를 처리할 수 있는 유연한 수집 메커니즘이 필요해요.\nFeed-Entity 시스템은 이러한 유연한 수집 메커니즘을 위해 데이터 유형별로 별도의 큐를 운영해요. 각 큐에서 데이터를 처리할 때는 멱등성(Idempotency)을 보장하도록 설계했어요. 덕분에 같은 SourceContent가 여러 번 전달되더라도 단 한 번만 처리돼요. 또한 실시간 데이터 스트림을 통해 새로운 콘텐츠나 업데이트된 콘텐츠를 지속적으로 수집하여 사용자에게 항상 최신 정보를 제공할 수 있어요.\n2. 데이터 변환\n데이터 변환 흐름\n두 번째 단계는 수집된 다양한 형태의 데이터를 표준화된 Feed-Entity 형식으로 변환하는 과정이에요. 이 과정에서 각 콘텐츠 타입에 맞는 변환 로직이 적용돼요. 예를 들어 중고거래 게시글은 상품 정보, 가격, 위치 등의 필드를 가지는 반면, 동네생활 게시글은 본문 내용, 카테고리, 댓글 수 등 다른 구조를 가져요. 변환 단계에서는 이러한 다양한 구조의 데이터들을 공통 필드(ID, 타입, 생성 시간 등)를 가진 Feed-Entity라는 통일된 형식으로 래핑하면서도, 각 콘텐츠의 고유한 특성은 Entity 필드 내부에 보존하여 변환해요.\n변환된 Feed-Entity 데이터는 별도의 저장소에 저장되어 피드 시스템 전체에서 일관되게 접근할 수 있게 돼요. 모듈화된 시스템 구조 덕분에 새로운 콘텐츠 타입이 추가되더라도, 전체 시스템을 수정하지 않고 해당 모듈만 추가하면 되는 유연한 구조를 갖추고 있어요. 이는 시스템의 확장성을 크게 향상시키고, 새로운 서비스를 빠르게 통합할 수 있게 해주는 핵심 장점이에요.\n3. 데이터 검증 및 DLQ 처리\n데이터 검증 및 DLQ 처리 흐름\n세 번째 단계는 데이터 검증 과정이에요. SourceContent나 Feed-Entity 데이터가 검증에 실패할 경우, 해당 데이터는 Dead Letter Queue(DLQ)로 이동하게 돼요. DLQ는 처리에 실패한 메시지를 저장해 두는 특별한 큐로, 이를 통해 데이터 손실 없이 나중에 다시 처리할 수 있게 해요.\n예를 들어 원본 데이터의 형식이 변경되었거나 필수 필드가 누락되었을 때, 데이터 변환 과정에서 오류가 발생할 수 있어요. 이런 경우 해당 데이터는 DLQ로 이동하고, 개발자들은 오류의 원인을 분석한 후 필요한 수정을 거쳐 데이터를 다시 처리할 수 있어요. 이러한 메커니즘을 통해 시스템의 안정성을 높이고, 데이터 무결성을 보장했어요.\n또한 DLQ 시스템은 문제가 발생한 데이터를 자동으로 모니터링하고 알림을 보내는 기능도 포함하고 있어요. 이를 통해 개발팀이 빠르게 문제를 인지하고 대응할 수 있게 되었어요. 이는 데이터 파이프라인의 신뢰성을 높이고, 장애 상황에서도 데이터 손실을 최소화할 수 있게 했어요.\n4. 메시지 큐 프로듀스 및 서비스 간 연동\n메시지 생성 및 서비스 간 연동 흐름\n마지막 단계는 변환된 Feed-Entity 데이터를 별도의 메시지 큐에 프로듀스(Produce)하는 과정이에요. 이를 통해 피드 시스템 외의 다른 서비스들도 이 표준화된 데이터를 활용할 수 있게 되었어요. 다양한 내부 서비스들이 Feed-Entity를 구독(Subscribe)하여 필요한 정보를 실시간으로 받아볼 수 있게 되었어요. 이런 구조는 서비스 간 데이터 일관성을 유지하고, 각 서비스가 독립적으로 발전할 수 있는 기반이 되었어요.\n또한 처음 Feed-Entity 시스템을 도입할 때는 콜드 스타트(Cold Start) 문제를 해결하기 위한 백필(Backfill) 기능도 중요했어요. 백필은 기존에 존재하던 콘텐츠들을 새로운 Feed-Entity 형식으로 변환하여 시스템에 채워 넣는 과정이에요. 이 과정을 통해 새 시스템을 도입하는 순간부터 충분한 양의 데이터를 확보하고, 사용자들에게 풍부한 콘텐츠를 제공할 수 있었어요.\n이러한 메시지 큐 기반의 아키텍처는 시스템 간 느슨한 결합(Loose Coupling)을 가능하게 하여, 각 서비스가 독립적으로 개발, 배포, 확장될 수 있게 해요. 또한 비동기 처리 방식을 통해 시스템의 부하를 분산시키고, 전체 시스템의 안정성과 확장성을 높이는 데 크게 기여해요.\nFeed-Entity Serving Flow\nFeed-Entity를 잘 저장한 것만큼 이를 효율적으로 서빙하는 것도 정말 중요해요. 특히 피드는 쓰기보다 읽기 트래픽이 훨씬 많다는 특성이 있어서, 읽기에 최적화된 구조로 데이터를 저장하고 적절한 캐싱 전략을 설계하는 것이 필수예요.\n피드인프라팀에서는 Feed-Entity 데이터를 효율적으로 서빙하기 위해 지역별 특성을 고려한 Redis 캐싱 전략을 구현했어요. Feed-Entity는 지역성이 강한 특징이 있기 때문에, 지역별로 다른 캐시를 구성하여 데이터 접근 속도를 크게 향상시켰어요. 사용자가 특정 지역의 피드를 요청하면, 해당 지역에 최적화된 캐시에서 데이터를 빠르게 가져와요.\nFeed-Entity 서빙 흐름\nFeed-Entity 서빙 시스템은 효율적인 데이터 접근을 위해 다단계 저장 및 조회 구조를 갖추고 있어요. 이 시스템은 크게 Redis 캐싱 레이어와 데이터베이스 저장소의 두 계층으로 구성되어 있어요.\n데이터 저장 프로세스\nFeed-Entity가 생성되거나 업데이트되면, 다음과 같은 과정을 거쳐요:\n\n데이터베이스 저장: 모든 Feed-Entity는 우선 영구 저장소인 데이터베이스에 저장됩니다. 이는 데이터의 영구성과 내구성을 보장하기 위함이에요.\n인덱스 생성: 저장과 동시에 해당 Feed-Entity의 메타데이터와 지역 정보를 포함한 인덱스가 생성됩니다. 이 인덱스는 빠른 검색과 필터링을 위한 핵심 자료구조예요.\n레디스 캐시 업데이트: 생성된 인덱스는 지역별로 구분된 Redis 캐시에 저장됩니다. 각 지역마다 별도의 캐시 버킷을 유지하여 지역 기반 쿼리의 성능을 최적화했어요.\n\n데이터 조회 프로세스\n사용자가 특정 지역의 피드를 요청하면, 시스템은 다음과 같은 단계로 데이터를 효율적으로 조회해요:\n\n캐시 조회 (Cache Hit): 먼저 해당 지역의 Redis 캐시에서 인덱스를 조회합니다. 이때 인덱스에는 Feed-Entity ID와 기본 메타데이터만 포함되어 있어 메모리 사용을 최소화하면서도 빠른 응답 속도를 제공해요.\n데이터베이스 조회 최소화: 캐시에서 인덱스를 찾으면(Cache Hit), 해당 인덱스를 기반으로 필요한 Feed-Entity만 선택적으로 캐시 혹은 데이터베이스에서 가져옵니다. 이를 통해 전체 데이터베이스 스캔 없이도 필요한 데이터만 효율적으로 접근할 수 있어요.\n캐시 미스 처리 (Cache Miss): 만약 캐시에서 해당 지역의 인덱스를 찾지 못하면(Cache Miss), 시스템은 데이터베이스를 직접 조회하여 필요한 Feed-Entity를 검색해요.\n\n성능 최적화\n이러한 다층 구조는 여러 성능 이점을 가져왔어요:\n\n읽기 트래픽 최적화: 피드 시스템은 특성상 쓰기보다 읽기 작업이 압도적으로 많은데, 인덱스 기반 캐싱을 통해 읽기 성능을 크게 향상시켰어요.\n지역별 부하 분산: 지역별로 별도의 캐시를 관리함으로써 특정 지역의 트래픽이 증가하더라도 다른 지역의 성능에 영향을 미치지 않도록 했어요.\n\n이렇게 설계된 Feed-Entity 서빙 시스템은 수백만 사용자의 지역 기반 피드 요청을 매우 낮은 지연 시간(p99 기준 평균 20ms 이하)으로 처리해요. 특히 사용자가 많은 대도시 지역에서도 안정적인 성능을 유지하며 사용자 경험을 크게 개선했어요.\nFeed-Entity의 장점과 효과\nFeed-Entity를 도입하면서 저희 팀은 여러 가지 중요한 개선 효과를 얻을 수 있었어요. 앞서 Feed-Entity를 통해 이루고 싶은 목표를 다섯 가지 측면에서 정리했었는데요. 각 측면에서 경험한 구체적인 장점들은 아래와 같아요.\n\n데이터 구조의 표준화: 이전에는 서로 다른 형식의 데이터를 처리하기 위해 각 서비스마다 별도의 로직이 필요했지만, Feed-Entity를 통해 모든 콘텐츠를 일관된 형식으로 관리하면서 개발 복잡성이 크게 줄어들었어요.\n시스템 확장성 개선: 기존에는 새로운 콘텐츠 타입을 추가할 때마다 피드 시스템 전체를 수정해야 했지만, Feed-Entity 도입 후에는 해당 콘텐츠에 대한 변환 모듈만 추가하면 되는 구조가 되었어요. 이런 유연한 아키텍처 덕분에 빠르게 변화하는 비즈니스 요구사항에 신속하게 대응할 수 있게 되었어요.\n데이터 일관성 확보: DLQ 처리 메커니즘을 통해 데이터 손실 없이 오류 상황을 효과적으로 관리하고, 표준화된 스키마를 통해 데이터 품질을 일관되게 유지할 수 있어요. 이는 서비스 안정성 향상과 운영 부담 감소로 이어졌어요.\n통합 관리 시스템 구축: 서비스 간 연동이 훨씬 용이해졌어요. Feed-Entity를 통해 이제는 한 곳에서 모든 콘텐츠를 관리할 수 있어요. 새로운 서비스를 추가하거나 기존 서비스를 수정할 때, 표준화된 인터페이스 덕분에 전체 시스템에 미치는 영향이 최소화되었어요. 이런 구조적 이점은 개발 속도와 효율성을 크게 향상시켰고, 팀 간 협업도 더 원활하게 만들었어요.\n사용자 경험 향상: 다양한 콘텐츠 타입(중고거래, 알바, 부동산, 동네생활 등)을 통합적으로 관리하게 되면서, 사용자에게 더 풍부하고 개인화된 피드 경험을 제공할 수 있어요. 사용자의 관심사와 위치에 기반한 맞춤형 콘텐츠를 보여줄 수 있게 되었어요.\n\n이러한 여러 장점들이 모여 피드 시스템은 Feed-Entity를 통해 더 안정적이고, 확장 가능한 시스템을 구축할 수 있었어요.\nFeed-Entity NEXT\nFeed-Entity는 현재 당근의 피드 시스템에서 핵심적인 역할을 담당하고 있지만, 아직 해결해야 할 여러 과제가 있어요. 지속적인 사용자 증가와 함께 콘텐츠의 다양성도 늘어나면서, 데이터 처리 효율성과 확장성에 대한 새로운 요구사항이 계속해서 등장하고 있어요. 또한 사용자별 맞춤형 경험을 더욱 세밀하게 제공하기 위해서는 Feed-Entity 시스템의 고도화가 필요한 상황이에요. 이러한 도전 과제들을 해결하기 위해 저희 팀은 다음과 같은 방향으로 개선을 계획하고 있어요.\n\n복합적 인덱싱 전략: 현재는 단순히 최신순 지역별 큐만 가지고 있지만, 이것만으로는 다양한 요구를 충족시키기에 부족해요. 향후에는 거리순, 카테고리별, 인기도별, 최신순 등 다양한 인덱싱 전략을 구현하여 멀티 큐 시스템을 운영할 필요성이 있어요. 이를 통해 사용자가 원하는 방식으로 콘텐츠를 빠르게 탐색할 수 있게 될 거예요. 특히 지리적 거리에 따른 인덱싱은 하이퍼로컬 서비스인 당근의 특성을 더욱 강화할 수 있는 중요한 요소예요. 또한 카테고리별 인덱싱은 사용자가 관심 있는 특정 영역의 콘텐츠만 효율적으로 찾을 수 있게 도와줄 거예요. 이러한 복합적 인덱싱 전략을 구현함으로써 Feed-Entity의 서빙 속도와 정확도를 크게 향상시킬 수 있을 것으로 기대해요.\n추천 모델학습 활용: Feed-Entity는 아직 추천 시스템 전반에 적용되지 않아요. 이를 추천 모델 학습과 서빙에 활용한다면, 사용자별 선호도 예측의 정확도를 높이고 개인화된 피드 경험을 제공할 수 있을 것이에요. 또한 통일된 데이터 구조를 기반으로 ML 파이프라인을 구축하면 모델 업데이트와 학습 프로세스를 더욱 효율적으로 관리할 수 있을 거라 기대돼요.\n탐색 시스템의 SSOT 확장: 현재 Feed-Entity는 해시태그 시스템과 추천 알림에서 SSOT(Single Source of Truth)로 성공적으로 활용되고 있어요. 이러한 성공을 바탕으로 더 많은 시스템으로 Feed-Entity의 활용 범위를 확장할 계획이에요. 이를 통해 전체 서비스에서 일관된 데이터 구조와 높은 신뢰성을 확보할 수 있을 것으로 기대돼요.\n\n이러한 과제들을 해결하여 Feed-Entity를 추천 전반에 활용할 수 있게 된다면, 당근의 피드 시스템은 한 단계 더 발전된 형태로 사용자들에게 가치 있는 경험을 제공할 수 있을 것이라고 믿어요.\n마치며…\n이렇게 피드라는 복잡한 문제를 해결하기 위해 Feed-Entity라는 개념을 도입하고 발전시켜 온 피드인프라팀. 저희는 항상 더 나은 기술과 사용자 경험을 위해 끊임없이 도전하고 있어요. 혹시 이런 기술적 도전에 함께하고 싶으신가요? 당근 피드인프라팀에서는 수백만 명의 사용자들에게 하이퍼로컬 가치 있는 정보를 효율적으로 전달하기 위한 기술을 개발하고 다양한 실험에 도전할 열정적인 동료를 찾고 있어요.\n당근 피드인프라팀과 함께 하이퍼로컬 피드 경험을 만들어 갈 동료를 기다립니다.\n당근마켓 채용 페이지: https://team.daangn.com/jobs/\n\nFeed-Entity: 당근 피드의 심장 was originally published in 당근 테크 블로그 on Medium, where people are continuing the conversation by highlighting and responding to this story.",
    "dc:creator": "Lebron J",
    "guid": "https://medium.com/p/e2ba0a7f57fa",
    "categories": [
      "software-engineering",
      "programming",
      "startup",
      "system-architecture"
    ],
    "isoDate": "2025-04-03T06:05:58.000Z"
  },
  {
    "creator": "당근",
    "title": "AI 툴 개발은 처음이라, 당근 비개발자 구성원들의 AI 도전기",
    "link": "https://medium.com/daangn/ai-%ED%88%B4-%EA%B0%9C%EB%B0%9C%EC%9D%80-%EC%B2%98%EC%9D%8C%EC%9D%B4%EB%9D%BC-%EB%8B%B9%EA%B7%BC-%EB%B9%84%EA%B0%9C%EB%B0%9C%EC%9E%90-%EA%B5%AC%EC%84%B1%EC%9B%90%EB%93%A4%EC%9D%98-ai-%EB%8F%84%EC%A0%84%EA%B8%B0-fb62d2a6c2f3?source=rss----4505f82a2dbd---4",
    "pubDate": "Tue, 01 Apr 2025 07:05:14 GMT",
    "content:encoded": "<h3>AI 툴 개발은 처음이라, 당근 비개발자 구성원들의 AI 도전기 — 당근 AI Show &amp; Tell #1</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*L4Xt94VeWiQ2GJ3MfzRYgA.png\" /><figcaption>해당 이미지는 OpenAI의 이미지 생성 모델인 DALL·E를 활용하여 GPT-4o에서 생성되었습니다.</figcaption></figure><p>최근 IT 업계는 생성형 AI의 등장으로 또 한 번의 큰 전환점을 맞이했어요. 사용자 경험이 AI 중심으로 재편되면서, 기존 서비스의 PMF(Product Market Fit)가 빠르게 무너지는 사례가 많아졌죠. 하루아침에 기존 방식이 낡은 것으로 여겨지는 시대가 열린 거예요. 당근도 이런 흐름 속에서 멈춰 있지 않고, AI를 활용한 다양한 실험을 빠르게 시도하고 있어요.</p><p>이 과정에서 중요한 건 완벽한 정답을 찾는 게 아니라, 실패하더라도 직접 실험하고 실행해보는 경험 그 자체예요. 당근은 매주 화요일마다 ‘AI Show &amp; Tell’을 통해 각 팀의 프로젝트와 시행착오를 공유하고 있어요. 단순한 결과가 아닌, 실패 속에서 얻은 인사이트까지 나누는 자리죠. 업무나 서비스에 AI를 적용한 사례부터 개발 중인 기능의 어려움, 새로운 아이디어까지 다양하게 공유하면서, 모두가 함께 새로운 가능성을 발견하고 있어요.</p><p>앞으로 당근의 AI Show &amp; Tell에서 공유되는 실험들을 생생하게 나누려고 해요. 특히 이번 글에서는 직군에 관계없이 누구나 AI에 도전할 수 있다는 가능성을 보여주는 프로젝트 세 가지를 소개해 드릴게요. 엔지니어뿐 아니라 프로덕트 디자이너, CEO Staff, 운영 매니저까지 얼마나 다양한 구성원들이 AI 실험에 도전하고, 자신의 업무에서 변화를 만들어가고 있는지 확인해보세요. 🚀</p><p>*️이 콘텐츠는 생성형 AI를 활용해 제작된 콘텐츠입니다.</p><h3>개발은 몰라도, AI 실험은 누구나</h3><h3>Project 1. 디자이너가 직접 만든 피그마 플러그인, ‘Ratiosnap’</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*UzZG_aFyyw6_zMYs86qrDw.png\" /></figure><p>로컬비즈니스의 Product Designer Hazel은 Cursor를 활용해, 업무에 필요한 피그마 플러그인을 직접 개발했어요. 시작은 아주 사소한 불편함이었죠. 피그마에서 UI 여백을 2:3 비율로 배치하고 싶었는데, px 단위로 매번 직접 계산해서 넣어야 했거든요. 처음엔 ChatGPT에게 비율 계산을 요청하고 그 값을 복사해 사용하는 식으로 해결했지만, 이마저도 반복되다 보니 꽤 번거롭게 느껴졌어요. 결국 이런 수고로움을 줄이기 위해, 디자인 요소의 위치를 자동으로 조정해주는 플러그인을 직접 만들어보기로 했어요.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*YXUcm6Ml6eWoB4wEWCrU6g.png\" /></figure><p>개발 지식이 거의 없는 Hazel은 단 30분 만에 피그마 플러그인 ‘Ratiosnap’을 완성했어요. 더 놀라운 건 “이런 기능을 만들어줘”라고 구체적인 지시를 내리지 않았다는 점이에요. 평소에 겪던 비효율적인 작업 방식을 설명하고 아이디어를 요청했을 뿐인데, Cursor는 문제를 분석하고 기능 요구사항까지 마치 PRD처럼 정리해주었어요. Hazel은 그 흐름을 따라가며 필요한 부분은 피드백하고 디자인과 기능을 조율했죠. UI 구성부터 코드 작성까지 대부분의 과정을 AI가 주도했고, Hazel은 디자이너로서의 관점과 감각을 더해가며 제품을 완성해 나갔어요.</p><p>이 경험은 Hazel에게 단순한 플러그인 제작 이상의 의미였어요. 디자이너가 문제를 정의하고 솔루션을 기획하는 데서 그치는 것이 아니라, 직접 구현까지 주도할 수 있는 가능성을 확인한 순간이었죠. Cursor라는 도구 덕분에 코드를 몰라도 아이디어를 빠르게 실현할 수 있다는 자신감을 얻은 거예요. AI로 자동화되기 어려운 디자이너의 섬세한 미적 감각과 사용자에 대한 직관이 앞으로 프로덕트 디자인에 중요한 경쟁력이 될 수 있겠다는 인사이트도 얻었죠.</p><p>Hazel의 실험은 팀에도 긍정적인 영향을 주기도 했는데요. 같은 팀의 PM인 Heart도 Hazel이 만든 플러그인을 보고, Cursor를 사용해 PM에게 유용한 플러그인을 이어서 만든 거예요. 실험 하나가 또 다른 실험을 불러오며, 팀 안에 자연스럽게 AI 실험 문화가 번져나갔어요.</p><h3>Project 2. 사내 모든 AI 프로젝트를 한 곳에, ‘AI 프로젝트 캐처’</h3><p>전략지원팀의 CEO Staff인 Kyle은 인프라팀의 Site Reliability Engineer인 Alden의 도움을 받아, 전사 AI 프로젝트를 한 곳에 모아 공유하는 ‘AI 프로젝트 캐처’를 만들었어요. 당근에서는 여러 팀이 다양한 AI 실험을 진행 중이지만, 전사적으로 정보가 공유되지 않아 중복 작업이 생기거나 협업 기회를 놓치는 경우가 많았죠. 이 문제를 해결하기 위해 Kyle은 슬랙에서 오가는 AI 관련 논의를 자동 수집하고 시각화하는 시스템을 기획했고, 인프라팀의 Alden이 기술적으로 구현하며 함께 시스템을 빠르게 만들었어요.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*JJVHuowIehbJGUwblr70ow.png\" /></figure><p>‘AI 프로젝트 캐처’는 슬랙 메시지 중 AI 프로젝트와 관련된 내용을 감지해, 해당 게시글의 태스크, 카테고리, 요약을 자동 생성한 뒤 전용 채널에 등록하는 방식으로 작동해요. 누구나 이 채널에서 전사 프로젝트 현황을 확인할 수 있어요. 관심 있는 주제에 쉽게 접근하고 관련 팀과 협업할 수 있는 환경이 마련된 거죠. 프로젝트의 진행 상황이 실시간으로 공유되어 조직 전체의 AI 실험 속도를 높이고, 더 빠른 실행과 개선이 가능해진 것도 중요한 변화예요.</p><p>사실 이 시스템도 특정 팀이 단독으로 만든 것이 아니라, 다양한 구성원이 협업한 결과인데요. Kyle이 슬랙에 아이디어를 올린 것을 계기로 다양한 피드백이 모였고, 과거 <a href=\"https://about.daangn.com/blog/archive/%EB%8B%B9%EA%B7%BC-%ED%95%B4%EC%BB%A4%ED%86%A4-%EA%B0%9C%EB%B0%9C%EC%9E%90-%EB%AA%B0%EC%9E%85-%ED%98%91%EC%97%85/\">당근 Gen AI 해커톤</a>에서 만들어진 유사한 슬랙 봇의 레포지토리가 공유되며 실험이 빠르게 구체화됐죠. Alden이 이를 기반으로 Cursor와 프롬프트 스튜디오*를 활용해 단 하루 만에 시스템을 구현했어요.</p><blockquote>*프롬프트 스튜디오: 다양한 LLM을 활용해, 간편한 프롬프트 엔지니어링으로 서비스 기능을 구현할 수 있는 사내 AI 툴</blockquote><p>프로젝트를 설계한 Kyle도 프롬프트 스튜디오를 활용해 슬랙 메시지 분류 기준을 직접 수정하며 개선에 적극 참여했는데요. Kyle은 이번 실험을 통해 “코딩을 몰라도 프롬프트 스튜디오 같은 도구를 활용하면, 기능 개발에 더 적극적으로 기여할 수 있겠다는 확신이 들었다”고 말했어요. Kyle은 개발자에게 의존하지 않고 문제를 개선해가는 경험을 통해 큰 동기부여를 받았고, 앞으로 더 다양한 실험에 도전하고 싶다는 자신감도 얻었다고 해요. 실제로 이번 프로젝트를 계기로 이후로도 모바일 앱이나 슬랙 봇을 직접 만들어 보고, API 연동과 서버 배포까지 시도해봤다고 해요.</p><h3>Project 3. AI로 운영 업무의 효율을 높이다, ‘정규표현식 생성기 &amp; 폴리시 체커’</h3><p>중고거래 서비스의 운영을 담당하는 Operations Manager, Sang은 AI를 활용해 당근의 운영 업무를 더욱 효율적으로 바꾼 실험을 소개했어요. Sang은 평소 중고거래 게시글에서 사기, 전문 판매, 구인구직 등 금지 행위를 시도하는 어뷰저들을 탐지하고 대응하는 업무를 맡고 있는데요. 문제는 이 어뷰저들이 규제를 피하기 위해 표현 방식을 계속 바꾼다는 점이에요. 이를 잡아내려면 그때그때 새로운 정규표현식을 수작업으로 만들어야 했는데, 이는 시간이 오래 걸리고 빠른 대응을 어렵게 한다는 단점이 있었어요.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*yU9MGrehg2AO3XoHYX2orQ.png\" /></figure><p>Sang은 이 비효율을 해결하기 위해 Cursor로 자동화 툴을 만들었어요. 그가 만든 정규표현식 생성기에 학습 데이터를 제공하고 프롬프트만 입력하면, JS 형식의 정규표현식을 자동으로 생성해줘요. 예를 들어 먼저 ‘구인구직’으로 신고된 게시글 500건을 CSV 파일로 업로드하고, “일자리를 구하는 사람의 패턴을 잡아줘”라는 프롬프트만 입력하면 되죠.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*QytnOqiZZfrlc33elneS1A.png\" /></figure><p>이어 Sang은 같은 방식으로 내부 정책 문서를 AI가 직접 점검하고 개선점을 제안하는 툴도 만들어봤어요. 이른바 ‘폴리시 체커(Policy Checker)’인데요. 당근의 거래 금지 항목이나 사용자 가이드라인 등 기존 문서를 AI에게 입력하니, 누락된 정보나 모호한 표현, 관련 법률이나 연관 가이드라인까지 짚어줬어요. 문서를 하나하나 검토하지 않아도, 실제로 의미 있는 개선 포인트들을 얻을 수 있었죠.</p><p>이 실험을 통해 Sang은 AI가 단순히 도와주는 도구를 넘어, 운영 정책을 함께 만들어가는 파트너가 될 수 있다는 가능성을 체감했다고 말했어요. Cursor 같은 도구가 처음엔 낯설었지만, 여러 효과적인 툴을 이틀 만에 완성할 수 있었던 경험은 비개발자인 그에게도 큰 자신감을 안겨줬죠. “워드나 노션을 누구나 쓰듯이, AI 툴도 의지만 있다면 누구나 쓸 수 있다”고 말하며, 진짜 장벽은 기술이 아니라 ‘심리적 거리감’이라는 점도 강조했어요.</p><h4>누구나 아이디어를 실현하는 곳</h4><p>AI 실험은 더 이상 엔지니어만의 영역이 아니라, 아이디어만 있다면 누구든지 직접 실행할 수 있는 것이 됐어요. 당근에서는 직군이라는 경계 없이 다양한 구성원들이 각자의 문제를 정의하고, 스스로 해결책을 실험하며 실제 변화를 만들어내고 있어요.</p><p>이런 변화는 단순히 AI 기술을 도입했다는 의미를 넘어, 당근이 더 빠르고 효과적으로 일할 수 있는 방법을 실험하고 실제로 실현해가고 있음을 보여줘요. 실제로 매주 전사적으로 실험을 공유하고, 사소한 아이디어라도 실행해볼 수 있도록 아낌없이 지원하는 환경을 만들어가고 있어요. 덕분에 누구든지, 어떤 팀이든, AI를 활용한 실험에 뛰어들 수 있는 문화가 자연스럽게 자리잡고 있죠.</p><p>앞으로도 당근의 AI Show &amp; Tell에서는 이런 실험들이 계속 이어질 거예요. 다양한 팀에서 시작된 변화의 흐름이 계속해서 전사로 퍼져나가고 있거든요. 누구나 새로운 시도의 주체가 될 수 있는 AI 실험의 현장, 그 생생한 이야기들이 궁금하다면 다음 편도 기대해주세요!</p><blockquote>함께 AI로 새로운 가능성을 발견하고 싶다면? <br>👉 <a href=\"https://about.daangn.com/jobs/\"><strong>당근 채용 공고 바로 가기</strong></a></blockquote><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=fb62d2a6c2f3\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/daangn/ai-%ED%88%B4-%EA%B0%9C%EB%B0%9C%EC%9D%80-%EC%B2%98%EC%9D%8C%EC%9D%B4%EB%9D%BC-%EB%8B%B9%EA%B7%BC-%EB%B9%84%EA%B0%9C%EB%B0%9C%EC%9E%90-%EA%B5%AC%EC%84%B1%EC%9B%90%EB%93%A4%EC%9D%98-ai-%EB%8F%84%EC%A0%84%EA%B8%B0-fb62d2a6c2f3\">AI 툴 개발은 처음이라, 당근 비개발자 구성원들의 AI 도전기</a> was originally published in <a href=\"https://medium.com/daangn\">당근 테크 블로그</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "content:encodedSnippet": "AI 툴 개발은 처음이라, 당근 비개발자 구성원들의 AI 도전기 — 당근 AI Show & Tell #1\n해당 이미지는 OpenAI의 이미지 생성 모델인 DALL·E를 활용하여 GPT-4o에서 생성되었습니다.\n최근 IT 업계는 생성형 AI의 등장으로 또 한 번의 큰 전환점을 맞이했어요. 사용자 경험이 AI 중심으로 재편되면서, 기존 서비스의 PMF(Product Market Fit)가 빠르게 무너지는 사례가 많아졌죠. 하루아침에 기존 방식이 낡은 것으로 여겨지는 시대가 열린 거예요. 당근도 이런 흐름 속에서 멈춰 있지 않고, AI를 활용한 다양한 실험을 빠르게 시도하고 있어요.\n이 과정에서 중요한 건 완벽한 정답을 찾는 게 아니라, 실패하더라도 직접 실험하고 실행해보는 경험 그 자체예요. 당근은 매주 화요일마다 ‘AI Show & Tell’을 통해 각 팀의 프로젝트와 시행착오를 공유하고 있어요. 단순한 결과가 아닌, 실패 속에서 얻은 인사이트까지 나누는 자리죠. 업무나 서비스에 AI를 적용한 사례부터 개발 중인 기능의 어려움, 새로운 아이디어까지 다양하게 공유하면서, 모두가 함께 새로운 가능성을 발견하고 있어요.\n앞으로 당근의 AI Show & Tell에서 공유되는 실험들을 생생하게 나누려고 해요. 특히 이번 글에서는 직군에 관계없이 누구나 AI에 도전할 수 있다는 가능성을 보여주는 프로젝트 세 가지를 소개해 드릴게요. 엔지니어뿐 아니라 프로덕트 디자이너, CEO Staff, 운영 매니저까지 얼마나 다양한 구성원들이 AI 실험에 도전하고, 자신의 업무에서 변화를 만들어가고 있는지 확인해보세요. 🚀\n*️이 콘텐츠는 생성형 AI를 활용해 제작된 콘텐츠입니다.\n개발은 몰라도, AI 실험은 누구나\nProject 1. 디자이너가 직접 만든 피그마 플러그인, ‘Ratiosnap’\n\n로컬비즈니스의 Product Designer Hazel은 Cursor를 활용해, 업무에 필요한 피그마 플러그인을 직접 개발했어요. 시작은 아주 사소한 불편함이었죠. 피그마에서 UI 여백을 2:3 비율로 배치하고 싶었는데, px 단위로 매번 직접 계산해서 넣어야 했거든요. 처음엔 ChatGPT에게 비율 계산을 요청하고 그 값을 복사해 사용하는 식으로 해결했지만, 이마저도 반복되다 보니 꽤 번거롭게 느껴졌어요. 결국 이런 수고로움을 줄이기 위해, 디자인 요소의 위치를 자동으로 조정해주는 플러그인을 직접 만들어보기로 했어요.\n\n개발 지식이 거의 없는 Hazel은 단 30분 만에 피그마 플러그인 ‘Ratiosnap’을 완성했어요. 더 놀라운 건 “이런 기능을 만들어줘”라고 구체적인 지시를 내리지 않았다는 점이에요. 평소에 겪던 비효율적인 작업 방식을 설명하고 아이디어를 요청했을 뿐인데, Cursor는 문제를 분석하고 기능 요구사항까지 마치 PRD처럼 정리해주었어요. Hazel은 그 흐름을 따라가며 필요한 부분은 피드백하고 디자인과 기능을 조율했죠. UI 구성부터 코드 작성까지 대부분의 과정을 AI가 주도했고, Hazel은 디자이너로서의 관점과 감각을 더해가며 제품을 완성해 나갔어요.\n이 경험은 Hazel에게 단순한 플러그인 제작 이상의 의미였어요. 디자이너가 문제를 정의하고 솔루션을 기획하는 데서 그치는 것이 아니라, 직접 구현까지 주도할 수 있는 가능성을 확인한 순간이었죠. Cursor라는 도구 덕분에 코드를 몰라도 아이디어를 빠르게 실현할 수 있다는 자신감을 얻은 거예요. AI로 자동화되기 어려운 디자이너의 섬세한 미적 감각과 사용자에 대한 직관이 앞으로 프로덕트 디자인에 중요한 경쟁력이 될 수 있겠다는 인사이트도 얻었죠.\nHazel의 실험은 팀에도 긍정적인 영향을 주기도 했는데요. 같은 팀의 PM인 Heart도 Hazel이 만든 플러그인을 보고, Cursor를 사용해 PM에게 유용한 플러그인을 이어서 만든 거예요. 실험 하나가 또 다른 실험을 불러오며, 팀 안에 자연스럽게 AI 실험 문화가 번져나갔어요.\nProject 2. 사내 모든 AI 프로젝트를 한 곳에, ‘AI 프로젝트 캐처’\n전략지원팀의 CEO Staff인 Kyle은 인프라팀의 Site Reliability Engineer인 Alden의 도움을 받아, 전사 AI 프로젝트를 한 곳에 모아 공유하는 ‘AI 프로젝트 캐처’를 만들었어요. 당근에서는 여러 팀이 다양한 AI 실험을 진행 중이지만, 전사적으로 정보가 공유되지 않아 중복 작업이 생기거나 협업 기회를 놓치는 경우가 많았죠. 이 문제를 해결하기 위해 Kyle은 슬랙에서 오가는 AI 관련 논의를 자동 수집하고 시각화하는 시스템을 기획했고, 인프라팀의 Alden이 기술적으로 구현하며 함께 시스템을 빠르게 만들었어요.\n\n‘AI 프로젝트 캐처’는 슬랙 메시지 중 AI 프로젝트와 관련된 내용을 감지해, 해당 게시글의 태스크, 카테고리, 요약을 자동 생성한 뒤 전용 채널에 등록하는 방식으로 작동해요. 누구나 이 채널에서 전사 프로젝트 현황을 확인할 수 있어요. 관심 있는 주제에 쉽게 접근하고 관련 팀과 협업할 수 있는 환경이 마련된 거죠. 프로젝트의 진행 상황이 실시간으로 공유되어 조직 전체의 AI 실험 속도를 높이고, 더 빠른 실행과 개선이 가능해진 것도 중요한 변화예요.\n사실 이 시스템도 특정 팀이 단독으로 만든 것이 아니라, 다양한 구성원이 협업한 결과인데요. Kyle이 슬랙에 아이디어를 올린 것을 계기로 다양한 피드백이 모였고, 과거 당근 Gen AI 해커톤에서 만들어진 유사한 슬랙 봇의 레포지토리가 공유되며 실험이 빠르게 구체화됐죠. Alden이 이를 기반으로 Cursor와 프롬프트 스튜디오*를 활용해 단 하루 만에 시스템을 구현했어요.\n*프롬프트 스튜디오: 다양한 LLM을 활용해, 간편한 프롬프트 엔지니어링으로 서비스 기능을 구현할 수 있는 사내 AI 툴\n프로젝트를 설계한 Kyle도 프롬프트 스튜디오를 활용해 슬랙 메시지 분류 기준을 직접 수정하며 개선에 적극 참여했는데요. Kyle은 이번 실험을 통해 “코딩을 몰라도 프롬프트 스튜디오 같은 도구를 활용하면, 기능 개발에 더 적극적으로 기여할 수 있겠다는 확신이 들었다”고 말했어요. Kyle은 개발자에게 의존하지 않고 문제를 개선해가는 경험을 통해 큰 동기부여를 받았고, 앞으로 더 다양한 실험에 도전하고 싶다는 자신감도 얻었다고 해요. 실제로 이번 프로젝트를 계기로 이후로도 모바일 앱이나 슬랙 봇을 직접 만들어 보고, API 연동과 서버 배포까지 시도해봤다고 해요.\nProject 3. AI로 운영 업무의 효율을 높이다, ‘정규표현식 생성기 & 폴리시 체커’\n중고거래 서비스의 운영을 담당하는 Operations Manager, Sang은 AI를 활용해 당근의 운영 업무를 더욱 효율적으로 바꾼 실험을 소개했어요. Sang은 평소 중고거래 게시글에서 사기, 전문 판매, 구인구직 등 금지 행위를 시도하는 어뷰저들을 탐지하고 대응하는 업무를 맡고 있는데요. 문제는 이 어뷰저들이 규제를 피하기 위해 표현 방식을 계속 바꾼다는 점이에요. 이를 잡아내려면 그때그때 새로운 정규표현식을 수작업으로 만들어야 했는데, 이는 시간이 오래 걸리고 빠른 대응을 어렵게 한다는 단점이 있었어요.\n\nSang은 이 비효율을 해결하기 위해 Cursor로 자동화 툴을 만들었어요. 그가 만든 정규표현식 생성기에 학습 데이터를 제공하고 프롬프트만 입력하면, JS 형식의 정규표현식을 자동으로 생성해줘요. 예를 들어 먼저 ‘구인구직’으로 신고된 게시글 500건을 CSV 파일로 업로드하고, “일자리를 구하는 사람의 패턴을 잡아줘”라는 프롬프트만 입력하면 되죠.\n\n이어 Sang은 같은 방식으로 내부 정책 문서를 AI가 직접 점검하고 개선점을 제안하는 툴도 만들어봤어요. 이른바 ‘폴리시 체커(Policy Checker)’인데요. 당근의 거래 금지 항목이나 사용자 가이드라인 등 기존 문서를 AI에게 입력하니, 누락된 정보나 모호한 표현, 관련 법률이나 연관 가이드라인까지 짚어줬어요. 문서를 하나하나 검토하지 않아도, 실제로 의미 있는 개선 포인트들을 얻을 수 있었죠.\n이 실험을 통해 Sang은 AI가 단순히 도와주는 도구를 넘어, 운영 정책을 함께 만들어가는 파트너가 될 수 있다는 가능성을 체감했다고 말했어요. Cursor 같은 도구가 처음엔 낯설었지만, 여러 효과적인 툴을 이틀 만에 완성할 수 있었던 경험은 비개발자인 그에게도 큰 자신감을 안겨줬죠. “워드나 노션을 누구나 쓰듯이, AI 툴도 의지만 있다면 누구나 쓸 수 있다”고 말하며, 진짜 장벽은 기술이 아니라 ‘심리적 거리감’이라는 점도 강조했어요.\n누구나 아이디어를 실현하는 곳\nAI 실험은 더 이상 엔지니어만의 영역이 아니라, 아이디어만 있다면 누구든지 직접 실행할 수 있는 것이 됐어요. 당근에서는 직군이라는 경계 없이 다양한 구성원들이 각자의 문제를 정의하고, 스스로 해결책을 실험하며 실제 변화를 만들어내고 있어요.\n이런 변화는 단순히 AI 기술을 도입했다는 의미를 넘어, 당근이 더 빠르고 효과적으로 일할 수 있는 방법을 실험하고 실제로 실현해가고 있음을 보여줘요. 실제로 매주 전사적으로 실험을 공유하고, 사소한 아이디어라도 실행해볼 수 있도록 아낌없이 지원하는 환경을 만들어가고 있어요. 덕분에 누구든지, 어떤 팀이든, AI를 활용한 실험에 뛰어들 수 있는 문화가 자연스럽게 자리잡고 있죠.\n앞으로도 당근의 AI Show & Tell에서는 이런 실험들이 계속 이어질 거예요. 다양한 팀에서 시작된 변화의 흐름이 계속해서 전사로 퍼져나가고 있거든요. 누구나 새로운 시도의 주체가 될 수 있는 AI 실험의 현장, 그 생생한 이야기들이 궁금하다면 다음 편도 기대해주세요!\n함께 AI로 새로운 가능성을 발견하고 싶다면? \n👉 당근 채용 공고 바로 가기\n\nAI 툴 개발은 처음이라, 당근 비개발자 구성원들의 AI 도전기 was originally published in 당근 테크 블로그 on Medium, where people are continuing the conversation by highlighting and responding to this story.",
    "dc:creator": "당근",
    "guid": "https://medium.com/p/fb62d2a6c2f3",
    "categories": [
      "ai-tools",
      "cursor",
      "automation",
      "ai"
    ],
    "isoDate": "2025-04-01T07:05:14.000Z"
  },
  {
    "creator": "Yany Choi",
    "title": "Karpenter 트러블슈팅 — 비용과 안정성 두 마리 토끼 잡기",
    "link": "https://medium.com/daangn/karpenter-%ED%8A%B8%EB%9F%AC%EB%B8%94%EC%8A%88%ED%8C%85-%EB%B9%84%EC%9A%A9%EA%B3%BC-%EC%95%88%EC%A0%95%EC%84%B1-%EB%91%90%EB%A7%88%EB%A6%AC-%ED%86%A0%EB%81%BC-%EC%9E%A1%EA%B8%B0-ce8bd45ec8f2?source=rss----4505f82a2dbd---4",
    "pubDate": "Thu, 27 Mar 2025 06:32:10 GMT",
    "content:encoded": "<h3>Karpenter 트러블슈팅 — 비용과 안정성 두 마리 토끼 잡기</h3><p>안녕하세요, 저는 당근페이 인프라팀에서 Site Reliability Engineer로 일하고 있는 Yany라고 해요. 저희 팀은 당근페이의 인프라를 안정적으로 관리해요. 개발자들의 프로덕트 개발 속도를 향상하고, 동시에 비용도 최적화하죠.</p><p>저희는 클러스터 오토스케일링 없이 ASG(AWS EC2 AutoScaling Group)로, 그리고 HorizontalPodAutoscaler 없이 클러스터를 관리하고 있었어요. 여기에는 몇 가지 문제가 있었어요:</p><ul><li>스케일 아웃 과정에서 네트워크에 여러 병목 지점이 생겼어요.</li><li>클러스터 업데이트를 진행하면서 ASG마다 AMI를 업데이트해야 했고, 오토스케일링이 원활하지 못했어요.</li><li>컴플라이언스 이슈로 인해 분리된 노드, 서브넷에서 동작해야 하는 워크로드가 증가하면서 ASG가 늘어나 관리 포인트가 증가하고 있었어요.</li><li>새벽 시간대에 트래픽이 현저히 적은 것에 비해 리소스를 너무 많이 사용하고 있었어요.</li></ul><p>당근페이의 거래량과 유저 수가 급격히 증가하면서, 기존의 ASG 기반 인프라 운영 방식으로는 한계가 명확해졌어요. 이에 따라 <strong>더 유연하고 자동화된 클러스터 스케일링이 필요했고, 그 해답으로 Karpenter를 도입하게 되었어요.</strong></p><p>그 여정은 저희가 생각한 것만큼 마냥 쉽지만은 않았는데요. 이번 글에서는 그 트러블슈팅 과정을 구체적으로 소개해드리려고 해요. Karpenter 도입을 고민 중이시거나 더 효율적으로 사용할 방법을 찾고 계신다면, 이 글이 큰 도움이 되길 바라요.</p><h3><a href=\"https://karpenter.sh/\">Karpenter</a>란?</h3><p>Karpenter는 쿠버네티스 클러스터에서 파드의 수요에 맞춰 노드의 양을 조절하는 Cluster Autoscaling Operator에요. 여러 컴포넌트를 통해 원하는 규격의 노드를 생성하고, 생성된 노드의 생명주기를 관리하도록 도와줘요.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ecaimmwGOl2GcGHOImZ_wA.png\" /><figcaption>출처: [<a href=\"https://karpenter.sh/\">https://karpenter.sh/</a>]</figcaption></figure><p>대표적인 기능은 아래와 같아요:</p><p><strong>Provisioning</strong></p><ul><li>Pending 상태의 파드가 존재하면, 스케줄링을 통해 필요한 노드를 생성하여 해당 파드가 스케줄링될 수 있도록 해요.</li><li>각 CSP(Cloud Service Provider, 저희의 경우 AWS가 여기에 해당해요.)에서 만든 NodeClass 구현체를 통해 인스턴스의 규격을 정해요.<br>- AWS로 가정했을 때 AMI, Subnet, Storage, Security Group, Userdata 등 EC2 인스턴스 자체와 관련된 설정을 진행할 수 있어요.</li><li>NodePool을 통해 기존 ASG처럼 목적별로 노드를 생성할 수 있어요.<br>- 여러 타입의 인스턴스를 생성할 수 있어, <a href=\"https://github.com/kubernetes/autoscaler\">Cluster Autoscaler</a> (이하 CA)보다 훨씬 효율적으로 스케일링을 진행할 수 있어요.</li></ul><p><strong>Disruption</strong></p><ul><li><strong>Drift</strong>: NodeClass, NodePool이 바뀌면 Drift를 통해 노드들을 원하는 상태로 Sync할 수 있어요.</li><li><strong>Consolidation</strong>: 충분히 사용하고 있지 않은 노드를 삭제해서 최적화된 양의 리소스를 사용할 수 있어요.<br>- <strong>SingleNodeConsolidation</strong>: 활용도가 낮은 개별 노드를 식별해요. 해당 노드의 워크로드를 다른 노드로 이동한 후 불필요한 노드를 삭제함으로써 리소스 낭비를 줄여요.<br>- <strong>MultiNodeConsolidation</strong>: 여러 개의 작은 노드에 분산된 워크로드를 더 적은 수의 큰 노드로 통합하여 리소스 효율성을 높여요. 이 과정에서 Karpenter는 기존 노드들을 대체할 수 있는 최적의 노드 구성을 자동으로 계산해요.<br>- <strong>EmptyNodeConsolidation</strong>: 워크로드가 전혀 실행되지 않는 빈 노드를 감지하여 신속하게 삭제함으로써 불필요한 리소스 비용을 절감해요. 이는 클러스터에서 사용되지 않는 자원을 즉시 회수하는 데 효과적이에요.</li><li><strong>Expiration</strong>: 노드의 수명을 정하고, 그 시간이 지나면 노드를 삭제해요.</li></ul><p>주요 컴포넌트는 NodeClass (<a href=\"https://github.com/aws/karpenter-provider-aws/blob/main/pkg/apis/v1/ec2nodeclass.go\">AWS 구현체</a>의 경우 EC2NodeClass, <a href=\"https://github.com/Azure/karpenter-provider-azure/blob/main/pkg/apis/v1alpha2/aksnodeclass.go\">Azure 구현체</a>의 경우 AKSNodeClass)<a href=\"https://github.com/kubernetes-sigs/karpenter/blob/main/pkg/apis/v1/nodepool.go\">NodePool</a>, <a href=\"https://github.com/kubernetes-sigs/karpenter/blob/main/pkg/apis/v1/nodeclaim.go\">NodeClaim</a>이 있어요. 각 역할은 다음과 같아요:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*P1OSGloYQoE9S5muAoEBBg.png\" /></figure><p>Karpenter와 CA의 특징을 항목별로 비교해 보면 아래와 같아요:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*uY3gUG2Y-0O_Nupd4sI44w.png\" /></figure><p>Karpenter는 확실히 CA보다 더 효율적이고 빠른 오토스케일링이 가능하도록 지원해 준다는 점에서 커뮤니티에서 인기가 많아요. 저희도 그런 이유로 도입했고요. 하지만 다양한 측면에서 예상하지 못했던 문제점들을 마주했는데요. 어떤 문제들을 마주했고 어떻게 해결했는지 본격적으로 설명해 드릴게요.</p><h3>Troubleshooting</h3><h3>1. 스케줄링이 생각처럼 되지 않아요</h3><p>처음 Karpenter를 PoC할 땐 대체로 잘 확장됐었지만, 때때로 한두 개의 파드들이 Pending 상태에서 풀리지 않고 대기하는 것을 발견했어요. 이 부분을 해결하기 위해 스케줄링 로직을 더 파보면서 재밌는 사실을 알게 되었어요. 바로 <strong>Karpenter 내부에서</strong> <strong>스케줄링을 시뮬레이션한다는 사실</strong>이었어요.</p><p>Karpenter의 스케줄링은 아래와 같은 상황에서 발생하게 돼요:</p><ul><li>Provisioning Loop가 돌 때<br>클러스터 내에서 파드가 Pending되는 이벤트를 탐지해요. 이런 Loop를 끊임없이 반복해서 지속적으로 클러스터 리소스들을 탐색하는 과정을 거쳐요. 파드의 수요가 실제 리소스를 넘는 순간을 빠르게 포착한 후 얼마나 리소스가 더 필요한지 계산해야 하기 때문에 스케줄링이 필요해요.</li><li>Disruption Loop가 돌 때 (Consolidation, Draft 등)<br>Disruption 또한 Provisioning Loop와 마찬가지로 끊임없이 반복하는데요. 현재 노드가 파드 수요보다 많아 불필요하게 사용되는 리소스를 탐지해요. 특정 노드를 지운 후의 파드 스케줄링 방법, 새로운 노드의 생성 여부를 결정해야 하기 때문에 스케줄링이 필요해요.</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*uY3gUG2Y-0O_Nupd4sI44w.png\" /></figure><h4>스케줄링 동작 방식</h4><p>우선 스케줄링 대상 파드를 선정하기부터 큐에서 파드 하나를 추출하기까지의 과정을 도식으로 나타내면 아래와 같은데요. 단계별로 각 과정을 설명할게요.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*tADeQg57V-AwX6m27P7raw.png\" /></figure><p>먼저 파드들은 아래의 조건에 부합해야 스케줄링 대상으로 선정돼요.</p><ul><li>Pending 상태의 파드들</li><li>삭제 대상인 노드의 파드 중 DaemonSet과 이미 삭제되고 있는 파드들<br>- 노드 status의 MarkedForDeletion이 true인지<br>- 노드 자체가 NodeClaim과 관계없이 삭제되고 있는지<br>- NodeClaim, 혹은 매핑된 노드가 삭제되고 있는지</li></ul><p>이렇게 스케줄링 대상 파드들을 정리했으면, 먼저 CPU와 메모리를 많이 사용하는 순서대로 정렬해요. 그 후 큐로 만들어서 리소스를 많이 사용하는 파드들부터 순차적으로 스케줄링을 시작해요.</p><pre>func byCPUAndMemoryDescending(pods []*v1.Pod, podRequests map[types.UID]v1.ResourceList) func(i int, j int) bool {<br> return func(i, j int) bool {<br>  lhsPod := pods[i]<br>  rhsPod := pods[j]<br><br>  lhs := podRequests[lhsPod.UID]<br>  rhs := podRequests[rhsPod.UID]<br><br>  cpuCmp := resources.Cmp(lhs[v1.ResourceCPU], rhs[v1.ResourceCPU])<br>  if cpuCmp &lt; 0 {<br>   return false<br>  } else if cpuCmp &gt; 0 {<br>   return true<br>  }<br>  memCmp := resources.Cmp(lhs[v1.ResourceMemory], rhs[v1.ResourceMemory])<br><br>  if memCmp &lt; 0 {<br>   return false<br>  } else if memCmp &gt; 0 {<br>   return true<br>  }<br>  return lhsPod.UID &lt; rhsPod.UID<br> }<br>}</pre><p>위 과정을 마쳤다면 이제 본격적으로 스케줄링을 시도할 수 있는데요. 이후의 과정을 도식으로 나타나면 아래와 같아요.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*b3KGTDo3KgQ5EGlakKeRAQ.png\" /></figure><p>가장 먼저 큐에서 파드들을 하나씩 꺼내서 노드에 배치하기 시작하는데, 여기서 기본적인 kube-scheduler의 동작을 모방하기 시작해요. (<a href=\"https://github.com/kubernetes-sigs/karpenter/blob/main/pkg/controllers/provisioning/scheduling/scheduler.go#L352\">소스코드</a>)</p><p>파드를 배치하고자 하는 노드에는 아래와 같은 우선순위로 작업이 진행돼요.</p><ol><li>클러스터 내 실제 노드에서 먼저 스케줄링 시도</li></ol><pre>// 클러스터 내 실제 노드에서 먼저 스케줄링을 시도해요.<br> for _, node := range s.existingNodes {<br>  if err := node.Add(ctx, s.kubeClient, pod, s.cachedPodData[pod.UID]); err == nil {<br>   return nil<br>  }<br> }</pre><p>2. 생성하려고 준비한 NodeClaim에 스케줄링 시도</p><pre>// Consider using https://pkg.go.dev/container/heap<br> sort.Slice(s.newNodeClaims, func(a, b int) bool { return len(s.newNodeClaims[a].Pods) &lt; len(s.newNodeClaims[b].Pods) })<br><br> // 생성하려고 준비한 NodeClaim에도 스케줄링을 시도해요.<br> for _, nodeClaim := range s.newNodeClaims {<br>  if err := nodeClaim.Add(pod, s.cachedPodData[pod.UID]); err == nil {<br>   return nil<br>  }<br> }</pre><p>3. 새로운 NodeClaim 생성</p><pre>// 노드를 새로 생성해요.<br> var errs error<br> for _, nodeClaimTemplate := range s.nodeClaimTemplates {<br>  instanceTypes := nodeClaimTemplate.InstanceTypeOptions<br><br>  // if limits have been applied to the nodepool, ensure we filter instance types to avoid violating those limits<br>  if remaining, ok := s.remainingResources[nodeClaimTemplate.NodePoolName]; ok {<br>   instanceTypes = filterByRemainingResources(instanceTypes, remaining)<br>   ... // (validation)<br>  }<br><br>  nodeClaim := NewNodeClaim(nodeClaimTemplate, s.topology, s.daemonOverhead[nodeClaimTemplate], instanceTypes)<br>  if err := nodeClaim.Add(pod, s.cachedPodData[pod.UID]); err != nil {<br>   ... // (error handling)<br>   continue<br>  }<br><br>  // we will launch this nodeClaim and need to track its maximum possible resource usage against our remaining resources<br>  s.newNodeClaims = append(s.newNodeClaims, nodeClaim)<br>  s.remainingResources[nodeClaimTemplate.NodePoolName] = subtractMax(s.remainingResources[nodeClaimTemplate.NodePoolName], nodeClaim.InstanceTypeOptions)<br>  return nil<br> }<br> return errs</pre><p>위의 우선순위에 맞춰 yaml로 작성하는 수많은 규칙을 반영하기 위해, Karpenter 내에서 스케줄링할 노드를 지정해요. 그 과정은 아래 순서대로 진행돼요. (이 코드는 <a href=\"https://github.com/kubernetes-sigs/karpenter/blob/main/pkg/controllers/provisioning/scheduling/existingnode.go#L68\">실제 클러스터에 존재하는 노드에 스케줄링하는 상황의 로직</a>이고, <a href=\"https://github.com/kubernetes-sigs/karpenter/blob/main/pkg/controllers/provisioning/scheduling/nodeclaim.go#L111\">NodeClaim에 파드를 추가하는 로직</a>과는 분리되어 작성되어 있어요.)</p><ol><li>노드와 파드의 taint와 toleration의 일치 여부를 파악해요.</li></ol><pre>// 노드와 파드의 taint-toleration이 일치해야 해요.<br> if err := scheduling.Taints(n.cachedTaints).ToleratesPod(pod); err != nil {<br>  return err<br> }</pre><p>2. 노드가 기존에 존재하면, 노드의 volume 제한을 넘지 않는지 확인해요.</p><pre>// 노드가 기존에 존재하면, 노드의 volume 제한을 넘지 않도록 해요.<br> volumes, err := scheduling.GetVolumes(ctx, kubeClient, pod)<br> if err != nil {<br>  return err<br> }<br> if err = n.VolumeUsage().ExceedsLimits(volumes); err != nil {<br>  return fmt.Errorf(&quot;checking volume usage, %w&quot;, err)<br> }</pre><p>3. 노드의 포트를 중복해서 사용하는지 확인해요.</p><pre>// 노드의 포트를 중복해서 사용하는지 확인해요.<br> hostPorts := scheduling.GetHostPorts(pod)<br> if err = n.HostPortUsage().Conflicts(pod, hostPorts); err != nil {<br>  return fmt.Errorf(&quot;checking host port usage, %w&quot;, err)<br> }</pre><p>4. 노드의 리소스 총량이 새로 뜰 파드를 포함한 request 수요를 감당할 수 있는지 확인해요. NodeClaim을 새로 생성한 경우에는 request 총량을 더해서 인스턴스를 새로 생성할 때 활용할 수 있도록 해요.</p><pre> // 노드의 리소스 총량이 새로 뜰 파드를 포함한 request 수요를 감당할 수 있는지 확인해요. <br> // NodeClaim을 새로 생성한 경우에는 request 총량을 더해서 인스턴스를 새로 생성할 때 활용할 수 있도록 해요.<br> requests := resources.Merge(n.requests, podData.Requests)<br> if !resources.Fits(requests, n.cachedAvailable) {<br>  return fmt.Errorf(&quot;exceeds node resources&quot;)<br> }</pre><p>5. nodeAffinity, nodeSelector를 확인해서 노드와 파드의 조건이 부합하는지 확인해요.</p><pre>// nodeAffinity, nodeSelector를 확인해서 노드와 파드의 조건이 부합하는지 확인해요.<br>nodeRequirements := scheduling.NewRequirements(n.requirements.Values()...)<br>if err = nodeRequirements.Compatible(podData.Requirements); err != nil {<br> return err<br>}<br>nodeRequirements.Add(podData.Requirements.Values()...)</pre><p>6. 토폴로지 요건을 확인해요. 이 부분은 nodeAffinity와 topologySpreadConstraint이 공존하는데, 둘 다 이 과정에서 같이 확인하게 돼요. 여기서 preferred 설정이 들어가 있는 affinity는 계산에 포함되지 않게 돼요.</p><pre>// topology 요건을 확인해요.<br> topologyRequirements, err := n.topology.AddRequirements(pod, n.cachedTaints, podData.StrictRequirements, nodeRequirements)<br> if err != nil {<br>  return err<br> }<br> if err = nodeRequirements.Compatible(topologyRequirements); err != nil {<br>  return err<br> }<br> nodeRequirements.Add(topologyRequirements.Values()...)</pre><p>7. 위 과정을 큐 안에 있는 모든 파드들의 시뮬레이션이 완료될 때까지 반복해요.</p><h4>Karpenter를 활용한 스케줄링의 장점과 한계</h4><p>이 과정의 코드를 보게 되면 kube-scheduler의 기본적인 작동 알고리즘과 동일하게 작동하도록 여러 k8s 라이브러리들을 랩핑해서 내부에서 같은 순서로 로직을 돌리고 있어요. 이렇게 구현하면 NodeClaim의 수요를 빠르게 파악할 수 있어, Karpenter의 최대 강점 중 하나인 빠른 프로비저닝을 제공할 수 있어요.</p><p>하지만 이 부분이 kube-scheduler와 완전하게 동일하다는 보장은 하긴 어려워요. 이 글을 작성하고 있는 Karpenter v1.1.1 현재, Kubernetes 1.28 버전에서 beta로 전환된 topologySpreadConstraints의 matchLabelKeys 는 스케줄링 과정에서 계산하지 않고 있어요. 저희는 Karpenter를 도입하기 이전, ReplicaSet 별로 skew를 계산하기 위해 matchLabelKey에 pod-template-hash (ReplicaSet 뒤의 난수)를 활용하고 있었는데, Karpenter를 사용하면서 이 기능을 포기해야 했어요.</p><p>이 기능은 1년 넘게 <a href=\"https://github.com/kubernetes-sigs/karpenter/pull/852\">Karpenter upstream PR</a>에 올라가 있었다가 1.3.0 버전에서 반영되었어요. 이렇듯 Karpenter는 쿠버네티스의 버전에 따른 변경 사항들을 빠르게 따라오지 못하는 이슈가 있어요. 개인적으로는 kube-scheduler에 접근할 수 있는 인터페이스가 아직 없어서, 더 정확하고 각 버전에 맞는 스케줄링 로직으로 노드를 생성할 수 없다는 게 조금 아쉬웠어요.</p><h3>2. 커스텀 AMI를 사용할 때 제약사항이 있어요.</h3><p>당근페이는 보안규정을 준수하는 노드를 효율적으로 제작하고 사용하기 위해 골든 이미지를 만들어요. 골든 이미지란 보안 컴플라이언스를 준수하기 위한 설정들과 접근제어 처리를 한 이미지예요. 추가 설정을 위해 packer + ansible로 베이킹할 필요 없이 준비가 완료된 이미지를 의미하죠. EKS AMI도 이 과정을 거쳐서 생성하고 있는데, 이 이미지들을 활용하기 위해서 EC2NodeClass에 해당 AMI를 사용해야 했어요.</p><p>우선 아무 설정 없이 AMI Family (OS)만 설정하면, AWS SSM Parameter Store로 이미지 AMI를 회수해요. (<a href=\"https://github.com/aws/karpenter-provider-aws/blob/main/pkg/providers/amifamily/al2023.go#L72\">소스코드</a>)</p><pre>func (a AL2023) resolvePath(architecture, variant, k8sVersion, amiVersion string) string {<br> name := lo.Ternary(<br>  amiVersion == v1.AliasVersionLatest,<br>  &quot;recommended&quot;,<br>  fmt.Sprintf(&quot;amazon-eks-node-al2023-%s-%s-%s-%s&quot;, architecture, variant, k8sVersion, amiVersion),<br> )<br> return fmt.Sprintf(&quot;/aws/service/eks/optimized-ami/%s/amazon-linux-2023/%s/%s/%s/image_id&quot;, k8sVersion, architecture, variant, name)<br>}</pre><p>하지만 저희의 커스텀 이미지를 Parameter Store에 보관한 다음 NodeClass 컨트롤러에서 주기적으로 변경 사항을 가져오는 기능은 없었어요. 대신 직접 AMI 지정하거나 AMI를 태그해서 가져올 수 있었는데요. 저희는 실제 프로덕션 환경으로 나가는 계정과 이러한 운영 작업을 위한 계정이 분리되어 있다는 게 문제였어요. AMI를 복사할 때 AMI에 붙은 태그를 타 계정으로 같이 이동시킬 수가 없었죠. 결국 이 과정에 추가적인 리소스를 사용해서 여러 개의 계정에 태그를 동시에 추가하는 별도의 파이프라인을 구성해야 했어요.</p><h3>3. 작은 노드 위주로 생성해요.</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*xX_HMJY5mfZXp08P8YiuWw.png\" /></figure><p>저희는 기존에 2xlarge 노드를 기본으로 ASG를 구성하고 있었어요. 이를 그대로 Karpenter에 올렸더니, 그 이후부터는 xlarge 위주로 노드를 생성하기 시작했어요. Karpenter의 스케줄링 알고리즘에 따르면 현재 파드들의 resource 수요에 맞게 더 촘촘히 노드를 배치할 수 있어 이러한 접근이 유리해요. 그러나 동시에 노드 개수와 비례하여 증가하는 DaemonSet 비용을 무시할 수 없었기 때문에, 팀에서는 저희가 원하는 방향성으로 스케줄링이 되지 않는 이유를 찾아 나섰어요.</p><p>그 원인은 저희가 설정한 budget에 있었어요. budget은 NodePool에서 consolidation의 reason 별로 동시에 몇 개의 노드를 삭제할 수 있는지 설정하는 값이에요. 저희는 전반적으로, 그리고 보수적으로 스케줄링하기 위해서 budget을 낮게 잡았고, 그 결과 MultiNodeConsolidation이 발생하지 않은 채 SingleNodeConsolidation만 발생했어요. 결국 하나하나의 노드를 삭제하게 되면서 여러 노드를 하나의 노드로 통합하는 액션이 실제로 작동되지 못했어요.</p><p>하지만 budget을 높게 잡아서 disruption의 강도를 높이게 되면, 워크로드들을 너무 공격적으로 이동시키는 것이라고 판단했어요. 그래서 최소 노드 크기를 2xlarge로 설정해서 daemonset으로 인해 발생하게 되는 오버헤드를 줄이려고 했죠.</p><h3>4. 실제 리소스와 Karpenter에서 인식하는 리소스의 양에 차이가 있어요.</h3><p>Karpenter 메트릭을 수집하고 대시보드로 관찰하기 시작했는데, 노드들의 실제 리소스 양보다 Karpenter에서 계산한 리소스 양이 적다는 사실을 알게 되었어요. 이에 따라 더 공격적으로 프로비저닝이 발생해 안정성이 떨어졌어요. 게다가 실제 스케줄링과 어긋나는 엣지 케이스들도 발견되었죠.</p><p>대시보드에는 EKS AMI와 인스턴스 타입에 따라 제공되는 인스턴스 리소스 크기가 표시되는데요. 정확한 리소스의 차이를 확인하기 위해 노드를 실제로 띄워서 확인해 본 결과, 실제 사용 가능한 리소스의 양과 일치하지 않았어요. 이 값은 OS, kubelet 등 노드를 운용하기 위해 필요한 컴포넌트들이 차지하는 공간인데, 이 공간에 대한 계산을 Karpenter에서 일괄적으로 퍼센티지로 설정해서 발생하는 이슈였어요. (<a href=\"https://github.com/aws/karpenter-provider-aws/blob/main/pkg/providers/instancetype/types.go#L329\">소스코드</a>)</p><pre>func memory(ctx context.Context, info ec2types.InstanceTypeInfo) *resource.Quantity {<br> sizeInMib := *info.MemoryInfo.SizeInMiB<br> ...<br> mem := resources.Quantity(fmt.Sprintf(&quot;%dMi&quot;, sizeInMib))<br> // Account for VM overhead in calculation<br> mem.Sub(resource.MustParse(fmt.Sprintf(&quot;%dMi&quot;, int64(math.Ceil(float64(mem.Value())*options.FromContext(ctx).VMMemoryOverheadPercent/1024/1024)))))<br> return mem<br>}</pre><p>이 부분을 해결하기 위해 가장 먼저 AL2023 EKS AMI를 기준으로 인스턴스를 띄우면 제공되는 메모리양과 free 명령어를 통해 나오는 Available 메모리의 갭을 측정했어요. 이후 저희가 허용하는 인스턴스 중 가장 큰 갭을 기준으로 그 일괄적인 값을 반영해서 사용했어요. 다만, 이 해결법은 엣지 케이스의 빈도를 줄였지만, kube-scheduler에서 인식하는 상태와 Karpenter에서 인식하는 상태가 동일하지 않다는 문제가 있었어요.</p><p>1.1.0 버전에서는 <a href=\"https://github.com/aws/karpenter-provider-aws/pull/7004\">한 번 생성된 인스턴스의 실제 리소스 양을 캐싱하도록 패치</a>됐어요. 덕분에 이후 같은 인스턴스 타입을 생성할 때 더 정확한 리소스 값을 반영할 수 있었어요. 특히, 이 업데이트로 인해 Karpenter의 리소스 계산 방식이 개선되면서, 평소 스케줄링의 정합성이 크게 향상되었어요.</p><h3>5. Node Churn이 발생해요.</h3><p>Node Churn은 Karpenter에서 consolidation이 한번 발생할 때 여러 개의 노드가 연쇄적으로 disruption되고 새로 생성되는 현상을 말해요. Churn은 휘젓는다는 뜻인데요. Node Churn이 발생하면 국자로 수프를 휘젓듯이 하나의 이벤트로 인해 많은 수의 워커 노드가 한 번에 재배치되기 시작해요.</p><p>저희는 처음에 이 문제가 너무 급진적으로 consolidation budget을 잡았기 때문이라고 생각했어요. budget을 10%로 설정한 상태에서 진행했는데 pdb를 겨우 지키는 수준에서 파드들이 계속 노드 사이를 오갔어요. CPU 사용량이 급증하게 되었고, 무려 클러스터 전체 노드 중 약 50%가 순차적으로 지워지고 다시 생성됐어요.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*6AS48ZTmkU0mRuW-oux_Zg.png\" /><figcaption>drift처럼 보이지만, 사실은 created와 underutilized가 겹쳐서 노란색처럼 보이는 것이에요. 생성 → 삭제 → 생성 → 삭제 → …를 몇 시간 동안 반복하고 있었어요.</figcaption></figure><p>그래서 모든 budget을 1로 설정하고 동시에 consolidation을 진행할 수 없게 하려고 했어요. 이에 따라 작은 노드를 큰 노드로 병합하는 MultiNodeConsolidation을 사용할 수 없게 됐죠. 하지만, 이렇게 해도 Node Churn이 지속적으로 발생했고, 어떨 때는 하루 종일 Churn이 발생하기도 했어요.</p><p>이후 메트릭을 확인해 보니 모든 consolidation의 시작 시점은 파드 수요의 변경 시점에 있었어요. Node Churn이 크게 발생할 정도의 본격적인 consolidation은 주로 영업일 낮 시간대였는데요. 새로 배포를 진행하면서 rolling, canary 업데이트를 진행하면서 파드의 수요가 요동치는 거였어요.</p><p>이 부분을 개선하고자 이후 개발자들이 배포하는 낮 시간대에는 budget을 1로, 그리고 새벽 시간대에는 budget을 2로 설정했어요. 밤에 budget이 모자라 consolidation이 밀려서 낮에 대규모로 churn이 발생하지 않도록 말이죠. 결과적으로 전체 노드의 10% 내에서 consolidation이 연쇄적으로 발생하는 수준까지 효과적으로 개선했어요.</p><h3>결과, 앞으로 할 일</h3><p>Karpenter를 도입하면서 저희는 여러 방면에서 긍정적인 효과를 보게 되었어요. 가장 큰 효과는 비용을 효율적으로 줄였다는 점이죠. 월간 인프라 비용을 약 10,000$ 절감할 수 있었어요. EKS 클러스터 업데이트 과정에서도 워커 노드의 AMI 교체, 노드의 점진적인 업데이트 등을 조금 더 손쉽게 작업할 수 있게 되었어요.</p><p>다만 아직 더 개선해야 할 부분도 많아요:</p><h4>1. 노드 웜업 시간 개선</h4><p>Karpenter 도입 후 노드가 빠르게 스케일링되면서, 새로 생성된 노드가 워크로드를 정상적으로 처리하기까지 걸리는 초기 웜업 시간 문제가 발생했어요. 이를 해결하기 위해 다음과 같은 방법을 적용했어요:</p><ul><li>일정 수준의 여유 노드를 유지하는 <strong>Overprovisioning 파드</strong>를 활용해 모든 가용 영역(Zone)에서 최소한 하나의 노드를 항상 유지하도록 했어요. 이를 통해 갑작스러운 스케일 아웃 시에도 빠르게 대응할 수 있게 됐어요.</li><li>JVM 서비스들에 readinessProbe를 통한 첫 접근을 유도해 클래스들을 미리 로딩함으로써 웜업 시간을 점진적으로 줄여나가고 있어요.</li></ul><h4>2. 레이턴시 안정화</h4><p>기존에는 스케일링 없이 진행해서 서비스 레이턴시 증가가 눈에 띄지 않았는데, Karpenter로 인한 Node Churn과 배포가 동시에 일어나 레이턴시가 크게 튀는 경우도 발생했어요. 이를 개선하기 위해 Karpenter에서 제공하는 disruption 방지 어노테이션(karpenter.sh/do-not-disrupt)을 배포 중인 서비스에 자동으로 삽입하는 컨트롤러를 개발 중이에요. 이를 통해 더 안정적이면서도 비용 효율적인 인프라를 조성하기 위해 노력하고 있어요.</p><h4>3. 스케줄링 정합성 향상</h4><p>Karpenter와 k8s를 사용하면서 가장 불편함을 느꼈던 스케줄링 흐름 파악을 위해, 현재 Karpenter 스케줄링 시뮬레이터를 개발하고 있어요. 개발이 완료되면 Karpenter와 kube-scheduler의 스케줄링 정합성이 깨졌을 때, 빠르게 원인을 파악하고 문제를 해결할 수 있을 것으로 기대하고 있어요.</p><h3>당근페이 SRE로 오세요!</h3><p>당근페이는 전자금융업자로 많은 규제를 받고 있지만 가능한 한 여러 기술에 대해 열린 마음으로 접근하고 있어요. 저희 당근페이 SRE들은 개발자들의 배포 편의성과 인프라의 효율적 운영을 위해서라면, 어떤 기술이라도 심층적으로 분석해요. 또 그 기술이 필요하다고 판단되면 빠르게 도입하죠. 신뢰와 충돌이라는 신념 아래에서 동료들과 다양한 기술을 심도 있게 테스트하고 있어요.</p><p>더 효율적이고 아름다운 인프라를 만들어가기 위해 저희와 함께할 분을 찾고 있어요. 많은 관심 부탁드려요!</p><p><a href=\"https://team.daangn.com/jobs/5792072003/\">https://team.daangn.com/jobs/5792072003/</a></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ce8bd45ec8f2\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/daangn/karpenter-%ED%8A%B8%EB%9F%AC%EB%B8%94%EC%8A%88%ED%8C%85-%EB%B9%84%EC%9A%A9%EA%B3%BC-%EC%95%88%EC%A0%95%EC%84%B1-%EB%91%90%EB%A7%88%EB%A6%AC-%ED%86%A0%EB%81%BC-%EC%9E%A1%EA%B8%B0-ce8bd45ec8f2\">Karpenter 트러블슈팅 — 비용과 안정성 두 마리 토끼 잡기</a> was originally published in <a href=\"https://medium.com/daangn\">당근 테크 블로그</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "content:encodedSnippet": "Karpenter 트러블슈팅 — 비용과 안정성 두 마리 토끼 잡기\n안녕하세요, 저는 당근페이 인프라팀에서 Site Reliability Engineer로 일하고 있는 Yany라고 해요. 저희 팀은 당근페이의 인프라를 안정적으로 관리해요. 개발자들의 프로덕트 개발 속도를 향상하고, 동시에 비용도 최적화하죠.\n저희는 클러스터 오토스케일링 없이 ASG(AWS EC2 AutoScaling Group)로, 그리고 HorizontalPodAutoscaler 없이 클러스터를 관리하고 있었어요. 여기에는 몇 가지 문제가 있었어요:\n\n스케일 아웃 과정에서 네트워크에 여러 병목 지점이 생겼어요.\n클러스터 업데이트를 진행하면서 ASG마다 AMI를 업데이트해야 했고, 오토스케일링이 원활하지 못했어요.\n컴플라이언스 이슈로 인해 분리된 노드, 서브넷에서 동작해야 하는 워크로드가 증가하면서 ASG가 늘어나 관리 포인트가 증가하고 있었어요.\n새벽 시간대에 트래픽이 현저히 적은 것에 비해 리소스를 너무 많이 사용하고 있었어요.\n\n당근페이의 거래량과 유저 수가 급격히 증가하면서, 기존의 ASG 기반 인프라 운영 방식으로는 한계가 명확해졌어요. 이에 따라 더 유연하고 자동화된 클러스터 스케일링이 필요했고, 그 해답으로 Karpenter를 도입하게 되었어요.\n그 여정은 저희가 생각한 것만큼 마냥 쉽지만은 않았는데요. 이번 글에서는 그 트러블슈팅 과정을 구체적으로 소개해드리려고 해요. Karpenter 도입을 고민 중이시거나 더 효율적으로 사용할 방법을 찾고 계신다면, 이 글이 큰 도움이 되길 바라요.\nKarpenter란?\nKarpenter는 쿠버네티스 클러스터에서 파드의 수요에 맞춰 노드의 양을 조절하는 Cluster Autoscaling Operator에요. 여러 컴포넌트를 통해 원하는 규격의 노드를 생성하고, 생성된 노드의 생명주기를 관리하도록 도와줘요.\n출처: [https://karpenter.sh/]\n대표적인 기능은 아래와 같아요:\nProvisioning\n\nPending 상태의 파드가 존재하면, 스케줄링을 통해 필요한 노드를 생성하여 해당 파드가 스케줄링될 수 있도록 해요.\n각 CSP(Cloud Service Provider, 저희의 경우 AWS가 여기에 해당해요.)에서 만든 NodeClass 구현체를 통해 인스턴스의 규격을 정해요.\n- AWS로 가정했을 때 AMI, Subnet, Storage, Security Group, Userdata 등 EC2 인스턴스 자체와 관련된 설정을 진행할 수 있어요.\nNodePool을 통해 기존 ASG처럼 목적별로 노드를 생성할 수 있어요.\n- 여러 타입의 인스턴스를 생성할 수 있어, Cluster Autoscaler (이하 CA)보다 훨씬 효율적으로 스케일링을 진행할 수 있어요.\n\nDisruption\n\nDrift: NodeClass, NodePool이 바뀌면 Drift를 통해 노드들을 원하는 상태로 Sync할 수 있어요.\nConsolidation: 충분히 사용하고 있지 않은 노드를 삭제해서 최적화된 양의 리소스를 사용할 수 있어요.\n- SingleNodeConsolidation: 활용도가 낮은 개별 노드를 식별해요. 해당 노드의 워크로드를 다른 노드로 이동한 후 불필요한 노드를 삭제함으로써 리소스 낭비를 줄여요.\n- MultiNodeConsolidation: 여러 개의 작은 노드에 분산된 워크로드를 더 적은 수의 큰 노드로 통합하여 리소스 효율성을 높여요. 이 과정에서 Karpenter는 기존 노드들을 대체할 수 있는 최적의 노드 구성을 자동으로 계산해요.\n- EmptyNodeConsolidation: 워크로드가 전혀 실행되지 않는 빈 노드를 감지하여 신속하게 삭제함으로써 불필요한 리소스 비용을 절감해요. 이는 클러스터에서 사용되지 않는 자원을 즉시 회수하는 데 효과적이에요.\nExpiration: 노드의 수명을 정하고, 그 시간이 지나면 노드를 삭제해요.\n\n주요 컴포넌트는 NodeClass (AWS 구현체의 경우 EC2NodeClass, Azure 구현체의 경우 AKSNodeClass)NodePool, NodeClaim이 있어요. 각 역할은 다음과 같아요:\n\nKarpenter와 CA의 특징을 항목별로 비교해 보면 아래와 같아요:\n\nKarpenter는 확실히 CA보다 더 효율적이고 빠른 오토스케일링이 가능하도록 지원해 준다는 점에서 커뮤니티에서 인기가 많아요. 저희도 그런 이유로 도입했고요. 하지만 다양한 측면에서 예상하지 못했던 문제점들을 마주했는데요. 어떤 문제들을 마주했고 어떻게 해결했는지 본격적으로 설명해 드릴게요.\nTroubleshooting\n1. 스케줄링이 생각처럼 되지 않아요\n처음 Karpenter를 PoC할 땐 대체로 잘 확장됐었지만, 때때로 한두 개의 파드들이 Pending 상태에서 풀리지 않고 대기하는 것을 발견했어요. 이 부분을 해결하기 위해 스케줄링 로직을 더 파보면서 재밌는 사실을 알게 되었어요. 바로 Karpenter 내부에서 스케줄링을 시뮬레이션한다는 사실이었어요.\nKarpenter의 스케줄링은 아래와 같은 상황에서 발생하게 돼요:\n\nProvisioning Loop가 돌 때\n클러스터 내에서 파드가 Pending되는 이벤트를 탐지해요. 이런 Loop를 끊임없이 반복해서 지속적으로 클러스터 리소스들을 탐색하는 과정을 거쳐요. 파드의 수요가 실제 리소스를 넘는 순간을 빠르게 포착한 후 얼마나 리소스가 더 필요한지 계산해야 하기 때문에 스케줄링이 필요해요.\nDisruption Loop가 돌 때 (Consolidation, Draft 등)\nDisruption 또한 Provisioning Loop와 마찬가지로 끊임없이 반복하는데요. 현재 노드가 파드 수요보다 많아 불필요하게 사용되는 리소스를 탐지해요. 특정 노드를 지운 후의 파드 스케줄링 방법, 새로운 노드의 생성 여부를 결정해야 하기 때문에 스케줄링이 필요해요.\n\n스케줄링 동작 방식\n우선 스케줄링 대상 파드를 선정하기부터 큐에서 파드 하나를 추출하기까지의 과정을 도식으로 나타내면 아래와 같은데요. 단계별로 각 과정을 설명할게요.\n\n먼저 파드들은 아래의 조건에 부합해야 스케줄링 대상으로 선정돼요.\n\nPending 상태의 파드들\n삭제 대상인 노드의 파드 중 DaemonSet과 이미 삭제되고 있는 파드들\n- 노드 status의 MarkedForDeletion이 true인지\n- 노드 자체가 NodeClaim과 관계없이 삭제되고 있는지\n- NodeClaim, 혹은 매핑된 노드가 삭제되고 있는지\n\n이렇게 스케줄링 대상 파드들을 정리했으면, 먼저 CPU와 메모리를 많이 사용하는 순서대로 정렬해요. 그 후 큐로 만들어서 리소스를 많이 사용하는 파드들부터 순차적으로 스케줄링을 시작해요.\nfunc byCPUAndMemoryDescending(pods []*v1.Pod, podRequests map[types.UID]v1.ResourceList) func(i int, j int) bool {\n return func(i, j int) bool {\n  lhsPod := pods[i]\n  rhsPod := pods[j]\n  lhs := podRequests[lhsPod.UID]\n  rhs := podRequests[rhsPod.UID]\n  cpuCmp := resources.Cmp(lhs[v1.ResourceCPU], rhs[v1.ResourceCPU])\n  if cpuCmp < 0 {\n   return false\n  } else if cpuCmp > 0 {\n   return true\n  }\n  memCmp := resources.Cmp(lhs[v1.ResourceMemory], rhs[v1.ResourceMemory])\n  if memCmp < 0 {\n   return false\n  } else if memCmp > 0 {\n   return true\n  }\n  return lhsPod.UID < rhsPod.UID\n }\n}\n위 과정을 마쳤다면 이제 본격적으로 스케줄링을 시도할 수 있는데요. 이후의 과정을 도식으로 나타나면 아래와 같아요.\n\n가장 먼저 큐에서 파드들을 하나씩 꺼내서 노드에 배치하기 시작하는데, 여기서 기본적인 kube-scheduler의 동작을 모방하기 시작해요. (소스코드)\n파드를 배치하고자 하는 노드에는 아래와 같은 우선순위로 작업이 진행돼요.\n\n클러스터 내 실제 노드에서 먼저 스케줄링 시도\n\n// 클러스터 내 실제 노드에서 먼저 스케줄링을 시도해요.\n for _, node := range s.existingNodes {\n  if err := node.Add(ctx, s.kubeClient, pod, s.cachedPodData[pod.UID]); err == nil {\n   return nil\n  }\n }\n2. 생성하려고 준비한 NodeClaim에 스케줄링 시도\n// Consider using https://pkg.go.dev/container/heap\n sort.Slice(s.newNodeClaims, func(a, b int) bool { return len(s.newNodeClaims[a].Pods) < len(s.newNodeClaims[b].Pods) })\n // 생성하려고 준비한 NodeClaim에도 스케줄링을 시도해요.\n for _, nodeClaim := range s.newNodeClaims {\n  if err := nodeClaim.Add(pod, s.cachedPodData[pod.UID]); err == nil {\n   return nil\n  }\n }\n3. 새로운 NodeClaim 생성\n// 노드를 새로 생성해요.\n var errs error\n for _, nodeClaimTemplate := range s.nodeClaimTemplates {\n  instanceTypes := nodeClaimTemplate.InstanceTypeOptions\n  // if limits have been applied to the nodepool, ensure we filter instance types to avoid violating those limits\n  if remaining, ok := s.remainingResources[nodeClaimTemplate.NodePoolName]; ok {\n   instanceTypes = filterByRemainingResources(instanceTypes, remaining)\n   ... // (validation)\n  }\n  nodeClaim := NewNodeClaim(nodeClaimTemplate, s.topology, s.daemonOverhead[nodeClaimTemplate], instanceTypes)\n  if err := nodeClaim.Add(pod, s.cachedPodData[pod.UID]); err != nil {\n   ... // (error handling)\n   continue\n  }\n  // we will launch this nodeClaim and need to track its maximum possible resource usage against our remaining resources\n  s.newNodeClaims = append(s.newNodeClaims, nodeClaim)\n  s.remainingResources[nodeClaimTemplate.NodePoolName] = subtractMax(s.remainingResources[nodeClaimTemplate.NodePoolName], nodeClaim.InstanceTypeOptions)\n  return nil\n }\n return errs\n위의 우선순위에 맞춰 yaml로 작성하는 수많은 규칙을 반영하기 위해, Karpenter 내에서 스케줄링할 노드를 지정해요. 그 과정은 아래 순서대로 진행돼요. (이 코드는 실제 클러스터에 존재하는 노드에 스케줄링하는 상황의 로직이고, NodeClaim에 파드를 추가하는 로직과는 분리되어 작성되어 있어요.)\n\n노드와 파드의 taint와 toleration의 일치 여부를 파악해요.\n\n// 노드와 파드의 taint-toleration이 일치해야 해요.\n if err := scheduling.Taints(n.cachedTaints).ToleratesPod(pod); err != nil {\n  return err\n }\n2. 노드가 기존에 존재하면, 노드의 volume 제한을 넘지 않는지 확인해요.\n// 노드가 기존에 존재하면, 노드의 volume 제한을 넘지 않도록 해요.\n volumes, err := scheduling.GetVolumes(ctx, kubeClient, pod)\n if err != nil {\n  return err\n }\n if err = n.VolumeUsage().ExceedsLimits(volumes); err != nil {\n  return fmt.Errorf(\"checking volume usage, %w\", err)\n }\n3. 노드의 포트를 중복해서 사용하는지 확인해요.\n// 노드의 포트를 중복해서 사용하는지 확인해요.\n hostPorts := scheduling.GetHostPorts(pod)\n if err = n.HostPortUsage().Conflicts(pod, hostPorts); err != nil {\n  return fmt.Errorf(\"checking host port usage, %w\", err)\n }\n4. 노드의 리소스 총량이 새로 뜰 파드를 포함한 request 수요를 감당할 수 있는지 확인해요. NodeClaim을 새로 생성한 경우에는 request 총량을 더해서 인스턴스를 새로 생성할 때 활용할 수 있도록 해요.\n // 노드의 리소스 총량이 새로 뜰 파드를 포함한 request 수요를 감당할 수 있는지 확인해요. \n // NodeClaim을 새로 생성한 경우에는 request 총량을 더해서 인스턴스를 새로 생성할 때 활용할 수 있도록 해요.\n requests := resources.Merge(n.requests, podData.Requests)\n if !resources.Fits(requests, n.cachedAvailable) {\n  return fmt.Errorf(\"exceeds node resources\")\n }\n5. nodeAffinity, nodeSelector를 확인해서 노드와 파드의 조건이 부합하는지 확인해요.\n// nodeAffinity, nodeSelector를 확인해서 노드와 파드의 조건이 부합하는지 확인해요.\nnodeRequirements := scheduling.NewRequirements(n.requirements.Values()...)\nif err = nodeRequirements.Compatible(podData.Requirements); err != nil {\n return err\n}nodeRequirements.Add(podData.Requirements.Values()...)\n6. 토폴로지 요건을 확인해요. 이 부분은 nodeAffinity와 topologySpreadConstraint이 공존하는데, 둘 다 이 과정에서 같이 확인하게 돼요. 여기서 preferred 설정이 들어가 있는 affinity는 계산에 포함되지 않게 돼요.\n// topology 요건을 확인해요.\n topologyRequirements, err := n.topology.AddRequirements(pod, n.cachedTaints, podData.StrictRequirements, nodeRequirements)\n if err != nil {\n  return err\n }\n if err = nodeRequirements.Compatible(topologyRequirements); err != nil {\n  return err\n }\n nodeRequirements.Add(topologyRequirements.Values()...)\n7. 위 과정을 큐 안에 있는 모든 파드들의 시뮬레이션이 완료될 때까지 반복해요.\nKarpenter를 활용한 스케줄링의 장점과 한계\n이 과정의 코드를 보게 되면 kube-scheduler의 기본적인 작동 알고리즘과 동일하게 작동하도록 여러 k8s 라이브러리들을 랩핑해서 내부에서 같은 순서로 로직을 돌리고 있어요. 이렇게 구현하면 NodeClaim의 수요를 빠르게 파악할 수 있어, Karpenter의 최대 강점 중 하나인 빠른 프로비저닝을 제공할 수 있어요.\n하지만 이 부분이 kube-scheduler와 완전하게 동일하다는 보장은 하긴 어려워요. 이 글을 작성하고 있는 Karpenter v1.1.1 현재, Kubernetes 1.28 버전에서 beta로 전환된 topologySpreadConstraints의 matchLabelKeys 는 스케줄링 과정에서 계산하지 않고 있어요. 저희는 Karpenter를 도입하기 이전, ReplicaSet 별로 skew를 계산하기 위해 matchLabelKey에 pod-template-hash (ReplicaSet 뒤의 난수)를 활용하고 있었는데, Karpenter를 사용하면서 이 기능을 포기해야 했어요.\n이 기능은 1년 넘게 Karpenter upstream PR에 올라가 있었다가 1.3.0 버전에서 반영되었어요. 이렇듯 Karpenter는 쿠버네티스의 버전에 따른 변경 사항들을 빠르게 따라오지 못하는 이슈가 있어요. 개인적으로는 kube-scheduler에 접근할 수 있는 인터페이스가 아직 없어서, 더 정확하고 각 버전에 맞는 스케줄링 로직으로 노드를 생성할 수 없다는 게 조금 아쉬웠어요.\n2. 커스텀 AMI를 사용할 때 제약사항이 있어요.\n당근페이는 보안규정을 준수하는 노드를 효율적으로 제작하고 사용하기 위해 골든 이미지를 만들어요. 골든 이미지란 보안 컴플라이언스를 준수하기 위한 설정들과 접근제어 처리를 한 이미지예요. 추가 설정을 위해 packer + ansible로 베이킹할 필요 없이 준비가 완료된 이미지를 의미하죠. EKS AMI도 이 과정을 거쳐서 생성하고 있는데, 이 이미지들을 활용하기 위해서 EC2NodeClass에 해당 AMI를 사용해야 했어요.\n우선 아무 설정 없이 AMI Family (OS)만 설정하면, AWS SSM Parameter Store로 이미지 AMI를 회수해요. (소스코드)\nfunc (a AL2023) resolvePath(architecture, variant, k8sVersion, amiVersion string) string {\n name := lo.Ternary(\n  amiVersion == v1.AliasVersionLatest,\n  \"recommended\",\n  fmt.Sprintf(\"amazon-eks-node-al2023-%s-%s-%s-%s\", architecture, variant, k8sVersion, amiVersion),\n )\n return fmt.Sprintf(\"/aws/service/eks/optimized-ami/%s/amazon-linux-2023/%s/%s/%s/image_id\", k8sVersion, architecture, variant, name)\n}\n하지만 저희의 커스텀 이미지를 Parameter Store에 보관한 다음 NodeClass 컨트롤러에서 주기적으로 변경 사항을 가져오는 기능은 없었어요. 대신 직접 AMI 지정하거나 AMI를 태그해서 가져올 수 있었는데요. 저희는 실제 프로덕션 환경으로 나가는 계정과 이러한 운영 작업을 위한 계정이 분리되어 있다는 게 문제였어요. AMI를 복사할 때 AMI에 붙은 태그를 타 계정으로 같이 이동시킬 수가 없었죠. 결국 이 과정에 추가적인 리소스를 사용해서 여러 개의 계정에 태그를 동시에 추가하는 별도의 파이프라인을 구성해야 했어요.\n3. 작은 노드 위주로 생성해요.\n\n저희는 기존에 2xlarge 노드를 기본으로 ASG를 구성하고 있었어요. 이를 그대로 Karpenter에 올렸더니, 그 이후부터는 xlarge 위주로 노드를 생성하기 시작했어요. Karpenter의 스케줄링 알고리즘에 따르면 현재 파드들의 resource 수요에 맞게 더 촘촘히 노드를 배치할 수 있어 이러한 접근이 유리해요. 그러나 동시에 노드 개수와 비례하여 증가하는 DaemonSet 비용을 무시할 수 없었기 때문에, 팀에서는 저희가 원하는 방향성으로 스케줄링이 되지 않는 이유를 찾아 나섰어요.\n그 원인은 저희가 설정한 budget에 있었어요. budget은 NodePool에서 consolidation의 reason 별로 동시에 몇 개의 노드를 삭제할 수 있는지 설정하는 값이에요. 저희는 전반적으로, 그리고 보수적으로 스케줄링하기 위해서 budget을 낮게 잡았고, 그 결과 MultiNodeConsolidation이 발생하지 않은 채 SingleNodeConsolidation만 발생했어요. 결국 하나하나의 노드를 삭제하게 되면서 여러 노드를 하나의 노드로 통합하는 액션이 실제로 작동되지 못했어요.\n하지만 budget을 높게 잡아서 disruption의 강도를 높이게 되면, 워크로드들을 너무 공격적으로 이동시키는 것이라고 판단했어요. 그래서 최소 노드 크기를 2xlarge로 설정해서 daemonset으로 인해 발생하게 되는 오버헤드를 줄이려고 했죠.\n4. 실제 리소스와 Karpenter에서 인식하는 리소스의 양에 차이가 있어요.\nKarpenter 메트릭을 수집하고 대시보드로 관찰하기 시작했는데, 노드들의 실제 리소스 양보다 Karpenter에서 계산한 리소스 양이 적다는 사실을 알게 되었어요. 이에 따라 더 공격적으로 프로비저닝이 발생해 안정성이 떨어졌어요. 게다가 실제 스케줄링과 어긋나는 엣지 케이스들도 발견되었죠.\n대시보드에는 EKS AMI와 인스턴스 타입에 따라 제공되는 인스턴스 리소스 크기가 표시되는데요. 정확한 리소스의 차이를 확인하기 위해 노드를 실제로 띄워서 확인해 본 결과, 실제 사용 가능한 리소스의 양과 일치하지 않았어요. 이 값은 OS, kubelet 등 노드를 운용하기 위해 필요한 컴포넌트들이 차지하는 공간인데, 이 공간에 대한 계산을 Karpenter에서 일괄적으로 퍼센티지로 설정해서 발생하는 이슈였어요. (소스코드)\nfunc memory(ctx context.Context, info ec2types.InstanceTypeInfo) *resource.Quantity {\n sizeInMib := *info.MemoryInfo.SizeInMiB\n ...\n mem := resources.Quantity(fmt.Sprintf(\"%dMi\", sizeInMib))\n // Account for VM overhead in calculation\n mem.Sub(resource.MustParse(fmt.Sprintf(\"%dMi\", int64(math.Ceil(float64(mem.Value())*options.FromContext(ctx).VMMemoryOverheadPercent/1024/1024)))))\n return mem\n}\n이 부분을 해결하기 위해 가장 먼저 AL2023 EKS AMI를 기준으로 인스턴스를 띄우면 제공되는 메모리양과 free 명령어를 통해 나오는 Available 메모리의 갭을 측정했어요. 이후 저희가 허용하는 인스턴스 중 가장 큰 갭을 기준으로 그 일괄적인 값을 반영해서 사용했어요. 다만, 이 해결법은 엣지 케이스의 빈도를 줄였지만, kube-scheduler에서 인식하는 상태와 Karpenter에서 인식하는 상태가 동일하지 않다는 문제가 있었어요.\n1.1.0 버전에서는 한 번 생성된 인스턴스의 실제 리소스 양을 캐싱하도록 패치됐어요. 덕분에 이후 같은 인스턴스 타입을 생성할 때 더 정확한 리소스 값을 반영할 수 있었어요. 특히, 이 업데이트로 인해 Karpenter의 리소스 계산 방식이 개선되면서, 평소 스케줄링의 정합성이 크게 향상되었어요.\n5. Node Churn이 발생해요.\nNode Churn은 Karpenter에서 consolidation이 한번 발생할 때 여러 개의 노드가 연쇄적으로 disruption되고 새로 생성되는 현상을 말해요. Churn은 휘젓는다는 뜻인데요. Node Churn이 발생하면 국자로 수프를 휘젓듯이 하나의 이벤트로 인해 많은 수의 워커 노드가 한 번에 재배치되기 시작해요.\n저희는 처음에 이 문제가 너무 급진적으로 consolidation budget을 잡았기 때문이라고 생각했어요. budget을 10%로 설정한 상태에서 진행했는데 pdb를 겨우 지키는 수준에서 파드들이 계속 노드 사이를 오갔어요. CPU 사용량이 급증하게 되었고, 무려 클러스터 전체 노드 중 약 50%가 순차적으로 지워지고 다시 생성됐어요.\ndrift처럼 보이지만, 사실은 created와 underutilized가 겹쳐서 노란색처럼 보이는 것이에요. 생성 → 삭제 → 생성 → 삭제 → …를 몇 시간 동안 반복하고 있었어요.\n그래서 모든 budget을 1로 설정하고 동시에 consolidation을 진행할 수 없게 하려고 했어요. 이에 따라 작은 노드를 큰 노드로 병합하는 MultiNodeConsolidation을 사용할 수 없게 됐죠. 하지만, 이렇게 해도 Node Churn이 지속적으로 발생했고, 어떨 때는 하루 종일 Churn이 발생하기도 했어요.\n이후 메트릭을 확인해 보니 모든 consolidation의 시작 시점은 파드 수요의 변경 시점에 있었어요. Node Churn이 크게 발생할 정도의 본격적인 consolidation은 주로 영업일 낮 시간대였는데요. 새로 배포를 진행하면서 rolling, canary 업데이트를 진행하면서 파드의 수요가 요동치는 거였어요.\n이 부분을 개선하고자 이후 개발자들이 배포하는 낮 시간대에는 budget을 1로, 그리고 새벽 시간대에는 budget을 2로 설정했어요. 밤에 budget이 모자라 consolidation이 밀려서 낮에 대규모로 churn이 발생하지 않도록 말이죠. 결과적으로 전체 노드의 10% 내에서 consolidation이 연쇄적으로 발생하는 수준까지 효과적으로 개선했어요.\n결과, 앞으로 할 일\nKarpenter를 도입하면서 저희는 여러 방면에서 긍정적인 효과를 보게 되었어요. 가장 큰 효과는 비용을 효율적으로 줄였다는 점이죠. 월간 인프라 비용을 약 10,000$ 절감할 수 있었어요. EKS 클러스터 업데이트 과정에서도 워커 노드의 AMI 교체, 노드의 점진적인 업데이트 등을 조금 더 손쉽게 작업할 수 있게 되었어요.\n다만 아직 더 개선해야 할 부분도 많아요:\n1. 노드 웜업 시간 개선\nKarpenter 도입 후 노드가 빠르게 스케일링되면서, 새로 생성된 노드가 워크로드를 정상적으로 처리하기까지 걸리는 초기 웜업 시간 문제가 발생했어요. 이를 해결하기 위해 다음과 같은 방법을 적용했어요:\n\n일정 수준의 여유 노드를 유지하는 Overprovisioning 파드를 활용해 모든 가용 영역(Zone)에서 최소한 하나의 노드를 항상 유지하도록 했어요. 이를 통해 갑작스러운 스케일 아웃 시에도 빠르게 대응할 수 있게 됐어요.\nJVM 서비스들에 readinessProbe를 통한 첫 접근을 유도해 클래스들을 미리 로딩함으로써 웜업 시간을 점진적으로 줄여나가고 있어요.\n\n2. 레이턴시 안정화\n기존에는 스케일링 없이 진행해서 서비스 레이턴시 증가가 눈에 띄지 않았는데, Karpenter로 인한 Node Churn과 배포가 동시에 일어나 레이턴시가 크게 튀는 경우도 발생했어요. 이를 개선하기 위해 Karpenter에서 제공하는 disruption 방지 어노테이션(karpenter.sh/do-not-disrupt)을 배포 중인 서비스에 자동으로 삽입하는 컨트롤러를 개발 중이에요. 이를 통해 더 안정적이면서도 비용 효율적인 인프라를 조성하기 위해 노력하고 있어요.\n3. 스케줄링 정합성 향상\nKarpenter와 k8s를 사용하면서 가장 불편함을 느꼈던 스케줄링 흐름 파악을 위해, 현재 Karpenter 스케줄링 시뮬레이터를 개발하고 있어요. 개발이 완료되면 Karpenter와 kube-scheduler의 스케줄링 정합성이 깨졌을 때, 빠르게 원인을 파악하고 문제를 해결할 수 있을 것으로 기대하고 있어요.\n당근페이 SRE로 오세요!\n당근페이는 전자금융업자로 많은 규제를 받고 있지만 가능한 한 여러 기술에 대해 열린 마음으로 접근하고 있어요. 저희 당근페이 SRE들은 개발자들의 배포 편의성과 인프라의 효율적 운영을 위해서라면, 어떤 기술이라도 심층적으로 분석해요. 또 그 기술이 필요하다고 판단되면 빠르게 도입하죠. 신뢰와 충돌이라는 신념 아래에서 동료들과 다양한 기술을 심도 있게 테스트하고 있어요.\n더 효율적이고 아름다운 인프라를 만들어가기 위해 저희와 함께할 분을 찾고 있어요. 많은 관심 부탁드려요!\nhttps://team.daangn.com/jobs/5792072003/\n\nKarpenter 트러블슈팅 — 비용과 안정성 두 마리 토끼 잡기 was originally published in 당근 테크 블로그 on Medium, where people are continuing the conversation by highlighting and responding to this story.",
    "dc:creator": "Yany Choi",
    "guid": "https://medium.com/p/ce8bd45ec8f2",
    "categories": [
      "programming",
      "sre",
      "kubernetes",
      "karpenter"
    ],
    "isoDate": "2025-03-27T06:32:10.000Z"
  },
  {
    "creator": "Ho Yeon",
    "title": "구글처럼 복잡한 권한 쉽게 관리하기 feat. GraphQL",
    "link": "https://medium.com/daangn/%EA%B5%AC%EA%B8%80%EC%B2%98%EB%9F%BC-%EB%B3%B5%EC%9E%A1%ED%95%9C-%EA%B6%8C%ED%95%9C-%EC%89%BD%EA%B2%8C-%EA%B4%80%EB%A6%AC%ED%95%98%EA%B8%B0-feat-graphql-9ce80d34d39b?source=rss----4505f82a2dbd---4",
    "pubDate": "Thu, 20 Mar 2025 08:21:54 GMT",
    "content:encoded": "<p>안녕하세요. 당근 알림 경험팀에서 프론트엔드 엔지니어로 일하고 있는 딜런(Dylan.lee)이라고 해요.</p><p>알림 경험팀은 당근 사용자들뿐만 아니라 당근 구성원들의 알림 경험(Notification Experiences)을 책임져요. 사용자가 그동안 받은 알림을 모아볼 수 있는 알림함부터 당근 구성원이 알림을 간편하게 발송할 수 있는 알림 센터까지, 알림과 관련된 다양한 서비스를 만들고 있죠.</p><p>알림 센터와 같은 서비스를 만들다 보면 필연적으로 인증과 인가를 다루게 돼요. 알림 센터는 다양한 소속의 당근 구성원들이 사용하기 때문에, 민감한 정보는 외부에 노출되지 않도록 소속에 따라 권한을 분리해야 하죠. 또한 마케팅성 알림은 마케팅 팀의 승인이 있는 경우에만 발송할 수 있어야 하고요.</p><p>이렇게 서비스가 복잡해지고 기능이 고도화될수록 인증과 관련된 코드가 길어지고 복잡해지는 문제가 생겼어요. 이번 글에서는 권한을 선언적으로 관리하며 코드의 복잡성을 효과적으로 개선한 방법을 소개해 드리려고 해요.</p><h3>역할을 기반으로 권한을 관리한다면?</h3><p>알림 센터에서는 사용자에게 발송되는 알림 내용을 템플릿 형태로 관리해요. 이것을 알림 시나리오라고 하는데요. 당근의 모든 구성원은 여기에 접근 권한을 갖고 있지만 외부 협력사는 볼 수 없어야 해요. 이러한 권한은 단순했기 때문에 간단한 분기만으로도 관리할 수 있었어요.</p><pre>const canAccess = (user, entity) =&gt; {<br>  if (entity === &quot;scenarioPage&quot; &amp;&amp; user.company === &quot;daangn&quot;) return true;<br>  return false;<br>}</pre><p>그러다 권한 정책이 업데이트되었어요. 외부 협력사 직원이더라도 특정 이메일 값을 갖는 사용자라면, 알림 시나리오 페이지에 접근 권한을 가지도록 수정이 필요했어요. 그래서 저는 allow list를 만들어 관리했어요.</p><pre>const allowList = [&quot;john@external.com&quot;];<br><br>const canAccess = (user, entity) =&gt; {<br>  if (entity === &quot;scenarioPage&quot; &amp;&amp; (user.company === &quot;daangn&quot; || allowList.includes(user.email))) return true;<br>  return false;<br>}</pre><p>그러다가 새로운 기능이 추가되었어요. 마케팅성 알림을 발송하기 전에는 마케팅팀에게 리뷰를 요청하고, 마케팅팀이 승인할 때만 발송할 수 있어야 했죠.</p><pre>const allowList = [&quot;john@external.com&quot;];<br><br>const canAccess = (user, entity) =&gt; {<br>  if (entity === &quot;scenarioPage&quot; &amp;&amp; (user.company === &quot;daangn&quot; || allowList.includes(user.email))) return true;<br>  if (entity === &quot;approveButton&quot; &amp;&amp; user.team === &quot;marketing&quot;) return true;<br>  return false;<br>}</pre><p>이번엔 알림 경험팀에 특수한 권한을 부여해야 했어요.</p><ul><li>아직 개발이 진행 중인 기능을 전사에 공개하기 전에 알림 경험팀만 먼저 사용할 수 있어야 했어요.</li><li>알림 경험팀은 모든 알림에 대해 승인 권한을 가져야 했어요.</li></ul><pre>const allowList = [&quot;john@external.com&quot;];<br><br>const canAccess = (user, entity) =&gt; {<br>  if (entity === &quot;scenarioPage&quot; &amp;&amp; (user.company === &quot;daangn&quot; || allowList.includes(user.email))) return true;<br>  if (entity === &quot;approveButton&quot; &amp;&amp; (user.team === &quot;marketing&quot; || user.team === &quot;nx&quot;)) return true;<br>  if (entity === &quot;experimental&quot; &amp;&amp; user.team === &quot;nx&quot;) return true;<br>  return false;<br>}</pre><p>권한이 복잡해질 때마다 코드가 읽기 어려워지니, 이건 지속 가능한 방법이 아니라는 생각이 들었어요.</p><h3>RBAC에서 ReBAC으로</h3><p>기존에 사용하던 방식을 <strong>RBAC(Role-Based Access Control)</strong>이라고 불러요. 사용자가 어떠한 권한을 가지고 있느냐에 따라서 접근 권한을 제어하는 방식인데요. 간단한 권한 관계에서는 효과적일 수 있지만, 관계가 복잡해질수록 가독성이 저하되고 관리하기 어렵다는 단점이 있어요. 위에서의 사례처럼 역할만을 기반으로 권한을 관리하다 보니 코드의 복잡성이 과도하게 증가하는 <strong>권한 폭발</strong>이 발생할 수도 있고요.</p><blockquote><strong>권한 폭발(Role Explosion)</strong></blockquote><blockquote>조직 구조가 복잡해지고 역할의 수가 기하급수적으로 증가하는 현상</blockquote><pre>const canAccess = (user, entity) =&gt; {<br>  if (entity === &quot;scenarioPage&quot; &amp;&amp; (user.company === &quot;daangn&quot; || user.email === &quot;john@external.com&quot;)) return true;<br>  if (entity === &quot;approveButton&quot; &amp;&amp; (user.team === &quot;marketing&quot; || user.team === &quot;nx&quot; || user.rank === &quot;manager&quot;)) return true;<br>  if (entity === &quot;experimental&quot; &amp;&amp; (user.team === &quot;nx&quot; || (user.team === &quot;engineering&quot; &amp;&amp; user.rank === &quot;senior&quot;))) return true;<br>  if (entity === &quot;reports&quot; &amp;&amp; (user.team === &quot;analytics&quot; || user.team === &quot;finance&quot; || user.email === &quot;ceo@daangn.com&quot;)) return true;<br>  if (entity === &quot;userAdmin&quot; &amp;&amp; (user.team === &quot;security&quot; || (user.rank === &quot;director&quot; &amp;&amp; user.department === &quot;IT&quot;))) return true;<br>  if (entity === &quot;settings&quot; &amp;&amp; (user.isAdmin || user.email === &quot;system@daangn.com&quot; || user.team === &quot;devops&quot;)) return true;<br>  if (entity === &quot;billing&quot; &amp;&amp; (user.team === &quot;finance&quot; || user.rank === &quot;vp&quot; || user.email.endsWith(&quot;@accounting.daangn.com&quot;))) return true;<br>  if (entity === &quot;metrics&quot; &amp;&amp; (user.hasMetricsAccess || user.team === &quot;data&quot; || (user.projects &amp;&amp; user.projects.includes(&quot;analytics&quot;)))) return true;<br>  return false;<br>}</pre><p>RBAC은 대상 엔티티(Entity, 상호작용하려고 하는 요소)마다 접근이 가능한지 판단해 줄 기준이 필요하고, 그 기준이 바로 역할이에요. 그런데 대상 엔티티가 위계를 가지고 있는 경우, 상위 엔티티에 역할이 추가될 때마다 하위 엔티티에도 역할을 추가해 주어야 해요. 예를 들어, 팀과 팀원처럼 사용자가 위계를 가지고 있는 경우, 팀 역할과 팀원 역할로 나누어서 관리되어야 해요.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*NeP27gYsiY-hUW5EMnouAw.png\" /></figure><p>서론에서 제시한 예제 중 “알림 시나리오 페이지 접근 권한” 부분만 도식화해 본 그림이에요. RBAC은 역할의 위계를 표현하지 못하기 때문에 특정 엔티티에 접근할 수 있는 모든 권한을 명시해 주어야 해요.</p><p>여기서 <strong>ReBAC(Relation-Based Access Control)</strong>을 사용하면 복잡한 권한을 더 쉽게 관리할 수 있어요. ReBAC은 구글의 Zanzibar에서도 사용하고 있는 권한 관리 방식인데요. RBAC과는 다르게 권한 간의 관계를 기반으로 권한을 제어해요.</p><p>ReBAC을 사용하면 팀과 팀원의 관계가 자연스럽게 연결돼요. 따라서 어떤 User가 직접 NotificationScenario에 접근할 수 있는 권한이 없더라도 member로 연결된 Team에 접근 권한이 있다면, 그 User는 NotificationScenario에 접근할 수 있다는 것을 알 수 있어요.</p><h3>TypeScript로 구현하기</h3><p>타입스크립트를 사용해서 ReBAC을 구현할 수 있는 다양한 방법들이 있어요. 저는 그중에서 graplix(<a href=\"https://github.com/daangn/graplix\">https://github.com/daangn/graplix</a>)를 사용했어요.</p><p>graplix는 당근에서 오픈소스로 공개한 ReBAC 기반 인증/인가 프레임워크예요. ReBAC의 권한 관계를 그래프로 표현할 수 있다는 점에서 착안해 GraphQL에서 영감을 받아 만들어졌어요. graplix를 사용해서 예시의 권한 관계를 구현해 볼게요.</p><h3>Step 1. 스키마 작성하기</h3><p><strong>[엔티티 간의 관계 표현하기]</strong></p><p>graplix는 스키마 우선 접근 방식(Schema-First Approach)을 사용해서 ReBAC을 만들고 있어요. 그래서 먼저 스키마를 작성해 주어야 해요.</p><pre>const schema = {<br>  Employee: {},<br>  Team: {},<br>  NotificationScenario: {}<br>}</pre><p>사용하고자 하는 엔티티를 타입으로 작성하면 돼요. 이 엔티티들은 그래프의 노드가 되고, 스키마에서 그 관계를 명시해 줄 거예요.</p><p>예를 들어 어떤 팀은 어떤 직원을 멤버로 가져요. 다시 말해보면,</p><blockquote><em>Team과 Employee는 member라는 관계를 맺는다.</em></blockquote><p>라고 쓸 수 있어요. 그 관계를 스키마로 표현하면 아래와 같아요.</p><pre>const schema = {<br>  Employee: {},<br>  Team: {<br>    member: {<br>      type: &quot;Employee&quot;<br>    }<br>  },<br>  NotificationScenario: {}<br>}</pre><p>그림으로 표현하면 아래처럼 표현할 수 있어요.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*8t76DvngOob2xVqkkwDDFw.png\" /></figure><p><strong>[관계 참조하기]</strong></p><p>마찬가지로 접근하고자 하는 타겟 엔티티와의 관계도 표현할 수 있어요.</p><pre>const schema = {<br>  Employee: {},<br>  Team: {<br>    member: {<br>      type: &quot;Employee&quot;<br>    }<br>  },<br>  NotificationScenario: {<br>    viewer_employee: {<br>      type: &quot;Employee&quot;<br>    },<br>  }<br>}</pre><p>Employee라는 엔티티와 NotificationScenario 엔티티는 viewer_employee라는 관계를 맺는다고 정의할 수 있어요. 마찬가지로 Team 엔티티와 NotificationScenario 엔티티는 Team 타입의 관계를 맺어요.</p><pre>const schema = {<br>  Employee: {},<br>  Team: {<br>    member: {<br>      type: &quot;Employee&quot;<br>    }<br>  },<br>  NotificationScenario: {<br>    viewer_team: {<br>      type: &quot;Team&quot;<br>    },<br>    viewer_employee: {<br>      type: &quot;Employee&quot;<br>    },<br>  }<br>}</pre><p>graplix에서는 when 절을 사용할 수 있는데요. 다른 관계를 참조해서 관계를 만들 수도 있어요. 예를 들어 can_access라는 관계를 만들고 “viewer_employee 관계일 때 접근 가능하다”라는 표현을 해주고 싶다면 아래처럼 써줄 수 있어요.</p><pre>const schema = {<br>  Employee: {},<br>  Team: {<br>    member: {<br>      type: &quot;Employee&quot;<br>    }<br>  },<br>  NotificationScenario: {<br>    viewer_team: {<br>      type: &quot;Team&quot;<br>    },<br>    viewer_employee: {<br>      type: &quot;Employee&quot;<br>    },<br>    can_access: [{ when: &quot;viewer_employee&quot; }]<br>  }<br>}</pre><p>지금까지 작성된 관계를 그림으로 표현해 보면 아래와 같아요.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*5KF2p1_OAIPMrkMDYSGgUA.png\" /></figure><p><strong>[관계 상속하기]</strong></p><p>from 절을 이용해서 관계를 상속해 줄 수도 있어요. 예를 들어, viewer_team은 Team 타입이고, Team 엔티티에는 member로 관계를 맺고 있는 Employee라는 엔티티가 있어요. 즉 “viewer_team의 member로 있는 Employee도 NotificationScenario에 접근할 수 있다”라는 걸 표현해 주기 위해서는 아래와 같이 표현해 줄 수 있어요.</p><pre>const schema = {<br>  Employee: {},<br>  Team: {<br>    member: {<br>      type: &quot;Employee&quot;<br>    }<br>  },<br>  NotificationScenario: {<br>    viewer_team: {<br>      type: &quot;Team&quot;<br>    },<br>    viewer_employee: {<br>      type: &quot;Employee&quot;<br>    },<br>    can_access: [<br>      { when: &quot;viewer_employee&quot; },<br>      { when: &quot;member&quot;, from: &quot;viewer_team&quot; }<br>    ]<br>  }<br>}</pre><p>추가된 관계를 그림으로 표현해 보면 아래와 같아요.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*pj2rbvShbI2i3zD2_qRDrA.png\" /></figure><p>이렇게 간단하게 작성한 스키마만으로,</p><ol><li>어떤 직원은 어떤 알림 시나리오에 접근 권한이 있다.</li><li>어떤 팀에 속하는 직원은 어떤 알림 시나리오에 접근 권한이 있다.</li></ol><p>라는 것을 표현해 줄 수 있게 되었어요. 나머지 관계도 스키마로 표현하면 아래와 같아요.</p><pre>const schema = {<br>  Employee: {},<br>  Team: {<br>    member: {<br>      type: &quot;Employee&quot;<br>    }<br>  },<br>  NotificationScenario: {<br>    viewer_team: {<br>      type: &quot;Team&quot;<br>    },<br>    viewer_employee: {<br>      type: &quot;Employee&quot;<br>    },<br>    reviewer: {<br>      type: &quot;Employee&quot;<br>    },<br>    can_access: [{ when: &quot;viewer_employee&quot; }, { when: &quot;member&quot;, from: &quot;viewer_team&quot; }],<br>    can_approve: [{ when: &quot;reviewer&quot; }]<br>  }<br>}</pre><p>만들어진 스키마를 도식화하면 아래와 같아요.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*iQzb5AhoKH905QYLEYidBg.png\" /></figure><p>“알림 경험팀의 Dylan.lee가 NotificationScenario와 viewer 관계를 맺는가?”라는 조건은 다음과 같은 순서로 평가되어요.</p><ul><li>Dylan.lee라는 Employee 엔티티는 NotificationsExperiences팀이라는 Team 엔티티와 member 관계를 맺는지</li><li>NotificationsExperiences팀이라는 Team 엔티티와 NotificationScenario라는 엔티티가 viewer_team 관계를 맺는지</li></ul><p>두 관계가 참이라면 Dylan.lee는 NotificationScenario와 can_access 관계를 맺기 때문에 접근이 가능하다는 결과가 나와요.</p><h3>Step 2. Identify 함수 만들기</h3><p>identify 함수는 타입을 알 수 없는 엔티티를 입력받고 어떤 엔티티인지 구분해 주는 함수예요.</p><p>예를 들어 Employee는 email, Team은 code, NotificationScenario는 id를 식별자로 사용하고자 하면 아래와 같이 엔티티 타입을 만들어줄 수 있어요.</p><pre>type Entity = {<br>  Employee: { type: &quot;Employee&quot;; email: string };<br>  Team: { type: &quot;Team&quot;; code: string };<br>  NotificationScenario: { type: &quot;NotificationScenario&quot;; id: string };<br>}</pre><p>이제 identify 함수는 이렇게 써줄 수 있어요.</p><pre>const identify: GraplixIdentifier&lt;Entity&gt; = (entity) =&gt; {<br>  switch (entity.type) {<br>    case &#39;User&#39;:<br>      return { type: &#39;User&#39;, id: entity.email };<br>    case &#39;Team&#39;:<br>      return { type: &#39;Team&#39;, id: entity.code };<br>    case &#39;NotificationScenario&#39;:<br>      return { type: &#39;NotificationScenario&#39;, id: entity.id };<br>  }<br>};</pre><h3>Step 3. 리졸버 작성하기</h3><p>스키마를 정의했으니 이제 리졸버를 작성해 줘야 해요. 리졸버는 실제로 그 관계를 어떻게 평가할 것인지 작성해 주는 부분이에요. 여기서 Entity에 선언된 타입을 활용할 수 있어요. Employee 엔티티가 어떤 Team 엔티티와 member 관계를 맺는지 평가한다고 가정해 볼게요.</p><pre>const resolvers = {<br>  Employee: {},<br>  Team: {<br>    member: {<br>      type: &quot;Employee&quot;,<br>      async resolve(entity, context) {}<br>    }<br>  },<br>  // ...<br>}</pre><p>그러면 작성한 Entity 타입에 의해 resolve 메소드의 첫 번째 인자(entity)는 { type: &#39;Team&#39;, code: string } 타입을 가지게 돼요. entity.code를 이용해서 특정 부서에 속하는 사용자 목록을 반환할 수 있어요.</p><pre>const resolvers = {<br>  Employee: {},<br>  Team: {<br>    member: {<br>      type: &quot;Employee&quot;,<br>      async resolve(entity, context) {<br>        const employees = await context.listAllEmployees();<br>        const filteredEmployees = employees.filter((employee) =&gt; employee.code === code);<br>        <br>        return filteredEmployees;<br>      }<br>    }<br>  },<br>  // ...<br>}</pre><p>이제 작성한 리졸버와 스키마를 graplix 함수에 넣으면 check 함수를 반환해요.</p><pre>export const { check } = graplix({<br>  resolvers,<br>  schemas,<br>  identify,<br>  context: { listAllEmployees: () ⇒ /* ... */ }<br>})</pre><p>이제 check 함수를 활용하면 복잡한 권한을 간편하게 선언적으로 다룰 수 있게 돼요.</p><pre>const canAccess = await check({<br>  user: req.user,<br>  object: page,<br>  relation: &quot;can_access&quot;<br>});</pre><h3>Step 4. 활용하기</h3><p>이렇게 구현한 인증/인가 기술을 활용해서 기능별 권한 제어를 구현한 사례를 소개해 드릴게요. 알림 센터에서 가장 많이 쓰이고 있는 형태는 게이트 컴포넌트예요. &lt;Permission /&gt;이라는 이름으로 만들어 사용하고 있어요.</p><h3>&lt;Permission /&gt; 컴포넌트</h3><p>&lt;Permission /&gt;은 확인하고 싶은 featureId만 전달해 주면 현재 보고 있는 사용자의 정보를 담아 권한을 확인하고, 결과에 따라 해당 컴포넌트를 보여줄지 말지 결정해 주는 컴포넌트예요. 내부적으로 usePermission 훅을 사용하기 때문에 권한을 체크하는 과정에서 Suspense가 발생해요.</p><pre>type PermissionProps = {<br>  featureId: FeatureId;<br>  children: React.ReactNode;<br>  fallback?: React.ReactNode;<br>  decorate?: boolean | ((children: React.ReactNode) =&gt; React.ReactNode);<br>};</pre><p>featureId와 children 외에 fallback과 decorate props도 볼 수 있는데요. fallback은 권한이 없거나 인증에 실패했을 경우 보여줄 ReactNode를 입력받도록 하는 prop이에요. decorate는 어떤 featureId로 인가를 해주고 있는지 시각적으로 보여주기 위한 prop이에요.</p><p>&lt;Permission /&gt; 컴포넌트를 이용하면 이렇게 간단하게 인가 여부를 관리할 수 있어요.</p><pre>&lt;Permission featureId=&quot;read:notification-scenario&quot;&gt;<br>  &lt;Tab&gt;모니터링&lt;/Tab&gt;<br>&lt;/Permission&gt;</pre><h3>내부 구현</h3><p>실제 알림 센터에서는 BFF에서 graplix를 활용하고 있어요. 아래는 featureId에 따라 can_access를 평가하도록 설정한 코드예요.</p><pre>app.get(&quot;/api/authorize/:featureId&quot;, (req, res) =&gt; {<br>  const canAccess = await check({<br>    user: req.email,<br>    object: req.param.featureId,<br>    relation: &#39;can_access&#39;,<br>  });<br><br>  res.send(canAccess);<br>});</pre><p>그리고 usePermission 훅을 구현해서 리액트 클라이언트에서 쉽게 사용할 수 있어요. 예제 코드는 TanStack Query를 사용해서 구현한 예시예요.</p><pre>function usePermission(featureId) {<br>  const { data: hasPermission } = useSuspenseQuery({ <br>    queryKey: [&#39;permission&#39;, featureId], <br>    queryFn: async ({ queryKey }) =&gt; {<br>      const [_, featureId] = queryKey;<br>      const response = await fetch(`/api/authorize/${featureId}`);<br>    <br>      return response.json();<br>    }<br>  });<br>  <br>  return hasPermission;<br>  }<br>}</pre><p>이제 usePermission 훅을 이용해서 &lt;Permission /&gt; 컴포넌트를 만들 수 있어요. usePermission이 반환한 결과가 true라면 children을, false라면 fallback을 반환해 줌으로써 권한이 없는 경우를 핸들링해 줄 수 있어요.</p><pre>function Permission({ featureId, children, fallback, decorate }: PermissionProps): React.ReactNode {<br>  const hasPermission = usePermission(featureId);<br><br>  if (!hasPermission) {<br>    return fallback;<br>  }<br><br>  return children;<br>});</pre><p>기존에 사용하던 &lt;Permission /&gt; 컴포넌트가 어떤 컴포넌트를 제어하고 있는지 파악하기 어렵다는 의견이 있었어요. 실제로 전사에 정식으로 공개한 새로운 기능이 일부 구성원에게 보이지 않았던 문제가 있었고, 이것을 해결하기 위해 decorate를 추가했어요.</p><pre>function Permission({ featureId, children, fallback, decorate }: PermissionProps): React.ReactNode {<br>  const hasPermission = usePermission(featureId);<br><br>  if (!hasPermission) {<br>    return fallback;<br>  }<br><br>  if (typeof decorate === &#39;function&#39;) {<br>    return decorate(children);<br>  }<br><br>  if (decorate === true) {<br>    return &lt;Decorator featureId={featureId}&gt;{children}&lt;/Decorator&gt;;<br>  }<br><br>  return children;<br>});</pre><p>decorate prop은 boolean 혹은 render prop 형태로 제공돼요. 직접 사용하는 곳에서 컴포넌트를 만들어 주입할 수 있고, 그렇지 않으면 &lt;Decorator /&gt; 컴포넌트로 감싸게 돼요. 모든 곳에 테두리를 추가하면 시각적으로 복잡해지니 이렇게 분기를 추가해 두었어요.</p><p>&lt;Decorator /&gt; 컴포넌트는 radix의 &lt;Slot /&gt; 컴포넌트를 활용했어요. 자식 컴포넌트의 레이아웃을 바꾸지 않고 border 스타일을 주입해 주는 용도로 사용하고 있어요.</p><pre>export const Decorator = ({ children, featureId, ...props }: DecoratorProps) =&gt; {<br>  return (<br>    &lt;div style={{ position: &#39;relative&#39; }}&gt;<br>      &lt;span<br>        style={{<br>          position: &#39;absolute&#39;,<br>          top: 0,<br>          right: 0,<br>          // ...<br>        }}<br>      &gt;<br>        {featureId}<br>      &lt;/span&gt;<br>      &lt;Slot {...props} style={{ border: &#39;2px solid hsl(var(--destructive))&#39; }}&gt;<br>        {children}<br>      &lt;/Slot&gt;<br>    &lt;/div&gt;<br>  );<br>};</pre><p>&lt;Decorator /&gt; 컴포넌트가 적용된 실제 모습은 아래와 같아요.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*kJOrs5D88g0Yuv_q0NyU5g.png\" /></figure><p>이제 이렇게 &lt;Permission /&gt;으로 감싸주기만 하면, 더 이상 if 문을 작성할 필요 없이 복잡한 권한들을 단순하게 제어하고 관리/추적할 수 있게 되었어요.</p><h3>마치며</h3><p>ReBAC을 적용하기 전에는 아래와 같이 써야 했던 코드가,</p><pre>function App() {<br>  const user = getUserInfo();<br>  const canViewMonitoring = user.team.codes.includes(&quot;CODE0001&quot;);<br>  <br>  return (<br>    &lt;Tabs&gt;<br>      &lt;Tab&gt;기본 정보&lt;/Tab&gt;<br>      &lt;Tab&gt;통계&lt;/Tab&gt;<br>      {canViewMonitoring &amp;&amp; (<br>        &lt;Tab&gt;모니터링&lt;/Tab&gt;<br>      )}<br>      &lt;Tab&gt;시나리오 버전 정보&lt;/Tab&gt;<br>    &lt;/Tabs&gt;<br>  );  <br>}</pre><p>이렇게 같이 간단하고 선언적인 코드로 바뀔 수 있었어요.</p><pre>function App() {<br>  return (<br>    &lt;Tabs&gt;<br>      &lt;Tab&gt;기본 정보&lt;/Tab&gt;<br>      &lt;Tab&gt;통계&lt;/Tab&gt;<br>      &lt;Permission featureId=&quot;nx&quot;&gt;<br>        &lt;Tab&gt;모니터링&lt;/Tab&gt;<br>      &lt;/Permission&gt;<br>      &lt;Tab&gt;시나리오 버전 정보&lt;/Tab&gt;<br>    &lt;/Tabs&gt;<br>  );  <br>}</pre><p>이번 글에서는 ReBAC을 이용해서 점점 복잡해지는 권한을 잘 다루는 법을 알아봤어요. 그런데 ReBAC에도 한계가 있어요. 관계가 복잡해지고 노드가 많아질수록 N+1 문제가 생길 확률이 높아질 수 있어요. 저희 팀 같은 경우에는 Dataloader나 적절한 캐싱으로 N+1 문제를 방지하고 있어요. 이런 부분들을 인지하고 사용한다면 ReBAC과 그 구현체인 graplix에도 높은 잠재력이 있다고 생각해요. graplix에 관심이 생겼다면 독자 여러분도 graplix에 많은 기여해 보시길 바라요!</p><ul><li>참조</li><li><a href=\"https://permify.co/post/role-explosion/\">https://permify.co/post/role-explosion/</a></li><li><a href=\"https://www.osohq.com/academy/relationship-based-access-control-rebac\">https://www.osohq.com/academy/relationship-based-access-control-rebac</a></li><li><a href=\"https://www.okta.com/blog/2023/06/authorization-back-to-basics-rbac-vs-rebac/\">https://www.okta.com/blog/2023/06/authorization-back-to-basics-rbac-vs-rebac/</a></li><li><a href=\"https://www.okta.com/kr/identity-101/role-based-access-control-vs-attribute-based-access-control/\">https://www.okta.com/kr/identity-101/role-based-access-control-vs-attribute-based-access-control/</a></li></ul><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=9ce80d34d39b\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/daangn/%EA%B5%AC%EA%B8%80%EC%B2%98%EB%9F%BC-%EB%B3%B5%EC%9E%A1%ED%95%9C-%EA%B6%8C%ED%95%9C-%EC%89%BD%EA%B2%8C-%EA%B4%80%EB%A6%AC%ED%95%98%EA%B8%B0-feat-graphql-9ce80d34d39b\">구글처럼 복잡한 권한 쉽게 관리하기 feat. GraphQL</a> was originally published in <a href=\"https://medium.com/daangn\">당근 테크 블로그</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "content:encodedSnippet": "안녕하세요. 당근 알림 경험팀에서 프론트엔드 엔지니어로 일하고 있는 딜런(Dylan.lee)이라고 해요.\n알림 경험팀은 당근 사용자들뿐만 아니라 당근 구성원들의 알림 경험(Notification Experiences)을 책임져요. 사용자가 그동안 받은 알림을 모아볼 수 있는 알림함부터 당근 구성원이 알림을 간편하게 발송할 수 있는 알림 센터까지, 알림과 관련된 다양한 서비스를 만들고 있죠.\n알림 센터와 같은 서비스를 만들다 보면 필연적으로 인증과 인가를 다루게 돼요. 알림 센터는 다양한 소속의 당근 구성원들이 사용하기 때문에, 민감한 정보는 외부에 노출되지 않도록 소속에 따라 권한을 분리해야 하죠. 또한 마케팅성 알림은 마케팅 팀의 승인이 있는 경우에만 발송할 수 있어야 하고요.\n이렇게 서비스가 복잡해지고 기능이 고도화될수록 인증과 관련된 코드가 길어지고 복잡해지는 문제가 생겼어요. 이번 글에서는 권한을 선언적으로 관리하며 코드의 복잡성을 효과적으로 개선한 방법을 소개해 드리려고 해요.\n역할을 기반으로 권한을 관리한다면?\n알림 센터에서는 사용자에게 발송되는 알림 내용을 템플릿 형태로 관리해요. 이것을 알림 시나리오라고 하는데요. 당근의 모든 구성원은 여기에 접근 권한을 갖고 있지만 외부 협력사는 볼 수 없어야 해요. 이러한 권한은 단순했기 때문에 간단한 분기만으로도 관리할 수 있었어요.\nconst canAccess = (user, entity) => {\n  if (entity === \"scenarioPage\" && user.company === \"daangn\") return true;\n  return false;\n}\n그러다 권한 정책이 업데이트되었어요. 외부 협력사 직원이더라도 특정 이메일 값을 갖는 사용자라면, 알림 시나리오 페이지에 접근 권한을 가지도록 수정이 필요했어요. 그래서 저는 allow list를 만들어 관리했어요.\nconst allowList = [\"john@external.com\"];\nconst canAccess = (user, entity) => {\n  if (entity === \"scenarioPage\" && (user.company === \"daangn\" || allowList.includes(user.email))) return true;\n  return false;\n}\n그러다가 새로운 기능이 추가되었어요. 마케팅성 알림을 발송하기 전에는 마케팅팀에게 리뷰를 요청하고, 마케팅팀이 승인할 때만 발송할 수 있어야 했죠.\nconst allowList = [\"john@external.com\"];\nconst canAccess = (user, entity) => {\n  if (entity === \"scenarioPage\" && (user.company === \"daangn\" || allowList.includes(user.email))) return true;\n  if (entity === \"approveButton\" && user.team === \"marketing\") return true;\n  return false;\n}\n이번엔 알림 경험팀에 특수한 권한을 부여해야 했어요.\n\n아직 개발이 진행 중인 기능을 전사에 공개하기 전에 알림 경험팀만 먼저 사용할 수 있어야 했어요.\n알림 경험팀은 모든 알림에 대해 승인 권한을 가져야 했어요.\n\nconst allowList = [\"john@external.com\"];\nconst canAccess = (user, entity) => {\n  if (entity === \"scenarioPage\" && (user.company === \"daangn\" || allowList.includes(user.email))) return true;\n  if (entity === \"approveButton\" && (user.team === \"marketing\" || user.team === \"nx\")) return true;\n  if (entity === \"experimental\" && user.team === \"nx\") return true;\n  return false;\n}\n권한이 복잡해질 때마다 코드가 읽기 어려워지니, 이건 지속 가능한 방법이 아니라는 생각이 들었어요.\nRBAC에서 ReBAC으로\n기존에 사용하던 방식을 RBAC(Role-Based Access Control)이라고 불러요. 사용자가 어떠한 권한을 가지고 있느냐에 따라서 접근 권한을 제어하는 방식인데요. 간단한 권한 관계에서는 효과적일 수 있지만, 관계가 복잡해질수록 가독성이 저하되고 관리하기 어렵다는 단점이 있어요. 위에서의 사례처럼 역할만을 기반으로 권한을 관리하다 보니 코드의 복잡성이 과도하게 증가하는 권한 폭발이 발생할 수도 있고요.\n권한 폭발(Role Explosion)\n조직 구조가 복잡해지고 역할의 수가 기하급수적으로 증가하는 현상\nconst canAccess = (user, entity) => {\n  if (entity === \"scenarioPage\" && (user.company === \"daangn\" || user.email === \"john@external.com\")) return true;\n  if (entity === \"approveButton\" && (user.team === \"marketing\" || user.team === \"nx\" || user.rank === \"manager\")) return true;\n  if (entity === \"experimental\" && (user.team === \"nx\" || (user.team === \"engineering\" && user.rank === \"senior\"))) return true;\n  if (entity === \"reports\" && (user.team === \"analytics\" || user.team === \"finance\" || user.email === \"ceo@daangn.com\")) return true;\n  if (entity === \"userAdmin\" && (user.team === \"security\" || (user.rank === \"director\" && user.department === \"IT\"))) return true;\n  if (entity === \"settings\" && (user.isAdmin || user.email === \"system@daangn.com\" || user.team === \"devops\")) return true;\n  if (entity === \"billing\" && (user.team === \"finance\" || user.rank === \"vp\" || user.email.endsWith(\"@accounting.daangn.com\"))) return true;\n  if (entity === \"metrics\" && (user.hasMetricsAccess || user.team === \"data\" || (user.projects && user.projects.includes(\"analytics\")))) return true;\n  return false;\n}\nRBAC은 대상 엔티티(Entity, 상호작용하려고 하는 요소)마다 접근이 가능한지 판단해 줄 기준이 필요하고, 그 기준이 바로 역할이에요. 그런데 대상 엔티티가 위계를 가지고 있는 경우, 상위 엔티티에 역할이 추가될 때마다 하위 엔티티에도 역할을 추가해 주어야 해요. 예를 들어, 팀과 팀원처럼 사용자가 위계를 가지고 있는 경우, 팀 역할과 팀원 역할로 나누어서 관리되어야 해요.\n\n서론에서 제시한 예제 중 “알림 시나리오 페이지 접근 권한” 부분만 도식화해 본 그림이에요. RBAC은 역할의 위계를 표현하지 못하기 때문에 특정 엔티티에 접근할 수 있는 모든 권한을 명시해 주어야 해요.\n여기서 ReBAC(Relation-Based Access Control)을 사용하면 복잡한 권한을 더 쉽게 관리할 수 있어요. ReBAC은 구글의 Zanzibar에서도 사용하고 있는 권한 관리 방식인데요. RBAC과는 다르게 권한 간의 관계를 기반으로 권한을 제어해요.\nReBAC을 사용하면 팀과 팀원의 관계가 자연스럽게 연결돼요. 따라서 어떤 User가 직접 NotificationScenario에 접근할 수 있는 권한이 없더라도 member로 연결된 Team에 접근 권한이 있다면, 그 User는 NotificationScenario에 접근할 수 있다는 것을 알 수 있어요.\nTypeScript로 구현하기\n타입스크립트를 사용해서 ReBAC을 구현할 수 있는 다양한 방법들이 있어요. 저는 그중에서 graplix(https://github.com/daangn/graplix)를 사용했어요.\ngraplix는 당근에서 오픈소스로 공개한 ReBAC 기반 인증/인가 프레임워크예요. ReBAC의 권한 관계를 그래프로 표현할 수 있다는 점에서 착안해 GraphQL에서 영감을 받아 만들어졌어요. graplix를 사용해서 예시의 권한 관계를 구현해 볼게요.\nStep 1. 스키마 작성하기\n[엔티티 간의 관계 표현하기]\ngraplix는 스키마 우선 접근 방식(Schema-First Approach)을 사용해서 ReBAC을 만들고 있어요. 그래서 먼저 스키마를 작성해 주어야 해요.\nconst schema = {\n  Employee: {},\n  Team: {},\n  NotificationScenario: {}\n}\n사용하고자 하는 엔티티를 타입으로 작성하면 돼요. 이 엔티티들은 그래프의 노드가 되고, 스키마에서 그 관계를 명시해 줄 거예요.\n예를 들어 어떤 팀은 어떤 직원을 멤버로 가져요. 다시 말해보면,\nTeam과 Employee는 member라는 관계를 맺는다.\n라고 쓸 수 있어요. 그 관계를 스키마로 표현하면 아래와 같아요.\nconst schema = {\n  Employee: {},\n  Team: {\n    member: {\n      type: \"Employee\"\n    }\n  },\n  NotificationScenario: {}\n}\n그림으로 표현하면 아래처럼 표현할 수 있어요.\n\n[관계 참조하기]\n마찬가지로 접근하고자 하는 타겟 엔티티와의 관계도 표현할 수 있어요.\nconst schema = {\n  Employee: {},\n  Team: {\n    member: {\n      type: \"Employee\"\n    }\n  },\n  NotificationScenario: {\n    viewer_employee: {\n      type: \"Employee\"\n    },\n  }\n}\nEmployee라는 엔티티와 NotificationScenario 엔티티는 viewer_employee라는 관계를 맺는다고 정의할 수 있어요. 마찬가지로 Team 엔티티와 NotificationScenario 엔티티는 Team 타입의 관계를 맺어요.\nconst schema = {\n  Employee: {},\n  Team: {\n    member: {\n      type: \"Employee\"\n    }\n  },\n  NotificationScenario: {\n    viewer_team: {\n      type: \"Team\"\n    },\n    viewer_employee: {\n      type: \"Employee\"\n    },\n  }\n}\ngraplix에서는 when 절을 사용할 수 있는데요. 다른 관계를 참조해서 관계를 만들 수도 있어요. 예를 들어 can_access라는 관계를 만들고 “viewer_employee 관계일 때 접근 가능하다”라는 표현을 해주고 싶다면 아래처럼 써줄 수 있어요.\nconst schema = {\n  Employee: {},\n  Team: {\n    member: {\n      type: \"Employee\"\n    }\n  },\n  NotificationScenario: {\n    viewer_team: {\n      type: \"Team\"\n    },\n    viewer_employee: {\n      type: \"Employee\"\n    },\n    can_access: [{ when: \"viewer_employee\" }]\n  }\n}\n지금까지 작성된 관계를 그림으로 표현해 보면 아래와 같아요.\n\n[관계 상속하기]\nfrom 절을 이용해서 관계를 상속해 줄 수도 있어요. 예를 들어, viewer_team은 Team 타입이고, Team 엔티티에는 member로 관계를 맺고 있는 Employee라는 엔티티가 있어요. 즉 “viewer_team의 member로 있는 Employee도 NotificationScenario에 접근할 수 있다”라는 걸 표현해 주기 위해서는 아래와 같이 표현해 줄 수 있어요.\nconst schema = {\n  Employee: {},\n  Team: {\n    member: {\n      type: \"Employee\"\n    }\n  },\n  NotificationScenario: {\n    viewer_team: {\n      type: \"Team\"\n    },\n    viewer_employee: {\n      type: \"Employee\"\n    },\n    can_access: [\n      { when: \"viewer_employee\" },\n      { when: \"member\", from: \"viewer_team\" }\n    ]\n  }\n}\n추가된 관계를 그림으로 표현해 보면 아래와 같아요.\n\n이렇게 간단하게 작성한 스키마만으로,\n\n어떤 직원은 어떤 알림 시나리오에 접근 권한이 있다.\n어떤 팀에 속하는 직원은 어떤 알림 시나리오에 접근 권한이 있다.\n\n라는 것을 표현해 줄 수 있게 되었어요. 나머지 관계도 스키마로 표현하면 아래와 같아요.\nconst schema = {\n  Employee: {},\n  Team: {\n    member: {\n      type: \"Employee\"\n    }\n  },\n  NotificationScenario: {\n    viewer_team: {\n      type: \"Team\"\n    },\n    viewer_employee: {\n      type: \"Employee\"\n    },\n    reviewer: {\n      type: \"Employee\"\n    },\n    can_access: [{ when: \"viewer_employee\" }, { when: \"member\", from: \"viewer_team\" }],\n    can_approve: [{ when: \"reviewer\" }]\n  }\n}\n만들어진 스키마를 도식화하면 아래와 같아요.\n\n“알림 경험팀의 Dylan.lee가 NotificationScenario와 viewer 관계를 맺는가?”라는 조건은 다음과 같은 순서로 평가되어요.\n\nDylan.lee라는 Employee 엔티티는 NotificationsExperiences팀이라는 Team 엔티티와 member 관계를 맺는지\nNotificationsExperiences팀이라는 Team 엔티티와 NotificationScenario라는 엔티티가 viewer_team 관계를 맺는지\n\n두 관계가 참이라면 Dylan.lee는 NotificationScenario와 can_access 관계를 맺기 때문에 접근이 가능하다는 결과가 나와요.\nStep 2. Identify 함수 만들기\nidentify 함수는 타입을 알 수 없는 엔티티를 입력받고 어떤 엔티티인지 구분해 주는 함수예요.\n예를 들어 Employee는 email, Team은 code, NotificationScenario는 id를 식별자로 사용하고자 하면 아래와 같이 엔티티 타입을 만들어줄 수 있어요.\ntype Entity = {\n  Employee: { type: \"Employee\"; email: string };\n  Team: { type: \"Team\"; code: string };\n  NotificationScenario: { type: \"NotificationScenario\"; id: string };\n}\n이제 identify 함수는 이렇게 써줄 수 있어요.\nconst identify: GraplixIdentifier<Entity> = (entity) => {\n  switch (entity.type) {\n    case 'User':\n      return { type: 'User', id: entity.email };\n    case 'Team':\n      return { type: 'Team', id: entity.code };\n    case 'NotificationScenario':\n      return { type: 'NotificationScenario', id: entity.id };\n  }\n};\nStep 3. 리졸버 작성하기\n스키마를 정의했으니 이제 리졸버를 작성해 줘야 해요. 리졸버는 실제로 그 관계를 어떻게 평가할 것인지 작성해 주는 부분이에요. 여기서 Entity에 선언된 타입을 활용할 수 있어요. Employee 엔티티가 어떤 Team 엔티티와 member 관계를 맺는지 평가한다고 가정해 볼게요.\nconst resolvers = {\n  Employee: {},\n  Team: {\n    member: {\n      type: \"Employee\",\n      async resolve(entity, context) {}\n    }\n  },\n  // ...\n}\n그러면 작성한 Entity 타입에 의해 resolve 메소드의 첫 번째 인자(entity)는 { type: 'Team', code: string } 타입을 가지게 돼요. entity.code를 이용해서 특정 부서에 속하는 사용자 목록을 반환할 수 있어요.\nconst resolvers = {\n  Employee: {},\n  Team: {\n    member: {\n      type: \"Employee\",\n      async resolve(entity, context) {\n        const employees = await context.listAllEmployees();\n        const filteredEmployees = employees.filter((employee) => employee.code === code);\n        \n        return filteredEmployees;\n      }\n    }\n  },\n  // ...\n}\n이제 작성한 리졸버와 스키마를 graplix 함수에 넣으면 check 함수를 반환해요.\nexport const { check } = graplix({\n  resolvers,\n  schemas,\n  identify,\n  context: { listAllEmployees: () ⇒ /* ... */ }\n})\n이제 check 함수를 활용하면 복잡한 권한을 간편하게 선언적으로 다룰 수 있게 돼요.\nconst canAccess = await check({\n  user: req.user,\n  object: page,\n  relation: \"can_access\"\n});\nStep 4. 활용하기\n이렇게 구현한 인증/인가 기술을 활용해서 기능별 권한 제어를 구현한 사례를 소개해 드릴게요. 알림 센터에서 가장 많이 쓰이고 있는 형태는 게이트 컴포넌트예요. <Permission />이라는 이름으로 만들어 사용하고 있어요.\n<Permission /> 컴포넌트\n<Permission />은 확인하고 싶은 featureId만 전달해 주면 현재 보고 있는 사용자의 정보를 담아 권한을 확인하고, 결과에 따라 해당 컴포넌트를 보여줄지 말지 결정해 주는 컴포넌트예요. 내부적으로 usePermission 훅을 사용하기 때문에 권한을 체크하는 과정에서 Suspense가 발생해요.\ntype PermissionProps = {\n  featureId: FeatureId;\n  children: React.ReactNode;\n  fallback?: React.ReactNode;\n  decorate?: boolean | ((children: React.ReactNode) => React.ReactNode);\n};\nfeatureId와 children 외에 fallback과 decorate props도 볼 수 있는데요. fallback은 권한이 없거나 인증에 실패했을 경우 보여줄 ReactNode를 입력받도록 하는 prop이에요. decorate는 어떤 featureId로 인가를 해주고 있는지 시각적으로 보여주기 위한 prop이에요.\n<Permission /> 컴포넌트를 이용하면 이렇게 간단하게 인가 여부를 관리할 수 있어요.\n<Permission featureId=\"read:notification-scenario\">\n  <Tab>모니터링</Tab>\n</Permission>\n내부 구현\n실제 알림 센터에서는 BFF에서 graplix를 활용하고 있어요. 아래는 featureId에 따라 can_access를 평가하도록 설정한 코드예요.\napp.get(\"/api/authorize/:featureId\", (req, res) => {\n  const canAccess = await check({\n    user: req.email,\n    object: req.param.featureId,\n    relation: 'can_access',\n  });\n  res.send(canAccess);\n});\n그리고 usePermission 훅을 구현해서 리액트 클라이언트에서 쉽게 사용할 수 있어요. 예제 코드는 TanStack Query를 사용해서 구현한 예시예요.\nfunction usePermission(featureId) {\n  const { data: hasPermission } = useSuspenseQuery({ \n    queryKey: ['permission', featureId], \n    queryFn: async ({ queryKey }) => {\n      const [_, featureId] = queryKey;\n      const response = await fetch(`/api/authorize/${featureId}`);\n    \n      return response.json();\n    }\n  });\n  \n  return hasPermission;\n  }\n}\n이제 usePermission 훅을 이용해서 <Permission /> 컴포넌트를 만들 수 있어요. usePermission이 반환한 결과가 true라면 children을, false라면 fallback을 반환해 줌으로써 권한이 없는 경우를 핸들링해 줄 수 있어요.\nfunction Permission({ featureId, children, fallback, decorate }: PermissionProps): React.ReactNode {\n  const hasPermission = usePermission(featureId);\n  if (!hasPermission) {\n    return fallback;\n  }\n  return children;\n});\n기존에 사용하던 <Permission /> 컴포넌트가 어떤 컴포넌트를 제어하고 있는지 파악하기 어렵다는 의견이 있었어요. 실제로 전사에 정식으로 공개한 새로운 기능이 일부 구성원에게 보이지 않았던 문제가 있었고, 이것을 해결하기 위해 decorate를 추가했어요.\nfunction Permission({ featureId, children, fallback, decorate }: PermissionProps): React.ReactNode {\n  const hasPermission = usePermission(featureId);\n  if (!hasPermission) {\n    return fallback;\n  }\n  if (typeof decorate === 'function') {\n    return decorate(children);\n  }\n  if (decorate === true) {\n    return <Decorator featureId={featureId}>{children}</Decorator>;\n  }\n  return children;\n});\ndecorate prop은 boolean 혹은 render prop 형태로 제공돼요. 직접 사용하는 곳에서 컴포넌트를 만들어 주입할 수 있고, 그렇지 않으면 <Decorator /> 컴포넌트로 감싸게 돼요. 모든 곳에 테두리를 추가하면 시각적으로 복잡해지니 이렇게 분기를 추가해 두었어요.\n<Decorator /> 컴포넌트는 radix의 <Slot /> 컴포넌트를 활용했어요. 자식 컴포넌트의 레이아웃을 바꾸지 않고 border 스타일을 주입해 주는 용도로 사용하고 있어요.\nexport const Decorator = ({ children, featureId, ...props }: DecoratorProps) => {\n  return (\n    <div style={{ position: 'relative' }}>\n      <span\n        style={{\n          position: 'absolute',\n          top: 0,\n          right: 0,\n          // ...\n        }}\n      >\n        {featureId}\n      </span>\n      <Slot {...props} style={{ border: '2px solid hsl(var(--destructive))' }}>\n        {children}\n      </Slot>\n    </div>\n  );\n};\n<Decorator /> 컴포넌트가 적용된 실제 모습은 아래와 같아요.\n\n이제 이렇게 <Permission />으로 감싸주기만 하면, 더 이상 if 문을 작성할 필요 없이 복잡한 권한들을 단순하게 제어하고 관리/추적할 수 있게 되었어요.\n마치며\nReBAC을 적용하기 전에는 아래와 같이 써야 했던 코드가,\nfunction App() {\n  const user = getUserInfo();\n  const canViewMonitoring = user.team.codes.includes(\"CODE0001\");\n  \n  return (\n    <Tabs>\n      <Tab>기본 정보</Tab>\n      <Tab>통계</Tab>\n      {canViewMonitoring && (\n        <Tab>모니터링</Tab>\n      )}\n      <Tab>시나리오 버전 정보</Tab>\n    </Tabs>\n  );  \n}\n이렇게 같이 간단하고 선언적인 코드로 바뀔 수 있었어요.\nfunction App() {\n  return (\n    <Tabs>\n      <Tab>기본 정보</Tab>\n      <Tab>통계</Tab>\n      <Permission featureId=\"nx\">\n        <Tab>모니터링</Tab>\n      </Permission>\n      <Tab>시나리오 버전 정보</Tab>\n    </Tabs>\n  );  \n}\n이번 글에서는 ReBAC을 이용해서 점점 복잡해지는 권한을 잘 다루는 법을 알아봤어요. 그런데 ReBAC에도 한계가 있어요. 관계가 복잡해지고 노드가 많아질수록 N+1 문제가 생길 확률이 높아질 수 있어요. 저희 팀 같은 경우에는 Dataloader나 적절한 캐싱으로 N+1 문제를 방지하고 있어요. 이런 부분들을 인지하고 사용한다면 ReBAC과 그 구현체인 graplix에도 높은 잠재력이 있다고 생각해요. graplix에 관심이 생겼다면 독자 여러분도 graplix에 많은 기여해 보시길 바라요!\n\n참조\nhttps://permify.co/post/role-explosion/\nhttps://www.osohq.com/academy/relationship-based-access-control-rebac\nhttps://www.okta.com/blog/2023/06/authorization-back-to-basics-rbac-vs-rebac/\nhttps://www.okta.com/kr/identity-101/role-based-access-control-vs-attribute-based-access-control/\n\n구글처럼 복잡한 권한 쉽게 관리하기 feat. GraphQL was originally published in 당근 테크 블로그 on Medium, where people are continuing the conversation by highlighting and responding to this story.",
    "dc:creator": "Ho Yeon",
    "guid": "https://medium.com/p/9ce80d34d39b",
    "categories": [
      "programming",
      "access-control",
      "rebac",
      "authorization"
    ],
    "isoDate": "2025-03-20T08:21:54.000Z"
  }
]