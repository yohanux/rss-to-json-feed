[
  {
    "creator": "Hochul Shin",
    "title": "무신사 Web 테스트 자동화 2.0: 더 빠르고 효율적인 테스트 환경 만들기",
    "link": "https://medium.com/musinsa-tech/%EB%AC%B4%EC%8B%A0%EC%82%AC-web-%ED%85%8C%EC%8A%A4%ED%8A%B8-%EC%9E%90%EB%8F%99%ED%99%94-2-0-%EB%8D%94-%EB%B9%A0%EB%A5%B4%EA%B3%A0-%ED%9A%A8%EC%9C%A8%EC%A0%81%EC%9D%B8-%ED%85%8C%EC%8A%A4%ED%8A%B8-%ED%99%98%EA%B2%BD-%EB%A7%8C%EB%93%A4%EA%B8%B0-af3ad971e0af?source=rss----f107b03c406e---4",
    "pubDate": "Tue, 06 May 2025 22:02:28 GMT",
    "content:encoded": "<p>안녕하세요, <strong>무신사 QA팀 플랫폼 파트에서 Web 테스트 자동화를 담당하고 있는 신호철입니다.</strong></p><p>무신사 서비스가 2.0으로 개편됨에 따라 테스트 자동화도 함께 변화했습니다. 이번 글에서는 Web 테스트 자동화 2.0의 주요 변경 사항을 중심으로 소개하고자 합니다.</p><h3>왜 변경해야 했나요?</h3><p>무신사의 Web 테스트 자동화는 그동안 원활하게 운영되어 왔습니다. 그러나 무신사 2.0 개편으로 UI가 크게 달라지면서, 기존 자동화 방식을 그대로 재활용하기가 어렵게 되었습니다.</p><p>이로 인해 아래와 같은 문제가 발생하여, 새로운 방식으로의 전환이 필요하다고 판단했습니다.</p><h4>무신사 2.0 개편으로 기존 스크립트 재활용이 어려움</h4><p>무신사 2.0에서는 UI가 대폭 변경되었습니다. 이와 함께 테스트 케이스도 전면적으로 수정해야 했는데,</p><p>기존 테스트 케이스를 하나하나 분석하고 보완하기보다는 새로운 구조로 처음부터 다시 작성하는 것이 더 효율적이라 판단했습니다.</p><p>또한, 저희 팀은 Selenium을 활용하여 Web 요소를 제어할 때 XPath를 사용하고 있습니다.</p><p>UI가 변경될 때마다 XPath도 변경되므로, 기존 테스트 케이스를 수정하는 것보다 새로운 XPath를 찾는 것이 더 효과적이라고 판단했습니다.</p><h4>수동 테스트 케이스(Manual TC)와 자동화 테스트 케이스(Auto TC) 의 차이로 인한 유지보수 비용 증가</h4><p>수동 테스트 케이스와 자동화 테스트 케이스는 테스트 수행 방식이 다릅니다. 자동화 테스트에서는 테스트 간 전환 과정에서 이루어지는 모든 동작을 일일이 정의해야 하기 때문입니다.</p><ul><li>“ID 입력 후 로그인 버튼 선택 시 오류 메시지를 확인”하는 테스트</li><li>“PW 입력 후 로그인 버튼 선택 시 오류 메시지를 확인”하는 테스트</li></ul><p>첫 번째 테스트가 끝나면, 두 번째 테스트를 수행하기 전에 이미 입력되어 있던 ID를 삭제해야 하는데, 이는 수동 테스트 케이스에는 명시되어 있지 않지만 자동화 테스트에서는 반드시 고려해야 하는 부분입니다.</p><p>이처럼 자동화 테스트는 특정 시나리오를 중심으로 동작 하므로, 일부 기능이 변경될 경우 수동 테스트 케이스와 자동화 테스트 케이스를 모두 수정해야 하는 문제가 발생합니다.</p><p>결국, 이러한 반복 수정이 누적되어 자동화 테스트의 유지보수가 더욱 복잡해지는 원인이 되었습니다.</p><h3>어떤것이 변경됐나요?</h3><h4>CI/CD 파이프라인 단순해졌어요</h4><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*-R2V6VnKEMCwU69k\" /><figcaption>as-is</figcaption></figure><p>기존(1.0) 자동화 환경에서는 Spinnaker를 사용해 스케줄러와 Webhook Trigger를 구성하고, 이를 통해 Jenkins로부터 전달받은 배포 요청을 처리한 뒤 Kubernetes에 애플리케이션을 배포했습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*iLfUWksvZHqMvazb\" /><figcaption>to-be</figcaption></figure><p>2.0으로 전환하면서는 Github Actions만으로 CI/CD 파이프라인을 구성하여 Jenkins나 Spinaker 의 별도 서버 유지보수 할 필요 없어졌습니다. 또한 Github Actions에서 모든 테스트 실행 및 배포가 가능해 졌으므로, 단일 환경에서 모든 정보를 확인 할 수 있습니다.</p><p>실행 트리거 역시 유연해져,</p><ul><li>PR(풀 리퀘스트) 생성 시</li><li>특정 브랜치에 병합될 때</li><li>일정 시간 간격으로(스케줄 실행)</li></ul><p>와 같이 다양한 조건으로 자동 실행을 설정할 수 있습니다.</p><h4>단순한 테스트 데이터 수정은 DB에서 해결 가능해요</h4><p>2023년 기준으로, 한 달 동안의 테스트 스크립트 유지보수 작업 중 약 30%가 XPath 수정과 같은 단순 작업이었습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/672/0*_SPRffEgyQGlKLww\" /></figure><p>기존에는 테스트 데이터가 코드에 포함되어 있어, 간단한 데이터 변경에도 브랜치를 생성하고 PR을 올린 후 병합 및 배포까지 진행해야 했습니다. 이 과정에는 최소 15분 이상이 소요되었습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*hGV9IMhihsl0ozKJ\" /><figcaption>xpath 관리 web page</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*PH1m-HkvlAMIX8k_\" /><figcaption>xpath 관리 xpath 상세 페이지</figcaption></figure><p>2.0에서는 테스트 데이터를 전부 DB화하여 코드 수정 없이 Web 페이지에서 직접 수정할 수 있게 되었습니다.</p><p>이를 통해 중복되는 XPath를 최소화하고, 유지보수 비용을 절감할 수 있었습니다.</p><h4>수동 테스트 케이스와 자동화 테스트 케이스가 하나로 통일됐어요</h4><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*jdfLnYA8us8UlkNh\" /><figcaption>수동 테스트 케이스 구조와 자동화 테스트 케이스 구조</figcaption></figure><p>자동화 테스트(Auto TC)의 수행 단위를 수동 테스트(Manual TC)와 동일하게 맞추기 위해, “step” 기반 구조를 새롭게 도입하고, pytest의 fixture 기능을 적극 활용하는 방식으로 개선했습니다.</p><p>step 단위는 개별적인 기능으로 관리합니다. 예를 들어, 로그인 테스트의 경우 다음과 같은 단위로 나누어집니다.</p><ul><li>ID 입력 step</li><li>PW 입력 step</li><li>로그인 버튼 클릭 step</li></ul><p>이렇게 분리된 step은 재사용이 가능하며, 하나만 수정해도 전체 테스트 케이스에 일괄 반영할 수 있어 유지보수가 훨씬 쉬워집니다.</p><p>또한, pytest의 fixture를 활용하여 step을 재사용할 수 있어 테스트 유지보수가 더욱 쉬워졌습니다.</p><p>fixture는 Python의 decorator 문법 중 하나로, pytest에서는 테스트의 사전 조건을 자동으로 설정하거나 초기화하는 데 활용됩니다.</p><p>예를 들어, 다음과 같은 테스트 케이스가 있다고 가정해 보겠습니다.</p><blockquote><em>기대 결과: “로그인 버튼을 선택하면 마이페이지로 이동한다.”</em></blockquote><p>보통 이 테스트 케이스에서는 사전 조건으로 ID와 PW 입력이 필요합니다. 이때, fixture를 사용하면 자동화 테스트 케이스가 실행되기 전에 ID와 PW를 입력하도록 설정할 수 있으며, 테스트 케이스에서 로그인 버튼 선택 step만 호출하여 수행 절차를 간결하게 만들 수 있습니다.</p><p>이렇게<strong> </strong>자동화 테스트 케이스를 사전 조건과 수행 절차로 나누어 작성하면, 아래와 같은 장점 있습니다.</p><ul><li>실제 결함인지, 환경적 요인으로 인해 실패한 것인지 쉽게 구분할 수 있습니다.</li><li>로그인이 필요한 테스트 시나리오를 수행하기 전에 미리 로그인 상태를 설정할 수 있습니다.</li><li>테스트 종료 후 로그아웃을 자동으로 실행하여 계정 상태를 초기화하는 것도 가능합니다.</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/986/0*N0cbkItwfN78XMF9\" /><figcaption>testrail의 실제 테스트 케이스</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/623/0*34AErrIKN2agu8Bq\" /><figcaption>코드로 구현된 자동화 테스트 케이스</figcaption></figure><p>실제 테스트에서는 로그인 및 주문서 이동과 같은 사전 조건을 auto_login, move_order 같은 fixture로 분리하여 처리하고 있습니다.<br>각 테스트 케이스의 기대 결과는 자동화 테스트 케이스 내부에 정의되어 있으며, 이를 통해 수동 테스트 케이스와 동일한 흐름으로 자동화 테스트를 수행할 수 있습니다.</p><h4>중복 동작은 1번만 수행해요</h4><p>pytest의 fixture는 scope 설정을 통해, 공통 동작을 테스트 실행 중 한 번만 수행하도록 최적화할 수 있습니다.</p><p>예를 들어,</p><p>• 결제 완료 후 주문내역 페이지에서 상품 정보를 확인</p><p>• 결제 완료 후 주문내역 페이지에서 주문 상세 페이지로 이동 확인</p><p>이 두 테스트 모두 로그인과 결제 완료라는 동일한 사전 조건을 포함합니다. 기존에는 각 테스트마다 로그인과 결제 과정을 반복 수행해야 했지만, 2.0에서는 해당 동작을 한 번만 실행하도록 최적화했습니다.</p><p>과거에는 약 350개의 테스트 케이스를 수행하는 데 1시간 40분(100분)이 소요되었으나, 최적화를 통해 현재는 약 740개의 테스트 케이스(TC)를 동일한 시간 내에 완료할 수 있게 되어 기존 대비 테스트 효율이 크게 향상되었습니다.</p><p>물론 과거와 현재의 테스트 케이스 구성이 완전히 동일하지는 않지만, TC당 평균 수행 시간 기준으로 보면 과거에는 약 <strong>17.1초/TC</strong>, 현재는 약 <strong>8.1초/TC</strong>로 <strong>절반 이하 수준으로 단축</strong>되었습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*QWOrVNOCLXOsc4Xm\" /><figcaption>중복 동작 예시 1</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*Kyhkz3AIiQQggxoh\" /><figcaption>중복 동작 예시 2</figcaption></figure><h4>필요한 결과만 볼 수 있어요</h4><p>기존 1.0에서는 Allure 리포트를 사용하여 테스트 결과를 관리했습니다. Allure는 매우 상세한 리포트를 제공하지만, 별도의 서버를 유지·관리해야 하는 단점이 있었습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*XJ-5_tqzB76xGX_f\" /><figcaption>slack에 발송된 테스트 케이스 수행 결과 예시</figcaption></figure><p>2.0에서는 Allure 서버를 제거하고, 대신 Slack과 TestRail을 활용해 필수 결과만 간결하게 전송하는 방식으로 변경했습니다. 덕분에 별도 서버 운영에 필요한 리소스를 줄일 수 있었고, 테스트가 실패하면 스크린샷, 수행 시간, 실패 원인과 같은 중요 정보를 Slack에서 바로 확인할 수 있도록 개선되었습니다.</p><h3>마치면서…</h3><p>Web 테스트 자동화 2.0은 <strong>완성된 솔루션은 아닙니다.</strong></p><p>앞으로도 더 나은 테스트 자동화로 발전해나가기 위해 플랫폼 파트에선 다양한 방법을 시각으로 검토 중이며 AI도입, MW이 아닌 PC 환경 자동화, 파트너 어드민 자동화 등이 예정되어 있습니다.</p><p><strong>이러한 변화는 단지 기술적 시도에 그치지 않고, 그동안 축적된 경험과 노하우 위에서 이루어지고 있습니다. </strong>기존 코드를 기반으로 발전시킬 수 있었던 만큼, <strong>팀원들의 도움과 기존 담당자의 노력이 큰 힘이 되었습니다.</strong></p><p>앞으로의 여정에도 많은 관심과 응원 부탁드립니다.</p><h3>MUSINSA CAREER</h3><blockquote>무신사 QA팀은 품질을 문화로 만들어, 사용자에게 신뢰할 수 있는 서비스 경험을 제공하는 것을 지향합니다.</blockquote><blockquote>웹, 모바일 앱, 백엔드 API 등 다양한 영역에서 지속 가능한 품질 확보를 목표로, 자동화, 테스트 전략, 모니터링을 체계적으로 운영합니다. 또한, 기획·설계 단계부터 품질을 함께 설계하며 조기 결함 예방에 힘쓰고, 개발, 기획, CS, 운영 등 전 부서와의 유기적인 협업을 통해 품질 책임을 조직 전체로 확장합니다.</blockquote><blockquote>테스트 결과, 결함 데이터, 고객 피드백을 수치화하여 데이터 기반으로 품질을 진단하고 개선 방향을 제시하는 것 역시 무신사 QA팀의 핵심 역할입니다.<br>우리는 테스트를 넘어, 무신사의 서비스 가치와 고객 신뢰를 함께 만들어 갑니다.</blockquote><blockquote>🚀 <a href=\"https://corp.musinsa.com/ko/career/\">팀 무신사 채용 페이지</a> (무신사/29CM 전체 포지션 확인이 가능해요)</blockquote><blockquote>🚀 <a href=\"https://kr.linkedin.com/company/musinsacom\">팀 무신사 테크 소식을 받아보는 링크드인</a></blockquote><blockquote>🚀 <a href=\"https://medium.com/musinsa-tech\">무신사 테크 블로그</a></blockquote><blockquote>🚀 <a href=\"https://medium.com/29cm\">29CM 테크 블로그</a></blockquote><blockquote>🚀 <a href=\"https://www.youtube.com/@MUSINSATECH\">무신사 테크 유튜브 채널</a></blockquote><p><em>채용이 완료되면 공고가 닫힐 수 있으니 빠르게 지원해 주세요!</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=af3ad971e0af\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/musinsa-tech/%EB%AC%B4%EC%8B%A0%EC%82%AC-web-%ED%85%8C%EC%8A%A4%ED%8A%B8-%EC%9E%90%EB%8F%99%ED%99%94-2-0-%EB%8D%94-%EB%B9%A0%EB%A5%B4%EA%B3%A0-%ED%9A%A8%EC%9C%A8%EC%A0%81%EC%9D%B8-%ED%85%8C%EC%8A%A4%ED%8A%B8-%ED%99%98%EA%B2%BD-%EB%A7%8C%EB%93%A4%EA%B8%B0-af3ad971e0af\">무신사 Web 테스트 자동화 2.0: 더 빠르고 효율적인 테스트 환경 만들기</a> was originally published in <a href=\"https://medium.com/musinsa-tech\">MUSINSA tech</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "content:encodedSnippet": "안녕하세요, 무신사 QA팀 플랫폼 파트에서 Web 테스트 자동화를 담당하고 있는 신호철입니다.\n무신사 서비스가 2.0으로 개편됨에 따라 테스트 자동화도 함께 변화했습니다. 이번 글에서는 Web 테스트 자동화 2.0의 주요 변경 사항을 중심으로 소개하고자 합니다.\n왜 변경해야 했나요?\n무신사의 Web 테스트 자동화는 그동안 원활하게 운영되어 왔습니다. 그러나 무신사 2.0 개편으로 UI가 크게 달라지면서, 기존 자동화 방식을 그대로 재활용하기가 어렵게 되었습니다.\n이로 인해 아래와 같은 문제가 발생하여, 새로운 방식으로의 전환이 필요하다고 판단했습니다.\n무신사 2.0 개편으로 기존 스크립트 재활용이 어려움\n무신사 2.0에서는 UI가 대폭 변경되었습니다. 이와 함께 테스트 케이스도 전면적으로 수정해야 했는데,\n기존 테스트 케이스를 하나하나 분석하고 보완하기보다는 새로운 구조로 처음부터 다시 작성하는 것이 더 효율적이라 판단했습니다.\n또한, 저희 팀은 Selenium을 활용하여 Web 요소를 제어할 때 XPath를 사용하고 있습니다.\nUI가 변경될 때마다 XPath도 변경되므로, 기존 테스트 케이스를 수정하는 것보다 새로운 XPath를 찾는 것이 더 효과적이라고 판단했습니다.\n수동 테스트 케이스(Manual TC)와 자동화 테스트 케이스(Auto TC) 의 차이로 인한 유지보수 비용 증가\n수동 테스트 케이스와 자동화 테스트 케이스는 테스트 수행 방식이 다릅니다. 자동화 테스트에서는 테스트 간 전환 과정에서 이루어지는 모든 동작을 일일이 정의해야 하기 때문입니다.\n\n“ID 입력 후 로그인 버튼 선택 시 오류 메시지를 확인”하는 테스트\n“PW 입력 후 로그인 버튼 선택 시 오류 메시지를 확인”하는 테스트\n\n첫 번째 테스트가 끝나면, 두 번째 테스트를 수행하기 전에 이미 입력되어 있던 ID를 삭제해야 하는데, 이는 수동 테스트 케이스에는 명시되어 있지 않지만 자동화 테스트에서는 반드시 고려해야 하는 부분입니다.\n이처럼 자동화 테스트는 특정 시나리오를 중심으로 동작 하므로, 일부 기능이 변경될 경우 수동 테스트 케이스와 자동화 테스트 케이스를 모두 수정해야 하는 문제가 발생합니다.\n결국, 이러한 반복 수정이 누적되어 자동화 테스트의 유지보수가 더욱 복잡해지는 원인이 되었습니다.\n어떤것이 변경됐나요?\nCI/CD 파이프라인 단순해졌어요\nas-is\n기존(1.0) 자동화 환경에서는 Spinnaker를 사용해 스케줄러와 Webhook Trigger를 구성하고, 이를 통해 Jenkins로부터 전달받은 배포 요청을 처리한 뒤 Kubernetes에 애플리케이션을 배포했습니다.\nto-be\n2.0으로 전환하면서는 Github Actions만으로 CI/CD 파이프라인을 구성하여 Jenkins나 Spinaker 의 별도 서버 유지보수 할 필요 없어졌습니다. 또한 Github Actions에서 모든 테스트 실행 및 배포가 가능해 졌으므로, 단일 환경에서 모든 정보를 확인 할 수 있습니다.\n실행 트리거 역시 유연해져,\n\nPR(풀 리퀘스트) 생성 시\n특정 브랜치에 병합될 때\n일정 시간 간격으로(스케줄 실행)\n\n와 같이 다양한 조건으로 자동 실행을 설정할 수 있습니다.\n단순한 테스트 데이터 수정은 DB에서 해결 가능해요\n2023년 기준으로, 한 달 동안의 테스트 스크립트 유지보수 작업 중 약 30%가 XPath 수정과 같은 단순 작업이었습니다.\n\n기존에는 테스트 데이터가 코드에 포함되어 있어, 간단한 데이터 변경에도 브랜치를 생성하고 PR을 올린 후 병합 및 배포까지 진행해야 했습니다. 이 과정에는 최소 15분 이상이 소요되었습니다.\nxpath 관리 web pagexpath 관리 xpath 상세 페이지\n2.0에서는 테스트 데이터를 전부 DB화하여 코드 수정 없이 Web 페이지에서 직접 수정할 수 있게 되었습니다.\n이를 통해 중복되는 XPath를 최소화하고, 유지보수 비용을 절감할 수 있었습니다.\n수동 테스트 케이스와 자동화 테스트 케이스가 하나로 통일됐어요\n수동 테스트 케이스 구조와 자동화 테스트 케이스 구조\n자동화 테스트(Auto TC)의 수행 단위를 수동 테스트(Manual TC)와 동일하게 맞추기 위해, “step” 기반 구조를 새롭게 도입하고, pytest의 fixture 기능을 적극 활용하는 방식으로 개선했습니다.\nstep 단위는 개별적인 기능으로 관리합니다. 예를 들어, 로그인 테스트의 경우 다음과 같은 단위로 나누어집니다.\n\nID 입력 step\nPW 입력 step\n로그인 버튼 클릭 step\n\n이렇게 분리된 step은 재사용이 가능하며, 하나만 수정해도 전체 테스트 케이스에 일괄 반영할 수 있어 유지보수가 훨씬 쉬워집니다.\n또한, pytest의 fixture를 활용하여 step을 재사용할 수 있어 테스트 유지보수가 더욱 쉬워졌습니다.\nfixture는 Python의 decorator 문법 중 하나로, pytest에서는 테스트의 사전 조건을 자동으로 설정하거나 초기화하는 데 활용됩니다.\n예를 들어, 다음과 같은 테스트 케이스가 있다고 가정해 보겠습니다.\n기대 결과: “로그인 버튼을 선택하면 마이페이지로 이동한다.”\n보통 이 테스트 케이스에서는 사전 조건으로 ID와 PW 입력이 필요합니다. 이때, fixture를 사용하면 자동화 테스트 케이스가 실행되기 전에 ID와 PW를 입력하도록 설정할 수 있으며, 테스트 케이스에서 로그인 버튼 선택 step만 호출하여 수행 절차를 간결하게 만들 수 있습니다.\n이렇게 자동화 테스트 케이스를 사전 조건과 수행 절차로 나누어 작성하면, 아래와 같은 장점 있습니다.\n\n실제 결함인지, 환경적 요인으로 인해 실패한 것인지 쉽게 구분할 수 있습니다.\n로그인이 필요한 테스트 시나리오를 수행하기 전에 미리 로그인 상태를 설정할 수 있습니다.\n테스트 종료 후 로그아웃을 자동으로 실행하여 계정 상태를 초기화하는 것도 가능합니다.\ntestrail의 실제 테스트 케이스코드로 구현된 자동화 테스트 케이스\n실제 테스트에서는 로그인 및 주문서 이동과 같은 사전 조건을 auto_login, move_order 같은 fixture로 분리하여 처리하고 있습니다.\n각 테스트 케이스의 기대 결과는 자동화 테스트 케이스 내부에 정의되어 있으며, 이를 통해 수동 테스트 케이스와 동일한 흐름으로 자동화 테스트를 수행할 수 있습니다.\n중복 동작은 1번만 수행해요\npytest의 fixture는 scope 설정을 통해, 공통 동작을 테스트 실행 중 한 번만 수행하도록 최적화할 수 있습니다.\n예를 들어,\n• 결제 완료 후 주문내역 페이지에서 상품 정보를 확인\n• 결제 완료 후 주문내역 페이지에서 주문 상세 페이지로 이동 확인\n이 두 테스트 모두 로그인과 결제 완료라는 동일한 사전 조건을 포함합니다. 기존에는 각 테스트마다 로그인과 결제 과정을 반복 수행해야 했지만, 2.0에서는 해당 동작을 한 번만 실행하도록 최적화했습니다.\n과거에는 약 350개의 테스트 케이스를 수행하는 데 1시간 40분(100분)이 소요되었으나, 최적화를 통해 현재는 약 740개의 테스트 케이스(TC)를 동일한 시간 내에 완료할 수 있게 되어 기존 대비 테스트 효율이 크게 향상되었습니다.\n물론 과거와 현재의 테스트 케이스 구성이 완전히 동일하지는 않지만, TC당 평균 수행 시간 기준으로 보면 과거에는 약 17.1초/TC, 현재는 약 8.1초/TC로 절반 이하 수준으로 단축되었습니다.\n중복 동작 예시 1중복 동작 예시 2\n필요한 결과만 볼 수 있어요\n기존 1.0에서는 Allure 리포트를 사용하여 테스트 결과를 관리했습니다. Allure는 매우 상세한 리포트를 제공하지만, 별도의 서버를 유지·관리해야 하는 단점이 있었습니다.\nslack에 발송된 테스트 케이스 수행 결과 예시\n2.0에서는 Allure 서버를 제거하고, 대신 Slack과 TestRail을 활용해 필수 결과만 간결하게 전송하는 방식으로 변경했습니다. 덕분에 별도 서버 운영에 필요한 리소스를 줄일 수 있었고, 테스트가 실패하면 스크린샷, 수행 시간, 실패 원인과 같은 중요 정보를 Slack에서 바로 확인할 수 있도록 개선되었습니다.\n마치면서…\nWeb 테스트 자동화 2.0은 완성된 솔루션은 아닙니다.\n앞으로도 더 나은 테스트 자동화로 발전해나가기 위해 플랫폼 파트에선 다양한 방법을 시각으로 검토 중이며 AI도입, MW이 아닌 PC 환경 자동화, 파트너 어드민 자동화 등이 예정되어 있습니다.\n이러한 변화는 단지 기술적 시도에 그치지 않고, 그동안 축적된 경험과 노하우 위에서 이루어지고 있습니다. 기존 코드를 기반으로 발전시킬 수 있었던 만큼, 팀원들의 도움과 기존 담당자의 노력이 큰 힘이 되었습니다.\n앞으로의 여정에도 많은 관심과 응원 부탁드립니다.\nMUSINSA CAREER\n무신사 QA팀은 품질을 문화로 만들어, 사용자에게 신뢰할 수 있는 서비스 경험을 제공하는 것을 지향합니다.\n웹, 모바일 앱, 백엔드 API 등 다양한 영역에서 지속 가능한 품질 확보를 목표로, 자동화, 테스트 전략, 모니터링을 체계적으로 운영합니다. 또한, 기획·설계 단계부터 품질을 함께 설계하며 조기 결함 예방에 힘쓰고, 개발, 기획, CS, 운영 등 전 부서와의 유기적인 협업을 통해 품질 책임을 조직 전체로 확장합니다.\n테스트 결과, 결함 데이터, 고객 피드백을 수치화하여 데이터 기반으로 품질을 진단하고 개선 방향을 제시하는 것 역시 무신사 QA팀의 핵심 역할입니다.\n우리는 테스트를 넘어, 무신사의 서비스 가치와 고객 신뢰를 함께 만들어 갑니다.\n🚀 팀 무신사 채용 페이지 (무신사/29CM 전체 포지션 확인이 가능해요)\n🚀 팀 무신사 테크 소식을 받아보는 링크드인\n🚀 무신사 테크 블로그\n🚀 29CM 테크 블로그\n🚀 무신사 테크 유튜브 채널\n채용이 완료되면 공고가 닫힐 수 있으니 빠르게 지원해 주세요!\n\n무신사 Web 테스트 자동화 2.0: 더 빠르고 효율적인 테스트 환경 만들기 was originally published in MUSINSA tech on Medium, where people are continuing the conversation by highlighting and responding to this story.",
    "dc:creator": "Hochul Shin",
    "guid": "https://medium.com/p/af3ad971e0af",
    "categories": [
      "musinsatech",
      "qa",
      "quality-engineer",
      "quality-assurance",
      "musinsa"
    ],
    "isoDate": "2025-05-06T22:02:28.000Z"
  },
  {
    "creator": "김도영",
    "title": "무신사 X GitHub Copilot은 정말로 우리의 생산성을 높였을까?",
    "link": "https://medium.com/musinsa-tech/%EB%AC%B4%EC%8B%A0%EC%82%AC-x-github-copilot%EC%9D%80-%EC%A0%95%EB%A7%90%EB%A1%9C-%EC%9A%B0%EB%A6%AC%EC%9D%98-%EC%83%9D%EC%82%B0%EC%84%B1%EC%9D%84-%EB%86%92%EC%98%80%EC%9D%84%EA%B9%8C-de149ad7b7f6?source=rss----f107b03c406e---4",
    "pubDate": "Tue, 11 Feb 2025 04:31:26 GMT",
    "content:encoded": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*9Hhk2OBW8TNF9wO7A9_7bA.jpeg\" /></figure><blockquote>“GitHub Copilot이 정말 개발 생산성을 높여줄까?”</blockquote><p>2024년 말, 회의실에서 이 질문을 마주했습니다.</p><p>GitHub Copilot은 AI 기반 코딩 도구로 주목받았지만, 대부분의 평가는 개인적인 경험에 의존하고 있었습니다. 일부 개발자는 “코딩 속도가 빨라졌다”고 했고, 다른 이들은 “큰 차이를 못 느끼겠다”고 말했습니다. 그렇다면, <strong>Copilot이 실제로 개발 생산성을 높이는지, 데이터로 증명할 수 있을까?</strong></p><p>이 질문에 답하기 위해 <strong>직접 실험에 나섰습니다.</strong> 단순한 파일럿 테스트가 아닌, <strong>30명의 개발자가 참여하는 데이터 중심의 검증 프로젝트</strong>를 설계했습니다.</p><p>검증 방식은 두 가지였습니다.<br>첫째, <strong>Cycle Time, PR 머지 속도, 리뷰 시간</strong> 등 핵심 지표를 추적해 Copilot 도입 전후의 변화를 분석했습니다.<br>둘째, 개발자들의 실제 경험과 피드백을 수집하기 위해 <strong>심층 인터뷰와 설문조사를 병행</strong>했습니다.</p><p><strong>단순히 ‘Copilot이 좋다’는 결론이 아니라, ‘어떤 면에서, 얼마나 효과적인지’를 명확히 입증하고자 했습니다.</strong></p><p>이 글에서는 무신사에서 Copilot 도입을 어떻게 검증했는지, 어떤 데이터를 활용했으며, 다른 조직이 이를 어떻게 적용할 수 있는지 구체적으로 분석합니다.</p><h3>정말 도움이 될까? 다른 회사들은 뭐라고 할까?</h3><p>AI 기반 코드 자동 생성 도구는 개발자 생산성을 향상시키는 가능성을 보여주지만, 일부 기업 사례와 연구를 통해 Copilot의 단기적인 생산성 향상 효과는 확인되었습니다. 다만, 장기간 실무 환경에서의 코드 품질 변화, 유지보수 비용, 개발자 피로도에 대한 종합적인 데이터는 아직 축적 중입니다.</p><ul><li><a href=\"https://github.blog/news-insights/research/research-quantifying-github-copilots-impact-in-the-enterprise-with-accenture/\"><strong>GitHub 연구(2023)</strong></a>: Copilot을 활용한 그룹이 55.8% 더 빠르게 개발 작업을 완료했으며, 경험이 적은 개발자일수록 효과가 더 컸음.</li><li><a href=\"https://techblog.lotteon.com/copilot-%EC%97%85%EB%AC%B4-%EC%A0%81%EC%9A%A9%EA%B8%B0-c8810a5ee898\"><strong>롯데ON (2024)</strong></a>: Copilot을 활용해 <strong>코드 자동 완성과 채팅 기능을 통해 코드 가독성과 개발 속도 향상을 경험</strong>. 특히, 코드 리팩토링과 코드 리뷰 기능을 적극 활용하여 <strong>개발 효율성과 코드 품질 향상에 기여</strong>.</li><li><a href=\"https://techtopic.skplanet.com/github-copilot/\"><strong>SK플래닛 (2024)</strong></a>: Copilot을 <strong>백엔드, 프론트엔드, 데이터 엔지니어링 등 다양한 개발 업무에 적용</strong>. 코드 자동 완성과 코드 추천 기능을 활용하여 <strong>평균 개발 속도가 41.7% 향상</strong>. 또한, 데이터베이스 스키마 변경 작업 자동화로 작업 시간을 크게 단축.</li><li><a href=\"https://techblog.gccompany.co.kr/github-copilot-%EC%82%AC%EC%9A%A9%EA%B8%B0-66d4ae1d367a\"><strong>여기어때 (2024)</strong></a>: Copilot을 도입 후, <strong>자동 코드 생성 기능을 활용해 금융 데이터 분석 업무에서 생산성 향상을 경험</strong>. 그러나 <strong>과도한 자동 추천으로 인해 코딩 흐름이 끊기는 문제</strong>도 있음.</li><li><a href=\"https://www.cio.com/article/3541670/ai-%EC%BD%94%EB%94%A9-%EB%8F%84%EA%B5%AC-%EC%83%9D%EC%82%B0%EC%84%B1-%ED%9A%A8%EA%B3%BC-%EA%B1%B0%EC%9D%98-%EC%97%86%EB%8B%A4-%EB%B2%88%EC%95%84%EC%9B%83%EC%97%90%EB%8F%84-%EB%A7%88%EC%B0%AC.html\"><strong>Uplevel 연구 (2024)</strong></a>: Copilot과 같은 AI 코딩 도구가 개발자의 인식과 달리 <strong>실제 생산성 향상에는 미미한 영향을 미친다는 연구 결과 발표</strong>. 또한, 이러한 도구들이 개발자의 번아웃 감소에도 큰 기여를 하지 않는다는 점을 강조.</li></ul><p>이러한 연구들에서 Copilot이 개발자들의 작업 속도를 향상시키고, 반복 작업을 줄이는 데 효과적일 수 있음을 시사하지만, 코드 품질 및 오류 발생에 대한 장기적인 검증은 부족하다는 점도 드러냈습니다. 또한, 일부 연구에서는 이 기술의 효과가 기대보다 크지 않으며, 개발자의 능동적인 개입이 여전히 중요하다는 점을 지적하고 있습니다.</p><h3>코파일럿의 효과를 어떻게 측정하면 좋을까?</h3><p>기존 타사 사례와 선행 연구를 분석한 결과, Copilot의 도입 효과는 주로 개발 속도 향상에 집중되어 측정되었습니다. 그러나 이번 실험에서는 단순한 속도 측정뿐만 아니라 <strong>개발자의 실제 체감 변화를 분석하기 위해 설문조사를 병행</strong>하였습니다. 정량지표는 GitHub과 Jira에서 데이터를 추출하고 이를 Grafana 대시보드로 시각화해 도입 전후 지표 변화를 면밀히 측정했습니다.이를 통해 Copilot이 코드 작성 속도뿐만 아니라 팀 협업 방식에도 미친 영향을 파악하려 했습니다.</p><h4>정량적 지표 설계</h4><p>우선 정량적 분석을 위해 다음 핵심 지표를 추적했습니다. 이는 GitHub 연구 및 롯데ON, SK플래닛, 여기어때 사례에서 활용된 측정 항목을 참고했으며, 내부 SRE팀과 협의해 무신사의 개발 프로세스에 맞게 조정했습니다.</p><ul><li><strong>Total Cycle Time</strong>: 개발 시작부터 배포 완료까지 걸리는 시간</li><li><strong>PR Coding Time</strong>: 코드 작성 시작부터 PR 생성까지 걸리는 시간</li><li><strong>PR Review Time</strong>: PR 생성 후 코드 리뷰가 완료되기까지 걸리는 시간</li><li><strong>PR Pickup Time</strong>: PR 생성 후 리뷰가 시작되기까지 대기하는 시간</li><li><strong>PR 생성률 변화</strong>: Copilot 도입 전후 팀별 PR 생성 건수 증감 추이</li></ul><p>각 지표는 <strong>GitHub</strong>, <strong>Jira</strong>등 사내 시스템에서 자동·반자동으로 수집합니다. 이후 <strong>Grafana</strong> 같은 대시보드에서 시각화·분석하여, Copilot 도입 전후 변화와 그 원인을 한눈에 파악할 수 있게 했습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*YKltN7TgWpLOnEfxlLGAWA.png\" /><figcaption>그라파나 화면 예시</figcaption></figure><h4>정성적 지표 수집</h4><p>2개월간 두 차례에 걸쳐 설문조사를 실시했습니다. 1차 설문과 2차 설문을 통해 다음 항목들을 측정했습니다.</p><ul><li>업무 생산성 기여도</li><li>업무 만족도 변화</li><li>업무 피로도 변화</li><li>새로운 기술 학습 효과</li><li>코드 품질 개선 체감도</li></ul><p>특히 설문은 연차별, 직무별로 세분화하여 이 기술의 도입 효과가 개발자 경력과 업무 특성에 따라 어떻게 달라지는지 파악할 수 있도록 설계했습니다.</p><p>이와 같은 정성적 평가 방식은 타사에서도 활용되었으며, 이를 참고해 개발자의 실질적 업무 만족도와 학습 효과를 측정하는 데 집중했습니다.</p><h3>실제 효과는 어땠을까요?</h3><p>2024년 11월부터 12월까지, 약 30명의 개발자를 대상으로 Copilot의 도입이 개발 생산성과 코드 품질에 미친 영향을 파악하기 위해 정량적·정성적 데이터를 분석했습니다. 이를 통해 단순한 ‘느낌’이 아닌, 실제 데이터를 기반으로 효과를 검증할 수 있었습니다.</p><h4>주요 개선점</h4><ul><li><strong>Total Cycle Time 단축 </strong>: 개발 시작부터 배포 완료까지 걸리는 시간이 <strong>평균적으로 약 50% 단축</strong>되었습니다. Copilot 도입 후 일부 긴 배포 주기가 줄어든 것이 주요 원인으로 보이며, 반복적인 작업 자동화가 속도 개선에 기여했을 가능성이 큽니다. 다만, 기존에도 많은 수의 배포가 빠르게 이루어지고 있었다는 점을 고려해야 합니다.</li><li><strong>PR Coding·Review·Pickup Time 개선 </strong>: PR 생성부터 리뷰 완료까지 걸리는 시간이 <strong>평균적으로 약 40% 단축</strong>되었습니다. 하지만 기존에도 대부분의 PR이 빠르게 처리되고 있었기 때문에, 실제로 체감할 수 있는 변화는 크지 않을 수 있습니다.<br>Copilot 도입 후에는 <strong>리뷰까지 오랜 시간이 걸리던 일부 PR의 수가 줄어들면서 평균 값이 크게 낮아진 것으로 보입니다.</strong> 그러나 중앙값 기준으로 보면, 원래도 대부분의 PR이 즉시 리뷰되고 있었기 때문에, Copilot이 PR 처리 시간을 획기적으로 줄였다기보다는 <strong>전체적인 프로세스를 최적화하고, 예외적으로 지연되던 PR을 감소시키는 역할을 한 것으로 해석하는 것이 적절합니다.</strong></li></ul><p>이러한 결과는 copilot이 단순한 코드 자동 완성 기능을 넘어, 개발 워크플로우 전반을 가속화하는 데 기여했음을 보여줍니다.</p><h4>개발자들의 실제 체감</h4><p>도입 후, <strong>두 차례</strong>에 걸쳐 개발자 설문조사를 진행하여 피드백을 수집했습니다. 특히 <strong>1차 설문(도입 초기)과 2차 설문(활용이 익숙해진 시점) 간의 응답 변화를 비교</strong>하며, Copilot에 대한 인식과 활용 방식이 어떻게 발전했는지 분석했습니다.</p><p><strong>1차 설문 : 반복 작업 자동화에 대한 긍정 평가</strong></p><p>도입 초기, 개발자들은 <strong>반복 작업 자동화</strong>에서 큰 만족을 보였습니다. 특히 <strong>보일러플레이트 코드 작성, 테스트 코드 자동화</strong> 등의 작업에서 유용하다는 의견이 많았습니다.</p><ul><li><strong>“루틴한 작업에서 시간을 절약할 수 있어 만족스럽다.”</strong></li><li><strong>“테스트 코드 자동 생성 기능이 특히 편리하다.”</strong></li></ul><p>하지만, 추천 코드를 검토해야 하는 부담이 존재한다는 피드백도 있었습니다.</p><ul><li><strong>“추천된 코드가 정확한지 확인하는 과정이 필요해 오히려 시간이 더 걸릴 때도 있다.”</strong></li><li><strong>“내가 원하는 로직과 다르게 제안될 때 피로감을 느낀다.”</strong></li></ul><p><strong>2차 설문 : 활용 범위의 확장</strong></p><p>시간이 지나면서 활용 방식이 보다 정교해졌습니다. 초기에는 반복 작업을 줄이는 데 집중했다면, 이제는 <strong>PR 리뷰, 코드 리팩토링, 복잡한 로직 초안 작성 등 개발 프로세스 전반에 적극적으로 활용</strong>하는 모습이 나타났습니다.</p><ul><li><strong>“PR 내용을 요약해주거나, 복잡한 로직의 초안을 제공하는 데 도움이 된다.”</strong></li><li><strong>“리팩토링 아이디어를 얻거나 코드 리뷰 과정에서 가이드라인을 설정하는 용도로 활용 중이다.”</strong></li></ul><p>초기에 지적됐던 <strong>엉뚱한 코드 추천 문제</strong>는 여전히 남아 있었지만, 이를 미리 예측하고 검증하는 습관이 생기면서 피로도가 완화됐다는 피드백도 있었습니다.</p><p><strong>업무 만족도 및 피로도 변화</strong></p><p>1차 설문에서는 <strong>“반복적인 작업 부담 감소”</strong>가 가장 큰 장점으로 꼽혔습니다. 하지만 2차 설문에서는 <strong>“코드 리뷰 효율 개선”</strong>, <strong>“테스트 코드 작성 시간 절감”</strong> 등 <strong>팀 단위 협업에서의 효과</strong>가 더욱 강조되었습니다.</p><ul><li><strong>“코드 리뷰 시간이 단축되면서 팀 전체 개발 사이클이 빨라졌다.”</strong></li><li><strong>“테스트 코드 작성 시간이 절감되면서 검증 프로세스가 수월해졌다.”</strong></li></ul><p>다만, <strong>“Copilot이 제안하는 코드를 무비판적으로 수용할 위험이 있다”</strong>는 의견도 계속 관찰되었습니다. 이에 따라, <strong>팀 차원의 가이드라인 수립 및 교육 필요성</strong>이 논의되고 있습니다.</p><h3>AI와의 공존, 그리고 예상치 못한 난관들</h3><p>Copilot 도입 후 개발 프로세스는 분명히 달라졌습니다. 반복적인 코드 작성에서 해방된 개발자들은 더 중요한 문제 해결에 집중할 수 있었고, PR 리뷰나 코드 리팩토링 과정에서도 AI 기능을 활용하는 사례가 늘어났습니다. 하지만 모든 변화가 긍정적이지만은 않았습니다. 코드 작성 속도가 빨라졌지만, 코드 품질을 유지하기 위한 검토 과정은 더욱 중요해졌습니다.</p><p>이 기술이 개발 속도를 높이는 데 기여한 것은 분명하지만, 무비판적으로 자동완성을 수용하면 오히려 코드 품질이 저하될 가능성도 존재합니다. 결국, AI의 도움을 받되 비판적인 사고와 협업 문화가 뒷받침되어야 한다는 점이 다시 한 번 강조되었습니다.</p><p>이 실험을 통해 Copilot이 단순한 생산성 도구가 아니라 개발 문화까지 변화시키는 요소라는 사실을 확인했습니다. 앞으로도 이것을 효과적으로 활용할 수 있도록 개발자의 역량 강화, 코드 리뷰 문화 개선, 도구 활용 가이드라인 정립 등의 노력이 필요합니다.</p><p>이 글을 마무리하면서, 이 실험이 순탄하게 진행되었냐고 묻는다면… 글쎄요. 정량 데이터 수집은 거의 자동이었지만, 설문 응답을 받는 것은 기다림의 연속이었습니다. “바빠서 못 했어요”라는 답변을 몇 번이나 들었는지 기억도 안 납니다. 게다가 실험 기간 동안 대형 이벤트가 겹치는 바람에 지표를 제대로 추적하는 것도 쉽지 않았습니다.</p><p>여기에 하늘에서 들려오는 무언의 압박이… 더해지면서 부담감이 없었다면 거짓말이겠죠 (농담입니다, 진짜 농담입니다 🥹). 그리고 이제는 실험 결과를 정리해서 (설날에) 글까지 쓰고 있는 저 자신을 돌아보며, “과연 이 과정이 더 어려웠을까, 아니면 설문을 받는 게 더 어려웠을까?”라는 질문을 던지고 있습니다.</p><p>하지만, 그 모든 난관(최종, 최최종, 최최최종, 최최최최종, 찐최종)을 넘었고, Copilot의 실제 효과를 데이터로 확인할 수 있었습니다 (솔직히 반신반의 했습니다.) 앞으로도 AI가 개발 문화를 어떻게 변화시킬지 궁금합니다. 더불어 AI가 할 수 없는 일은 무엇일까요? 그리고 다음이 있다면… 설문 협조가 조금 더 원활해지기를 간절히 바랍니다.</p><p>이 글은 무신사 회원개발팀 <a href=\"https://medium.com/u/197d6dc9cd63\"><strong>Beomseok Kim</strong></a>과 세일개발팀 <a href=\"https://medium.com/u/c1ae407b7fc8\"><strong>김도영</strong></a>이 함께 작성하였습니다.</p><h3>Musinsa CAREER</h3><blockquote><strong>함께할 동료를 찾습니다.<br></strong><em>세일개발팀은 무신사 전체 상품의 가격 할인과 쿠폰 서비스를 담당하고 있어, 서비스 곳곳에 걸쳐 다양한 영역에서 보여집니다. 또한, 할인된 가격이나 쿠폰을 사용하는 다른 팀과도 긴밀히 협력해야 하므로, 커뮤니케이션이 매우 중요한 조직입니다.</em></blockquote><blockquote>전국민이 사용하는 1위 패션 플랫폼 무신사에서 기술로 비즈니스를 성장시키는 경험을 함께하고 싶으시다면 아래 채용 페이지를 통해 지원해 주세요!</blockquote><blockquote>🚀 <a href=\"https://corp.musinsa.com/ko/career/\">팀 무신사 채용 페이지</a> (무신사/29CM 전체 포지션 확인이 가능해요)</blockquote><blockquote>🚀 <a href=\"https://kr.linkedin.com/company/musinsacom\">팀 무신사 테크 소식을 받아보는 링크드인</a></blockquote><blockquote>🚀 <a href=\"https://medium.com/musinsa-tech\">무신사 테크 블로그</a></blockquote><blockquote>🚀 <a href=\"https://medium.com/29cm\">29CM 테크 블로그</a></blockquote><blockquote>🚀 <a href=\"https://www.youtube.com/@MUSINSATECH\">무신사 테크 유튜브 채널</a></blockquote><blockquote>채용이 완료되면 공고가 닫힐 수 있으니 빠르게 지원해 주세요!</blockquote><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=de149ad7b7f6\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/musinsa-tech/%EB%AC%B4%EC%8B%A0%EC%82%AC-x-github-copilot%EC%9D%80-%EC%A0%95%EB%A7%90%EB%A1%9C-%EC%9A%B0%EB%A6%AC%EC%9D%98-%EC%83%9D%EC%82%B0%EC%84%B1%EC%9D%84-%EB%86%92%EC%98%80%EC%9D%84%EA%B9%8C-de149ad7b7f6\">무신사 X GitHub Copilot은 정말로 우리의 생산성을 높였을까?</a> was originally published in <a href=\"https://medium.com/musinsa-tech\">MUSINSA tech</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "content:encodedSnippet": "“GitHub Copilot이 정말 개발 생산성을 높여줄까?”\n2024년 말, 회의실에서 이 질문을 마주했습니다.\nGitHub Copilot은 AI 기반 코딩 도구로 주목받았지만, 대부분의 평가는 개인적인 경험에 의존하고 있었습니다. 일부 개발자는 “코딩 속도가 빨라졌다”고 했고, 다른 이들은 “큰 차이를 못 느끼겠다”고 말했습니다. 그렇다면, Copilot이 실제로 개발 생산성을 높이는지, 데이터로 증명할 수 있을까?\n이 질문에 답하기 위해 직접 실험에 나섰습니다. 단순한 파일럿 테스트가 아닌, 30명의 개발자가 참여하는 데이터 중심의 검증 프로젝트를 설계했습니다.\n검증 방식은 두 가지였습니다.\n첫째, Cycle Time, PR 머지 속도, 리뷰 시간 등 핵심 지표를 추적해 Copilot 도입 전후의 변화를 분석했습니다.\n둘째, 개발자들의 실제 경험과 피드백을 수집하기 위해 심층 인터뷰와 설문조사를 병행했습니다.\n단순히 ‘Copilot이 좋다’는 결론이 아니라, ‘어떤 면에서, 얼마나 효과적인지’를 명확히 입증하고자 했습니다.\n이 글에서는 무신사에서 Copilot 도입을 어떻게 검증했는지, 어떤 데이터를 활용했으며, 다른 조직이 이를 어떻게 적용할 수 있는지 구체적으로 분석합니다.\n정말 도움이 될까? 다른 회사들은 뭐라고 할까?\nAI 기반 코드 자동 생성 도구는 개발자 생산성을 향상시키는 가능성을 보여주지만, 일부 기업 사례와 연구를 통해 Copilot의 단기적인 생산성 향상 효과는 확인되었습니다. 다만, 장기간 실무 환경에서의 코드 품질 변화, 유지보수 비용, 개발자 피로도에 대한 종합적인 데이터는 아직 축적 중입니다.\n\nGitHub 연구(2023): Copilot을 활용한 그룹이 55.8% 더 빠르게 개발 작업을 완료했으며, 경험이 적은 개발자일수록 효과가 더 컸음.\n롯데ON (2024): Copilot을 활용해 코드 자동 완성과 채팅 기능을 통해 코드 가독성과 개발 속도 향상을 경험. 특히, 코드 리팩토링과 코드 리뷰 기능을 적극 활용하여 개발 효율성과 코드 품질 향상에 기여.\nSK플래닛 (2024): Copilot을 백엔드, 프론트엔드, 데이터 엔지니어링 등 다양한 개발 업무에 적용. 코드 자동 완성과 코드 추천 기능을 활용하여 평균 개발 속도가 41.7% 향상. 또한, 데이터베이스 스키마 변경 작업 자동화로 작업 시간을 크게 단축.\n여기어때 (2024): Copilot을 도입 후, 자동 코드 생성 기능을 활용해 금융 데이터 분석 업무에서 생산성 향상을 경험. 그러나 과도한 자동 추천으로 인해 코딩 흐름이 끊기는 문제도 있음.\nUplevel 연구 (2024): Copilot과 같은 AI 코딩 도구가 개발자의 인식과 달리 실제 생산성 향상에는 미미한 영향을 미친다는 연구 결과 발표. 또한, 이러한 도구들이 개발자의 번아웃 감소에도 큰 기여를 하지 않는다는 점을 강조.\n\n이러한 연구들에서 Copilot이 개발자들의 작업 속도를 향상시키고, 반복 작업을 줄이는 데 효과적일 수 있음을 시사하지만, 코드 품질 및 오류 발생에 대한 장기적인 검증은 부족하다는 점도 드러냈습니다. 또한, 일부 연구에서는 이 기술의 효과가 기대보다 크지 않으며, 개발자의 능동적인 개입이 여전히 중요하다는 점을 지적하고 있습니다.\n코파일럿의 효과를 어떻게 측정하면 좋을까?\n기존 타사 사례와 선행 연구를 분석한 결과, Copilot의 도입 효과는 주로 개발 속도 향상에 집중되어 측정되었습니다. 그러나 이번 실험에서는 단순한 속도 측정뿐만 아니라 개발자의 실제 체감 변화를 분석하기 위해 설문조사를 병행하였습니다. 정량지표는 GitHub과 Jira에서 데이터를 추출하고 이를 Grafana 대시보드로 시각화해 도입 전후 지표 변화를 면밀히 측정했습니다.이를 통해 Copilot이 코드 작성 속도뿐만 아니라 팀 협업 방식에도 미친 영향을 파악하려 했습니다.\n정량적 지표 설계\n우선 정량적 분석을 위해 다음 핵심 지표를 추적했습니다. 이는 GitHub 연구 및 롯데ON, SK플래닛, 여기어때 사례에서 활용된 측정 항목을 참고했으며, 내부 SRE팀과 협의해 무신사의 개발 프로세스에 맞게 조정했습니다.\n\nTotal Cycle Time: 개발 시작부터 배포 완료까지 걸리는 시간\nPR Coding Time: 코드 작성 시작부터 PR 생성까지 걸리는 시간\nPR Review Time: PR 생성 후 코드 리뷰가 완료되기까지 걸리는 시간\nPR Pickup Time: PR 생성 후 리뷰가 시작되기까지 대기하는 시간\nPR 생성률 변화: Copilot 도입 전후 팀별 PR 생성 건수 증감 추이\n\n각 지표는 GitHub, Jira등 사내 시스템에서 자동·반자동으로 수집합니다. 이후 Grafana 같은 대시보드에서 시각화·분석하여, Copilot 도입 전후 변화와 그 원인을 한눈에 파악할 수 있게 했습니다.\n그라파나 화면 예시\n정성적 지표 수집\n2개월간 두 차례에 걸쳐 설문조사를 실시했습니다. 1차 설문과 2차 설문을 통해 다음 항목들을 측정했습니다.\n\n업무 생산성 기여도\n업무 만족도 변화\n업무 피로도 변화\n새로운 기술 학습 효과\n코드 품질 개선 체감도\n\n특히 설문은 연차별, 직무별로 세분화하여 이 기술의 도입 효과가 개발자 경력과 업무 특성에 따라 어떻게 달라지는지 파악할 수 있도록 설계했습니다.\n이와 같은 정성적 평가 방식은 타사에서도 활용되었으며, 이를 참고해 개발자의 실질적 업무 만족도와 학습 효과를 측정하는 데 집중했습니다.\n실제 효과는 어땠을까요?\n2024년 11월부터 12월까지, 약 30명의 개발자를 대상으로 Copilot의 도입이 개발 생산성과 코드 품질에 미친 영향을 파악하기 위해 정량적·정성적 데이터를 분석했습니다. 이를 통해 단순한 ‘느낌’이 아닌, 실제 데이터를 기반으로 효과를 검증할 수 있었습니다.\n주요 개선점\n\nTotal Cycle Time 단축 : 개발 시작부터 배포 완료까지 걸리는 시간이 평균적으로 약 50% 단축되었습니다. Copilot 도입 후 일부 긴 배포 주기가 줄어든 것이 주요 원인으로 보이며, 반복적인 작업 자동화가 속도 개선에 기여했을 가능성이 큽니다. 다만, 기존에도 많은 수의 배포가 빠르게 이루어지고 있었다는 점을 고려해야 합니다.\nPR Coding·Review·Pickup Time 개선 : PR 생성부터 리뷰 완료까지 걸리는 시간이 평균적으로 약 40% 단축되었습니다. 하지만 기존에도 대부분의 PR이 빠르게 처리되고 있었기 때문에, 실제로 체감할 수 있는 변화는 크지 않을 수 있습니다.\nCopilot 도입 후에는 리뷰까지 오랜 시간이 걸리던 일부 PR의 수가 줄어들면서 평균 값이 크게 낮아진 것으로 보입니다. 그러나 중앙값 기준으로 보면, 원래도 대부분의 PR이 즉시 리뷰되고 있었기 때문에, Copilot이 PR 처리 시간을 획기적으로 줄였다기보다는 전체적인 프로세스를 최적화하고, 예외적으로 지연되던 PR을 감소시키는 역할을 한 것으로 해석하는 것이 적절합니다.\n\n이러한 결과는 copilot이 단순한 코드 자동 완성 기능을 넘어, 개발 워크플로우 전반을 가속화하는 데 기여했음을 보여줍니다.\n개발자들의 실제 체감\n도입 후, 두 차례에 걸쳐 개발자 설문조사를 진행하여 피드백을 수집했습니다. 특히 1차 설문(도입 초기)과 2차 설문(활용이 익숙해진 시점) 간의 응답 변화를 비교하며, Copilot에 대한 인식과 활용 방식이 어떻게 발전했는지 분석했습니다.\n1차 설문 : 반복 작업 자동화에 대한 긍정 평가\n도입 초기, 개발자들은 반복 작업 자동화에서 큰 만족을 보였습니다. 특히 보일러플레이트 코드 작성, 테스트 코드 자동화 등의 작업에서 유용하다는 의견이 많았습니다.\n\n“루틴한 작업에서 시간을 절약할 수 있어 만족스럽다.”\n“테스트 코드 자동 생성 기능이 특히 편리하다.”\n\n하지만, 추천 코드를 검토해야 하는 부담이 존재한다는 피드백도 있었습니다.\n\n“추천된 코드가 정확한지 확인하는 과정이 필요해 오히려 시간이 더 걸릴 때도 있다.”\n“내가 원하는 로직과 다르게 제안될 때 피로감을 느낀다.”\n\n2차 설문 : 활용 범위의 확장\n시간이 지나면서 활용 방식이 보다 정교해졌습니다. 초기에는 반복 작업을 줄이는 데 집중했다면, 이제는 PR 리뷰, 코드 리팩토링, 복잡한 로직 초안 작성 등 개발 프로세스 전반에 적극적으로 활용하는 모습이 나타났습니다.\n\n“PR 내용을 요약해주거나, 복잡한 로직의 초안을 제공하는 데 도움이 된다.”\n“리팩토링 아이디어를 얻거나 코드 리뷰 과정에서 가이드라인을 설정하는 용도로 활용 중이다.”\n\n초기에 지적됐던 엉뚱한 코드 추천 문제는 여전히 남아 있었지만, 이를 미리 예측하고 검증하는 습관이 생기면서 피로도가 완화됐다는 피드백도 있었습니다.\n업무 만족도 및 피로도 변화\n1차 설문에서는 “반복적인 작업 부담 감소”가 가장 큰 장점으로 꼽혔습니다. 하지만 2차 설문에서는 “코드 리뷰 효율 개선”, “테스트 코드 작성 시간 절감” 등 팀 단위 협업에서의 효과가 더욱 강조되었습니다.\n\n“코드 리뷰 시간이 단축되면서 팀 전체 개발 사이클이 빨라졌다.”\n“테스트 코드 작성 시간이 절감되면서 검증 프로세스가 수월해졌다.”\n\n다만, “Copilot이 제안하는 코드를 무비판적으로 수용할 위험이 있다”는 의견도 계속 관찰되었습니다. 이에 따라, 팀 차원의 가이드라인 수립 및 교육 필요성이 논의되고 있습니다.\nAI와의 공존, 그리고 예상치 못한 난관들\nCopilot 도입 후 개발 프로세스는 분명히 달라졌습니다. 반복적인 코드 작성에서 해방된 개발자들은 더 중요한 문제 해결에 집중할 수 있었고, PR 리뷰나 코드 리팩토링 과정에서도 AI 기능을 활용하는 사례가 늘어났습니다. 하지만 모든 변화가 긍정적이지만은 않았습니다. 코드 작성 속도가 빨라졌지만, 코드 품질을 유지하기 위한 검토 과정은 더욱 중요해졌습니다.\n이 기술이 개발 속도를 높이는 데 기여한 것은 분명하지만, 무비판적으로 자동완성을 수용하면 오히려 코드 품질이 저하될 가능성도 존재합니다. 결국, AI의 도움을 받되 비판적인 사고와 협업 문화가 뒷받침되어야 한다는 점이 다시 한 번 강조되었습니다.\n이 실험을 통해 Copilot이 단순한 생산성 도구가 아니라 개발 문화까지 변화시키는 요소라는 사실을 확인했습니다. 앞으로도 이것을 효과적으로 활용할 수 있도록 개발자의 역량 강화, 코드 리뷰 문화 개선, 도구 활용 가이드라인 정립 등의 노력이 필요합니다.\n이 글을 마무리하면서, 이 실험이 순탄하게 진행되었냐고 묻는다면… 글쎄요. 정량 데이터 수집은 거의 자동이었지만, 설문 응답을 받는 것은 기다림의 연속이었습니다. “바빠서 못 했어요”라는 답변을 몇 번이나 들었는지 기억도 안 납니다. 게다가 실험 기간 동안 대형 이벤트가 겹치는 바람에 지표를 제대로 추적하는 것도 쉽지 않았습니다.\n여기에 하늘에서 들려오는 무언의 압박이… 더해지면서 부담감이 없었다면 거짓말이겠죠 (농담입니다, 진짜 농담입니다 🥹). 그리고 이제는 실험 결과를 정리해서 (설날에) 글까지 쓰고 있는 저 자신을 돌아보며, “과연 이 과정이 더 어려웠을까, 아니면 설문을 받는 게 더 어려웠을까?”라는 질문을 던지고 있습니다.\n하지만, 그 모든 난관(최종, 최최종, 최최최종, 최최최최종, 찐최종)을 넘었고, Copilot의 실제 효과를 데이터로 확인할 수 있었습니다 (솔직히 반신반의 했습니다.) 앞으로도 AI가 개발 문화를 어떻게 변화시킬지 궁금합니다. 더불어 AI가 할 수 없는 일은 무엇일까요? 그리고 다음이 있다면… 설문 협조가 조금 더 원활해지기를 간절히 바랍니다.\n이 글은 무신사 회원개발팀 Beomseok Kim과 세일개발팀 김도영이 함께 작성하였습니다.\nMusinsa CAREER\n함께할 동료를 찾습니다.\n세일개발팀은 무신사 전체 상품의 가격 할인과 쿠폰 서비스를 담당하고 있어, 서비스 곳곳에 걸쳐 다양한 영역에서 보여집니다. 또한, 할인된 가격이나 쿠폰을 사용하는 다른 팀과도 긴밀히 협력해야 하므로, 커뮤니케이션이 매우 중요한 조직입니다.\n전국민이 사용하는 1위 패션 플랫폼 무신사에서 기술로 비즈니스를 성장시키는 경험을 함께하고 싶으시다면 아래 채용 페이지를 통해 지원해 주세요!\n🚀 팀 무신사 채용 페이지 (무신사/29CM 전체 포지션 확인이 가능해요)\n🚀 팀 무신사 테크 소식을 받아보는 링크드인\n🚀 무신사 테크 블로그\n🚀 29CM 테크 블로그\n🚀 무신사 테크 유튜브 채널\n채용이 완료되면 공고가 닫힐 수 있으니 빠르게 지원해 주세요!\n\n무신사 X GitHub Copilot은 정말로 우리의 생산성을 높였을까? was originally published in MUSINSA tech on Medium, where people are continuing the conversation by highlighting and responding to this story.",
    "dc:creator": "김도영",
    "guid": "https://medium.com/p/de149ad7b7f6",
    "categories": [
      "ai",
      "code-review",
      "backend",
      "musinsa",
      "github-copilot"
    ],
    "isoDate": "2025-02-11T04:31:26.000Z"
  },
  {
    "creator": "Min Heo",
    "title": "추상화 & 리팩토링을 통한 해외 물류사 개발 비용 절감",
    "link": "https://medium.com/musinsa-tech/%EC%B6%94%EC%83%81%ED%99%94-%EB%A6%AC%ED%8C%A9%ED%86%A0%EB%A7%81%EC%9D%84-%ED%86%B5%ED%95%9C-%ED%95%B4%EC%99%B8-%EB%AC%BC%EB%A5%98%EC%82%AC-%EA%B0%9C%EB%B0%9C-%EB%B9%84%EC%9A%A9-%EC%A0%88%EA%B0%90-c2bcc8d9624d?source=rss----f107b03c406e---4",
    "pubDate": "Thu, 05 Dec 2024 07:08:17 GMT",
    "content:encoded": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ZudNVRJn_4c-aXcdmqWcUQ.png\" /></figure><p>안녕하세요.</p><p>무신사 구매개발실 클레임 개발팀에서 배송 도메인을 담당하는 백엔드 개발자 허민입니다.</p><p>이번 포스팅에서는 무신사의 글로벌 배송 시스템을 다루며, 추상화와 리팩토링을 통해 어떻게 예상치 못한 사이드 이펙트를 방지하고 개발 효율을 높였는지에 대해 소개하고자 합니다.</p><p>실무에서 자주 마주치는 확장성과 유지보수성의 문제를 해결하는 과정에서 얻은 경험을 공유드리니, 비슷한 문제로 고민 중인 분들에게 도움이 되기를 바랍니다.</p><h4>배경</h4><p>무신사는 국내뿐만 아니라 글로벌 영역에서도 서비스를 확장하여 운영 중입니다.</p><p>특히 글로벌 비즈니스는 물류 및 배송 비용 절감이 경쟁력의 핵심입니다.</p><p>초기에는 특정 해외 물류 업체(P사)와 단독으로 제휴하여 모든 해외 물류 업무를 처리했기 때문에, 시스템 구조가 단순하고 P사의 비즈니스 로직에 종속적인 형태였습니다.</p><p>이런 와중에 미국에서 더 저렴한 비용으로 비즈니스가 가능한 C사를 추가로 계약했고 이에 따른 연동 및 추가 개발을 진행해야 했습니다.</p><h4>고민</h4><p>보통 특정 케이스에 따라 로직을 다르게 처리하려면 if ~ else 구문을 많이 사용하고 저 역시 마찬가지 였습니다.</p><p>그런데 P사와 C사 케이스를 if로 분기처리하여 코드를 작성하던 중 여러가지 문제를 당면하게 되었습니다.</p><h4>문제점</h4><ol><li>P사 코드에 C사 코드를 추가하는 방식으로 수정 작업을 진행해야 했기 때문에, C사만 개발 진행하는 것보다 더 높은 개발 비용이 발생했습니다.</li><li>P사 코드를 수정하면, 기존에 잘 동작하던 P사 로직에서 버그가 발생할 수 있습니다.</li><li>따라서 P사 로직도 다시 테스트해야 하는 추가적인 작업이 필요해졌습니다.</li><li>향후 또 다른 물류 업체를 연동한다면, 위 문제들이 반복적으로 발생할 수밖에 없었습니다.</li></ol><p>위와 같은 문제를 해결하고자 좀 더 유연하고, 확장 가능성이 있도록 코드의 구조를 고쳐야겠다는 생각이 들었습니다.</p><p>따라서, <strong>코드 개발의 영향도를 최소화하면서도 확장성을 확보할 수 있는 설계 방식</strong>이 필요하다고 판단했고, 이에 따라 SOLID 원칙 중 <strong>OCP(개방-폐쇄 원칙)</strong> 적용을 고려하게 되었습니다.</p><blockquote><strong>OCP (Open-Closed Principle) — 개방/폐쇄 원칙</strong></blockquote><blockquote>OCP(Open-Closed Principle, 개방-폐쇄 원칙)는 SOLID 원칙 중 하나로, <strong>“확장에는 열려 있고, 수정에는 닫혀 있어야 한다</strong>”는 객체지향 설계 원칙입니다.</blockquote><blockquote>즉, 새로운 기능이 필요하면 기존 코드를 변경하지 않고 확장할 수 있도록 설계해야 한다는 의미입니다. 이를 통해 코드의 유지보수성과 확장성을 높일 수 있습니다.</blockquote><blockquote>OCP는 <strong>상속</strong>이나 <strong>인터페이스</strong>를 사용하여 기능을 확장하는 방식으로 구현할 수 있습니다.</blockquote><h4>배송 프로세스</h4><p>다시 이제 배송 얘기를 해보도록 하겠습니다.</p><p>큰맥락의 글로벌 배송 프로세스는 다음과 같이 요약할 수 있습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/788/1*l1UCqyxG4jGI4o152G29zw.png\" /></figure><p>해외 물류사는 국가별로 1개만 세팅되어 있습니다.</p><p>주문 → 배송 정보 생성 후 주문한 국가에 따라 해외 물류사가 세팅 됩니다.</p><p>위 다이어그램에서 추상화 &amp; 리팩토링 대상은 해외 물류사의 개입이 있는 비즈니스 플로우 구간인 <strong>해외 배송 접수 요청 이후 구간</strong>입니다.</p><h4>기존 코드</h4><p>배송 시스템의 주요 클래스는 아래와 같이 구성되어 있었습니다.</p><blockquote><strong>KafkaProductService</strong>: 물류 연동, 고객 이메일 발송, 모바일 푸시 등 비동기 후속 처리를 위한 <strong>Kafka</strong> 메시지 발행을 담당.</blockquote><blockquote><strong>CacheService</strong>: 해외 배송 접수 단계에서 데이터세팅을 위해 <strong>Redis</strong>를 사용하는 클래스</blockquote><blockquote><strong>DeliveryTrackerService</strong>: 해외 배송처리 로직을 포함한 비즈니스 로직을 담당하는 클래스</blockquote><p>기존 코드는 P사에만 특화된 로직만 있는 클래스만 존재했고, C사를 추가하려면 P사 로직에서 C사를 분기 처리하는 형태로 개발해야 했습니다.</p><p>단순 분기처리하는 형태로 개발하면 다음과 같이 됩니다.</p><pre>public class AdminShippingReleaseService {<br>  <br>  private final P_CacheService p_CacheService;<br>  private final C_CacheService c_CacheService;<br>  <br>  private void sendToExecutionBatchSet(DeliveryTrackerType type, List&lt;String&gt; numbers) {<br>  <br>    LocalDateTime now = LocalDateTime.now(); <br>    if (DeliveryTrackerType.P == type) {<br>  <br>      p_CacheService.removeFromBatchSet(getCurrentBatchTimeKey(now), numbers);<br>      p_CacheService.addToBatchSet(getNextBatchTimeKey(now), numbers);<br>  <br>    } else if (DeliveryTrackerType.C == type) {<br>  <br>      c_CacheService.removeFromBatchSet(getCurrentBatchTimeKey(now), numbers);<br>      c_CacheService.addToBatchSet(getNextBatchTimeKey(now), numbers);<br>  <br>    } <br>    // 추가가 필요할 시 계속해서 if ~ else를 반복하게 됨<br>          <br>  }<br>}</pre><p>이렇게 되면 기존 코드에 영향이 가게되고, 개발 비용자체도 증가하게되며 테스트 비용 역시 증가하게 됩니다.</p><p>이 문제를 해결하기 위해 새로운 물류사가 추가될 때마다 기존 코드에 영향이 없고 개발 &amp; 테스트 비용을 절감하기 위해 추상화 처리를 진행했습니다.</p><h3>리팩토링</h3><h4>추상화</h4><p>우선 공통된 행위(=메서드) 패턴과 해외 물류사를 기준으로 구현체를 추가할 수 있도록 추상화를 진행했습니다.</p><blockquote>해외 물류사는 <strong>DeliveryTrackerType</strong>이라는 <strong>Enum</strong>으로 만들었고, <strong>DeliveryTrackerType</strong>는 최초 주문시 세팅됩니다.</blockquote><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/704/1*TnJVY5En_HDMjcPvQBj5ag.png\" /></figure><p><strong>KafkaProductService</strong>와 <strong>CacheService</strong>는 각각 <strong>TrackerKafkaProduceService</strong>, <strong>TrackerCacheService</strong>라는 <strong>Interface</strong>로 추상화했습니다. 그리고 <strong>DeliveryTrackerService</strong>는 공통으로 사용될 메서드가 많아 <strong>abstract class</strong>로 리팩토링 하였습니다.</p><h4>Factory</h4><p>물류사별로 각각의 비즈니스에 맞춰 플로우를 진행하기 위해선 구현체를 찾아내는 과정이 필요한데, 이를 위해 <strong>DeliveryTrackerType</strong>에 맞는 구현체를 반환해주는 Factory를 추가 개발했습니다.</p><pre>@Component<br>public class TrackerCacheFactory {<br><br>  private Map&lt;DeliveryTrackerType, TrackerCacheService&gt; trackingCacheServiceMap;<br><br>  @Autowired<br>  public TrackerCacheFactory(Set&lt;TrackerCacheService&gt; deliveryTrackerServiceSet) {<br>    createStrategy(deliveryTrackerServiceSet);<br>  }<br><br>  public TrackerCacheService findStrategy(DeliveryTrackerType type) {<br>    if (Objects.isNull(type) || DeliveryTrackerType.NONE.equals(type)) {<br>      return null;<br>  }<br><br>    return trackingCacheServiceMap.get(type);<br>  }<br><br>  private void createStrategy(Set&lt;TrackerCacheService&gt; trackerCacheServiceSet) {<br>    trackingCacheServiceMap = new HashMap&lt;&gt;();<br><br>    trackerCacheServiceSet.stream()<br>      .forEach(trackingCacheService -&gt; trackingCacheServiceMap.put(<br>      trackingCacheService.getTrackerType(),<br>      trackingCacheService<br>    ));<br>  }<br>}</pre><p>TrackerCacheFactory 클래스에는 해외 물류사 타입 DeliveryTrackerType를 key로, 각 물류사의 TrackerCacheSerive를 value로는 담고 있는 Map 형태의 trackingCacheServiceMap 멤버 변수가 있습니다.</p><p>이후 실제 사용시에는 findStrategy() 메소드에 파라미터로 DeliveryTrackerType을 전달하고, 해당 타입에 맞는 구현체를 받아서 사용합니다.</p><p>이렇게 추상화를 했을때 클라이언트 코드가 어떻게 바뀌는지 보겠습니다.</p><pre>public class AdminShippingReleaseService {<br><br>  private final TrackerCacheFactory trackerCacheFactory;<br><br>  private void sendToExecutionBatchSet(DeliveryTrackerType type, List&lt;String&gt; numbers) {<br>  <br>    TrackerCacheService trackerCacheService = trackerCacheFactory.findStrategy(type);<br>  <br>    LocalDateTime now = LocalDateTime.now();<br>    trackerCacheService.removeFromBatchSet(getCurrentBatchTimeKey(now), numbers);<br>    trackerCacheService.addToBatchSet(getNextBatchTimeKey(now), numbers);<br><br>    // after caching process<br>  <br>  }<br>}</pre><p>이제 리팩토링 이후에는 C사가 아닌 다른 물류 업체(F사)를 추가하게 되더라도 기존 코드에 영향을 주지 않고 TrackerCacheService를 구현한 F사 구현체만 개발하면 됩니다.</p><p>이렇게 추상화를 하게 되면 OCP를 만족하여 유연하고 확장성이 있는 코드를 작성할 수 있습니다.</p><p>그러나, <strong>상속</strong>은 강한 결합도가 생기는게 단점입니다.</p><p>인터페이스나 부모 클래스에서 정의한 메소드는 반드시 구현체나 하위 클래스에서 오버라이드 해야합니다.</p><p>그리고 클라이언트 코드 입장에서는 여러가지 인터페이스를 의존하게 되어 많은 책임을 지고 있습니다. 추가로 위의 예시를 보면 캐싱 처리에서 공통 처리 로직을 추가해야한다면 인터페이스를 추상 클래스로 변환하는 등 다른 수정 포인트가 생깁니다.</p><p>조합(Composition)을 사용하면 위의 문제 외에도 많은 문제를 해결할수 있는 좀 더 유연한 구조를 가질 수 있습니다.</p><h4>조합(Composition)</h4><p>우선 간단하게 조합(Composition)에 대해 알아보겠습니다.</p><p>조합이라는 용어는 생소할 수 있지만, 실은 많은 개발자가 이미 사용하고 있는 코딩 기법입니다. 조합은 하나의 클래스가 다른 클래스의 인스턴스를 <strong>필수로 멤버 변수로 포함하여 관계를 만드는 것을 의미합니다.</strong></p><p>흔히 “<strong>has-a 관계</strong>”로 설명되며, 상속의 “<strong>is-a 관계</strong>”와 대비되는 개념입니다.</p><p>조합(Composition)을 사용한 클래스 다이어그램은 다음과 같습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/807/1*wyzblf2NqrvVW154c16F0A.png\" /></figure><p>Composition을 생성하는 Factory 코드</p><pre>@Component<br>public class TrackerCacheFactory {<br><br>  private Map&lt;DeliveryTrackerType, TrackerCacheService&gt; trackerCacheServiceMap;<br>  private final CommonCacheService commonCacheService; <br><br>  public CacheProcessComposition getCacheProcessComposition(DeliveryTrackerType deliveryTrackerType) {<br>    return new CacheProcessComposition(this.findStrategy(deliveryTrackerType), commonCacheService);<br>  }<br><br>  private TrackerCacheService findStrategy(DeliveryTrackerType deliveryTrackerType) {<br>    return trackerCacheServiceMap.get(deliveryTrackerType);<br>  }<br><br>  private void createStrategy(Set&lt;TrackerCacheService&gt; trackerCacheServiceSet) {<br>    trackingCacheServiceMap = new HashMap&lt;&gt;();<br><br>    trackerCacheServiceSet.stream()<br>      .forEach(trackingCacheService -&gt; <br>              trackingCacheServiceMap.put(<br>                trackingCacheService.getTrackerType(),<br>                trackingCacheService<br>        ));<br>  }<br>}</pre><p>Factory는 TrackerCacheService의 여러가지 구현체를 Map으로 가지고 있습니다.</p><p>Composition은 Type에 따른 적합한 해외물류사 TrackerCacheService 구현체를 파라미터로 넘겨줘 생성합니다. 그리고 캐싱 처리에 공통 로직이 들어있는 CommonCacheService도 파라미터로 넘겨줍니다.</p><pre>public class CacheProcessComposition {<br><br>  private final TrackerCacheService trackerCacheService;<br>  private final CommonCacheService commonCacheService;<br><br>  public CacheProcessComposition(TrackerCacheService trackerCacheService, CommonCacheService commonCacheService) {<br>    this.trackerCacheService = trackerCacheService;<br>    this.commonCacheService = commonCacheService;<br>  }<br> <br>  public void trackerCachingProcess(List&lt;String&gt; numbers) {<br><br>    LocalDateTime now = LocalDateTime.now();<br>    commonCacheService.beforeCachingProcess();<br><br>    trackerCacheService.removeFromBatchSet(getCurrentBatchTimeKey(now), numbers);<br>    trackerCacheService.addToBatchSet(getNextBatchTimeKey(now), numbers);<br><br>    commonCacheService.afterCachingProcess();<br>  }<br>}</pre><p>CacheProcessComposition에서는 모든 해외물류사 개별 로직을 하는 TrackerCacheService의 구현체와 캐싱 공통로직을 담당하는 CommonCacheService를 멤버 변수로 가지고 있고 모든 캐싱 처리는 여기서 담당합니다.</p><p>클라이언트 클래스에서는 캐싱처리(=trackerCachingProcess) 메소드만 호출하면 로직이 완성됩니다.</p><pre>public class AdminShippingReleaseService {<br><br>  private final TrackerCacheFactory factory;<br>  <br>  public void release(DeliveryTrackerType type, List&lt;String&gt; numbers) {<br>    CacheProcessComposition composition = factory.getCacheProcessComposition(type);<br>    composition.trackerCachingProcess(numbers);<br><br>    // after caching process<br>  }<br>}</pre><blockquote>클라이언트 코드는 이전보다 훨씬 깔끔해졌습니다!!</blockquote><p>Composition을 통해 앞으로 추가적인 캐싱 로직이 필요하면 추상화 &amp; 상속과 관계없이 CacheProcessComposition안에서 로직을 추가하면 되기 때문에 좀 더 유연해졌습니다. 공통 로직 역시 클라이언트 코드에서는 불필요하게 작성하거나 신경쓸 필요가 없어졌습니다.</p><p>이렇게 합성(Composition)을 이용하면 좀 더 유연하게 코드를 구성할 수 있습니다.</p><h3>임팩트</h3><p><strong>OCP</strong>원칙을 적용하며 리팩토링을 하는 과정에서 얻는 임팩트는 크게 2가지가 있습니다.</p><ol><li>개발 기간 &amp; 비용 절감</li><li>테스트 비용 절감</li></ol><h4>개발 기간 &amp; 비용 절감</h4><p>리팩토링의 효과는 특히 C사 이후 F사를 추가하면서 더욱 명확하게 드러났습니다.</p><p>C사 개발 시에는 추상화 구조를 설계하고, 비즈니스 로직을 유연하게 적용할 수 있도록 코드를 리팩토링하는 데 <strong>약 한 달의 시간이 소요</strong>되었습니다.</p><p>그러나 이러한 초기 추상화 작업이 완료된 덕분에, 이후 F사와 추가 계약을 체결한 후의 개발 작업에서는 비즈니스 로직을 추가하는 데만 집중할 수 있었습니다.</p><p>그 결과, F사에 맞춘 개발 작업은 <strong>오직 1주 만에 완료</strong>할 수 있었으며, 이는 C사 작업 기간의 <strong>4분의 1</strong>에 불과한 시간입니다.</p><p>초기 추상화 및 리팩토링 작업으로 인해 상당한 시간이 소요되었지만, 이는 이후의 개발 효율을 획기적으로 높이는 결과를 가져왔습니다.</p><p>특히 새로운 고객사나 파트너사의 요구사항을 반영할 때, 코드 변경 없이 간단히 기능을 확장할 수 있어 <strong>개발 기간 단축과 비용 절감</strong>이라는 두 가지 주요 효과를 얻을 수 있었습니다.</p><p>따라서, 초기 추상화 작업은 단기적인 개발 비용을 소폭 증가시킬 수는 있지만 전체 프로젝트의 관점에서 <strong>장기적인 비용 절감과 유지보수 효율성을 확보</strong>하는 데 큰 도움이 되었음을 확인할 수 있습니다.</p><h4>테스트 비용 절감</h4><ul><li><strong><em>테스트 케이스의 일관성 확보</em></strong></li><li>각 하위 클래스마다 별도의 케이스를 작성하지 않고 공통된 테스트 시나리오를 사용해 <strong>효율적으로 테스트</strong>할 수 있습니다.</li><li><strong><em>확장된 기능의 안전한 검증</em></strong></li><li>OCP를 적용한 시스템에서는 새로운 기능이 추가될 때 기존 코드가 안전하게 보호되므로, <strong>기존 기능이 영향을 받지 않았다는 신뢰를 가지고</strong> 검증할 수 있습니다.</li><li>따라서 기능 확장이 발생해도 새로운 기능에 대한 테스트만 추가하면 되므로 <strong>테스트 범위와 노력의 감소</strong>가 이루어집니다.</li><li><strong><em>테스트의 가독성과 유지보수성 향상</em></strong></li><li>OCP가 준수된 코드는 일반적으로 <strong>모듈화가 잘 되어 있어 각 모듈의 테스트 케이스가 명확하고 구조적</strong>으로 작성됩니다.</li><li>각 모듈의 역할을 이해하기 쉽고, 모듈 별로 테스트를 독립적으로 작성하여 유지보수할 수 있으므로, 테스트 케이스의 가독성과 유지보수성이 크게 향상됩니다.</li><li><strong><em>버그 발생 가능성 감소</em></strong></li><li>추상화가 잘 되어 있어 모듈 간 의존성이 낮고 변경의 영향이 제한적이므로, <strong>기능 추가나 변경 시 기존 코드에서 발생할 수 있는 버그 가능성이 낮아</strong> 집니다.</li></ul><h3>마치며</h3><p>무신사는 지속적으로 성장하기 위해 굉장히 빠르고 다양한 방면에서 비즈니스가 진행되고 있습니다.</p><p>만약 코드가 확장성과 유지보수성을 고려하지 않고 개발된다면, 비즈니스 속도가 느려지고 이는 곧 무신사의 성장 속도에도 부정적인 영향을 미칠 수밖에 없습니다. 따라서 개발자는 항상 확장, 유연함과 유지보수를 고려하면서 개발할 필요가 있습니다.</p><p>이번 리팩토링을 통해 특정 해외 물류 업체에 로직이 강하게 종속되지 않고 확장할 수 있게되었고, 추후 비즈니스 변화에도 유연하게 대응할 수 있게 되었습니다.</p><p>이 글이 비슷한 문제로 고민하는 분들께 도움이 되길 바라며, 추상화와 설계 원칙이 시스템 확장성 확보에 어떻게 기여할 수 있는지 고민해보는 계기가 되었으면 좋겠습니다.</p><p>긴글을 읽어주셔서 감사합니다.</p><h3>Musinsa CAREER</h3><blockquote><em>함께할 동료를 찾습니다.</em></blockquote><blockquote><em>무신사에서는 지속적인 성장을 하기 위해 끊임없이 변화를 추구하고 있습니다. 이런 환경속에서 이번 포스팅과 같은 개발 방향을 고민하고 관심이 있는 분을 모시고 있습니다.</em></blockquote><blockquote>저희 클레임 개발팀은 클레임과 배송 도메인을 주로 담당하고 있습니다.</blockquote><blockquote>무신사에서 주문 이후의 비즈니스를 책임지고 고객에게 만족스러운 경험을 제공하기 위해 다양한 서비스를 개발 &amp; 운영하고 있습니다. 변화하는 커머스 환경에서 기술적 도전을 즐기고, 지속 가능한 가치를 만들어갈 수 있는 여러분의 지원을 기다립니다.</blockquote><blockquote><em>전국민이 사용하는 1위 패션 플랫폼 무신사에서 기술로 비즈니스를 성장시키는 경험을 함께하고 싶으시다면 아래 채용 페이지를 통해 지원해주세요!</em></blockquote><blockquote><em>🚀 </em><a href=\"https://corp.musinsa.com/ko/career/\"><em>팀 무신사 채용 페이지</em></a><em> (무신사/29CM 전체 포지션 확인이 가능해요)</em></blockquote><blockquote><em>🚀 </em><a href=\"https://kr.linkedin.com/company/musinsacom\"><em>팀 무신사 테크 소식을 받아보는 링크드인</em></a></blockquote><blockquote><em>🚀 </em><a href=\"https://medium.com/musinsa-tech\"><em>무신사 테크 블로그</em></a></blockquote><blockquote><em>🚀 </em><a href=\"https://medium.com/29cm\"><em>29CM 테크 블로그</em></a></blockquote><blockquote><em>🚀 </em><a href=\"https://www.youtube.com/@MUSINSATECH\"><em>무신사 테크 유튜브 채널</em></a></blockquote><blockquote><em>채용이 완료되면 공고가 닫힐 수 있으니 빠르게 지원해 주세요!</em></blockquote><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c2bcc8d9624d\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/musinsa-tech/%EC%B6%94%EC%83%81%ED%99%94-%EB%A6%AC%ED%8C%A9%ED%86%A0%EB%A7%81%EC%9D%84-%ED%86%B5%ED%95%9C-%ED%95%B4%EC%99%B8-%EB%AC%BC%EB%A5%98%EC%82%AC-%EA%B0%9C%EB%B0%9C-%EB%B9%84%EC%9A%A9-%EC%A0%88%EA%B0%90-c2bcc8d9624d\">추상화 &amp; 리팩토링을 통한 해외 물류사 개발 비용 절감</a> was originally published in <a href=\"https://medium.com/musinsa-tech\">MUSINSA tech</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "content:encodedSnippet": "안녕하세요.\n무신사 구매개발실 클레임 개발팀에서 배송 도메인을 담당하는 백엔드 개발자 허민입니다.\n이번 포스팅에서는 무신사의 글로벌 배송 시스템을 다루며, 추상화와 리팩토링을 통해 어떻게 예상치 못한 사이드 이펙트를 방지하고 개발 효율을 높였는지에 대해 소개하고자 합니다.\n실무에서 자주 마주치는 확장성과 유지보수성의 문제를 해결하는 과정에서 얻은 경험을 공유드리니, 비슷한 문제로 고민 중인 분들에게 도움이 되기를 바랍니다.\n배경\n무신사는 국내뿐만 아니라 글로벌 영역에서도 서비스를 확장하여 운영 중입니다.\n특히 글로벌 비즈니스는 물류 및 배송 비용 절감이 경쟁력의 핵심입니다.\n초기에는 특정 해외 물류 업체(P사)와 단독으로 제휴하여 모든 해외 물류 업무를 처리했기 때문에, 시스템 구조가 단순하고 P사의 비즈니스 로직에 종속적인 형태였습니다.\n이런 와중에 미국에서 더 저렴한 비용으로 비즈니스가 가능한 C사를 추가로 계약했고 이에 따른 연동 및 추가 개발을 진행해야 했습니다.\n고민\n보통 특정 케이스에 따라 로직을 다르게 처리하려면 if ~ else 구문을 많이 사용하고 저 역시 마찬가지 였습니다.\n그런데 P사와 C사 케이스를 if로 분기처리하여 코드를 작성하던 중 여러가지 문제를 당면하게 되었습니다.\n문제점\n\nP사 코드에 C사 코드를 추가하는 방식으로 수정 작업을 진행해야 했기 때문에, C사만 개발 진행하는 것보다 더 높은 개발 비용이 발생했습니다.\nP사 코드를 수정하면, 기존에 잘 동작하던 P사 로직에서 버그가 발생할 수 있습니다.\n따라서 P사 로직도 다시 테스트해야 하는 추가적인 작업이 필요해졌습니다.\n향후 또 다른 물류 업체를 연동한다면, 위 문제들이 반복적으로 발생할 수밖에 없었습니다.\n\n위와 같은 문제를 해결하고자 좀 더 유연하고, 확장 가능성이 있도록 코드의 구조를 고쳐야겠다는 생각이 들었습니다.\n따라서, 코드 개발의 영향도를 최소화하면서도 확장성을 확보할 수 있는 설계 방식이 필요하다고 판단했고, 이에 따라 SOLID 원칙 중 OCP(개방-폐쇄 원칙) 적용을 고려하게 되었습니다.\nOCP (Open-Closed Principle) — 개방/폐쇄 원칙\nOCP(Open-Closed Principle, 개방-폐쇄 원칙)는 SOLID 원칙 중 하나로, “확장에는 열려 있고, 수정에는 닫혀 있어야 한다”는 객체지향 설계 원칙입니다.\n즉, 새로운 기능이 필요하면 기존 코드를 변경하지 않고 확장할 수 있도록 설계해야 한다는 의미입니다. 이를 통해 코드의 유지보수성과 확장성을 높일 수 있습니다.\nOCP는 상속이나 인터페이스를 사용하여 기능을 확장하는 방식으로 구현할 수 있습니다.\n배송 프로세스\n다시 이제 배송 얘기를 해보도록 하겠습니다.\n큰맥락의 글로벌 배송 프로세스는 다음과 같이 요약할 수 있습니다.\n\n해외 물류사는 국가별로 1개만 세팅되어 있습니다.\n주문 → 배송 정보 생성 후 주문한 국가에 따라 해외 물류사가 세팅 됩니다.\n위 다이어그램에서 추상화 & 리팩토링 대상은 해외 물류사의 개입이 있는 비즈니스 플로우 구간인 해외 배송 접수 요청 이후 구간입니다.\n기존 코드\n배송 시스템의 주요 클래스는 아래와 같이 구성되어 있었습니다.\nKafkaProductService: 물류 연동, 고객 이메일 발송, 모바일 푸시 등 비동기 후속 처리를 위한 Kafka 메시지 발행을 담당.\nCacheService: 해외 배송 접수 단계에서 데이터세팅을 위해 Redis를 사용하는 클래스\nDeliveryTrackerService: 해외 배송처리 로직을 포함한 비즈니스 로직을 담당하는 클래스\n기존 코드는 P사에만 특화된 로직만 있는 클래스만 존재했고, C사를 추가하려면 P사 로직에서 C사를 분기 처리하는 형태로 개발해야 했습니다.\n단순 분기처리하는 형태로 개발하면 다음과 같이 됩니다.\npublic class AdminShippingReleaseService {\n  \n  private final P_CacheService p_CacheService;\n  private final C_CacheService c_CacheService;\n  \n  private void sendToExecutionBatchSet(DeliveryTrackerType type, List<String> numbers) {\n  \n    LocalDateTime now = LocalDateTime.now(); \n    if (DeliveryTrackerType.P == type) {\n  \n      p_CacheService.removeFromBatchSet(getCurrentBatchTimeKey(now), numbers);\n      p_CacheService.addToBatchSet(getNextBatchTimeKey(now), numbers);\n  \n    } else if (DeliveryTrackerType.C == type) {\n  \n      c_CacheService.removeFromBatchSet(getCurrentBatchTimeKey(now), numbers);\n      c_CacheService.addToBatchSet(getNextBatchTimeKey(now), numbers);\n  \n    } \n    // 추가가 필요할 시 계속해서 if ~ else를 반복하게 됨\n          \n  }\n}\n이렇게 되면 기존 코드에 영향이 가게되고, 개발 비용자체도 증가하게되며 테스트 비용 역시 증가하게 됩니다.\n이 문제를 해결하기 위해 새로운 물류사가 추가될 때마다 기존 코드에 영향이 없고 개발 & 테스트 비용을 절감하기 위해 추상화 처리를 진행했습니다.\n리팩토링\n추상화\n우선 공통된 행위(=메서드) 패턴과 해외 물류사를 기준으로 구현체를 추가할 수 있도록 추상화를 진행했습니다.\n해외 물류사는 DeliveryTrackerType이라는 Enum으로 만들었고, DeliveryTrackerType는 최초 주문시 세팅됩니다.\n\nKafkaProductService와 CacheService는 각각 TrackerKafkaProduceService, TrackerCacheService라는 Interface로 추상화했습니다. 그리고 DeliveryTrackerService는 공통으로 사용될 메서드가 많아 abstract class로 리팩토링 하였습니다.\nFactory\n물류사별로 각각의 비즈니스에 맞춰 플로우를 진행하기 위해선 구현체를 찾아내는 과정이 필요한데, 이를 위해 DeliveryTrackerType에 맞는 구현체를 반환해주는 Factory를 추가 개발했습니다.\n@Component\npublic class TrackerCacheFactory {\n  private Map<DeliveryTrackerType, TrackerCacheService> trackingCacheServiceMap;\n  @Autowired\n  public TrackerCacheFactory(Set<TrackerCacheService> deliveryTrackerServiceSet) {\n    createStrategy(deliveryTrackerServiceSet);\n  }\n  public TrackerCacheService findStrategy(DeliveryTrackerType type) {\n    if (Objects.isNull(type) || DeliveryTrackerType.NONE.equals(type)) {\n      return null;\n  }\n    return trackingCacheServiceMap.get(type);\n  }\n  private void createStrategy(Set<TrackerCacheService> trackerCacheServiceSet) {\n    trackingCacheServiceMap = new HashMap<>();\n    trackerCacheServiceSet.stream()\n      .forEach(trackingCacheService -> trackingCacheServiceMap.put(\n      trackingCacheService.getTrackerType(),\n      trackingCacheService\n    ));\n  }\n}\nTrackerCacheFactory 클래스에는 해외 물류사 타입 DeliveryTrackerType를 key로, 각 물류사의 TrackerCacheSerive를 value로는 담고 있는 Map 형태의 trackingCacheServiceMap 멤버 변수가 있습니다.\n이후 실제 사용시에는 findStrategy() 메소드에 파라미터로 DeliveryTrackerType을 전달하고, 해당 타입에 맞는 구현체를 받아서 사용합니다.\n이렇게 추상화를 했을때 클라이언트 코드가 어떻게 바뀌는지 보겠습니다.\npublic class AdminShippingReleaseService {\n  private final TrackerCacheFactory trackerCacheFactory;\n  private void sendToExecutionBatchSet(DeliveryTrackerType type, List<String> numbers) {\n  \n    TrackerCacheService trackerCacheService = trackerCacheFactory.findStrategy(type);\n  \n    LocalDateTime now = LocalDateTime.now();\n    trackerCacheService.removeFromBatchSet(getCurrentBatchTimeKey(now), numbers);\n    trackerCacheService.addToBatchSet(getNextBatchTimeKey(now), numbers);\n    // after caching process\n  \n  }\n}\n이제 리팩토링 이후에는 C사가 아닌 다른 물류 업체(F사)를 추가하게 되더라도 기존 코드에 영향을 주지 않고 TrackerCacheService를 구현한 F사 구현체만 개발하면 됩니다.\n이렇게 추상화를 하게 되면 OCP를 만족하여 유연하고 확장성이 있는 코드를 작성할 수 있습니다.\n그러나, 상속은 강한 결합도가 생기는게 단점입니다.\n인터페이스나 부모 클래스에서 정의한 메소드는 반드시 구현체나 하위 클래스에서 오버라이드 해야합니다.\n그리고 클라이언트 코드 입장에서는 여러가지 인터페이스를 의존하게 되어 많은 책임을 지고 있습니다. 추가로 위의 예시를 보면 캐싱 처리에서 공통 처리 로직을 추가해야한다면 인터페이스를 추상 클래스로 변환하는 등 다른 수정 포인트가 생깁니다.\n조합(Composition)을 사용하면 위의 문제 외에도 많은 문제를 해결할수 있는 좀 더 유연한 구조를 가질 수 있습니다.\n조합(Composition)\n우선 간단하게 조합(Composition)에 대해 알아보겠습니다.\n조합이라는 용어는 생소할 수 있지만, 실은 많은 개발자가 이미 사용하고 있는 코딩 기법입니다. 조합은 하나의 클래스가 다른 클래스의 인스턴스를 필수로 멤버 변수로 포함하여 관계를 만드는 것을 의미합니다.\n흔히 “has-a 관계”로 설명되며, 상속의 “is-a 관계”와 대비되는 개념입니다.\n조합(Composition)을 사용한 클래스 다이어그램은 다음과 같습니다.\n\nComposition을 생성하는 Factory 코드\n@Component\npublic class TrackerCacheFactory {\n  private Map<DeliveryTrackerType, TrackerCacheService> trackerCacheServiceMap;\n  private final CommonCacheService commonCacheService; \n  public CacheProcessComposition getCacheProcessComposition(DeliveryTrackerType deliveryTrackerType) {\n    return new CacheProcessComposition(this.findStrategy(deliveryTrackerType), commonCacheService);\n  }\n  private TrackerCacheService findStrategy(DeliveryTrackerType deliveryTrackerType) {\n    return trackerCacheServiceMap.get(deliveryTrackerType);\n  }\n  private void createStrategy(Set<TrackerCacheService> trackerCacheServiceSet) {\n    trackingCacheServiceMap = new HashMap<>();\n    trackerCacheServiceSet.stream()\n      .forEach(trackingCacheService -> \n              trackingCacheServiceMap.put(\n                trackingCacheService.getTrackerType(),\n                trackingCacheService\n        ));\n  }\n}\nFactory는 TrackerCacheService의 여러가지 구현체를 Map으로 가지고 있습니다.\nComposition은 Type에 따른 적합한 해외물류사 TrackerCacheService 구현체를 파라미터로 넘겨줘 생성합니다. 그리고 캐싱 처리에 공통 로직이 들어있는 CommonCacheService도 파라미터로 넘겨줍니다.\npublic class CacheProcessComposition {\n  private final TrackerCacheService trackerCacheService;\n  private final CommonCacheService commonCacheService;\n  public CacheProcessComposition(TrackerCacheService trackerCacheService, CommonCacheService commonCacheService) {\n    this.trackerCacheService = trackerCacheService;\n    this.commonCacheService = commonCacheService;\n  }\n   public void trackerCachingProcess(List<String> numbers) {\n    LocalDateTime now = LocalDateTime.now();\n    commonCacheService.beforeCachingProcess();\n    trackerCacheService.removeFromBatchSet(getCurrentBatchTimeKey(now), numbers);\n    trackerCacheService.addToBatchSet(getNextBatchTimeKey(now), numbers);\n    commonCacheService.afterCachingProcess();\n  }\n}\nCacheProcessComposition에서는 모든 해외물류사 개별 로직을 하는 TrackerCacheService의 구현체와 캐싱 공통로직을 담당하는 CommonCacheService를 멤버 변수로 가지고 있고 모든 캐싱 처리는 여기서 담당합니다.\n클라이언트 클래스에서는 캐싱처리(=trackerCachingProcess) 메소드만 호출하면 로직이 완성됩니다.\npublic class AdminShippingReleaseService {\n  private final TrackerCacheFactory factory;\n  \n  public void release(DeliveryTrackerType type, List<String> numbers) {\n    CacheProcessComposition composition = factory.getCacheProcessComposition(type);\n    composition.trackerCachingProcess(numbers);\n    // after caching process\n  }\n}\n클라이언트 코드는 이전보다 훨씬 깔끔해졌습니다!!\nComposition을 통해 앞으로 추가적인 캐싱 로직이 필요하면 추상화 & 상속과 관계없이 CacheProcessComposition안에서 로직을 추가하면 되기 때문에 좀 더 유연해졌습니다. 공통 로직 역시 클라이언트 코드에서는 불필요하게 작성하거나 신경쓸 필요가 없어졌습니다.\n이렇게 합성(Composition)을 이용하면 좀 더 유연하게 코드를 구성할 수 있습니다.\n임팩트\nOCP원칙을 적용하며 리팩토링을 하는 과정에서 얻는 임팩트는 크게 2가지가 있습니다.\n\n개발 기간 & 비용 절감\n테스트 비용 절감\n\n개발 기간 & 비용 절감\n리팩토링의 효과는 특히 C사 이후 F사를 추가하면서 더욱 명확하게 드러났습니다.\nC사 개발 시에는 추상화 구조를 설계하고, 비즈니스 로직을 유연하게 적용할 수 있도록 코드를 리팩토링하는 데 약 한 달의 시간이 소요되었습니다.\n그러나 이러한 초기 추상화 작업이 완료된 덕분에, 이후 F사와 추가 계약을 체결한 후의 개발 작업에서는 비즈니스 로직을 추가하는 데만 집중할 수 있었습니다.\n그 결과, F사에 맞춘 개발 작업은 오직 1주 만에 완료할 수 있었으며, 이는 C사 작업 기간의 4분의 1에 불과한 시간입니다.\n초기 추상화 및 리팩토링 작업으로 인해 상당한 시간이 소요되었지만, 이는 이후의 개발 효율을 획기적으로 높이는 결과를 가져왔습니다.\n특히 새로운 고객사나 파트너사의 요구사항을 반영할 때, 코드 변경 없이 간단히 기능을 확장할 수 있어 개발 기간 단축과 비용 절감이라는 두 가지 주요 효과를 얻을 수 있었습니다.\n따라서, 초기 추상화 작업은 단기적인 개발 비용을 소폭 증가시킬 수는 있지만 전체 프로젝트의 관점에서 장기적인 비용 절감과 유지보수 효율성을 확보하는 데 큰 도움이 되었음을 확인할 수 있습니다.\n테스트 비용 절감\n\n테스트 케이스의 일관성 확보\n각 하위 클래스마다 별도의 케이스를 작성하지 않고 공통된 테스트 시나리오를 사용해 효율적으로 테스트할 수 있습니다.\n확장된 기능의 안전한 검증\nOCP를 적용한 시스템에서는 새로운 기능이 추가될 때 기존 코드가 안전하게 보호되므로, 기존 기능이 영향을 받지 않았다는 신뢰를 가지고 검증할 수 있습니다.\n따라서 기능 확장이 발생해도 새로운 기능에 대한 테스트만 추가하면 되므로 테스트 범위와 노력의 감소가 이루어집니다.\n테스트의 가독성과 유지보수성 향상\nOCP가 준수된 코드는 일반적으로 모듈화가 잘 되어 있어 각 모듈의 테스트 케이스가 명확하고 구조적으로 작성됩니다.\n각 모듈의 역할을 이해하기 쉽고, 모듈 별로 테스트를 독립적으로 작성하여 유지보수할 수 있으므로, 테스트 케이스의 가독성과 유지보수성이 크게 향상됩니다.\n버그 발생 가능성 감소\n추상화가 잘 되어 있어 모듈 간 의존성이 낮고 변경의 영향이 제한적이므로, 기능 추가나 변경 시 기존 코드에서 발생할 수 있는 버그 가능성이 낮아 집니다.\n\n마치며\n무신사는 지속적으로 성장하기 위해 굉장히 빠르고 다양한 방면에서 비즈니스가 진행되고 있습니다.\n만약 코드가 확장성과 유지보수성을 고려하지 않고 개발된다면, 비즈니스 속도가 느려지고 이는 곧 무신사의 성장 속도에도 부정적인 영향을 미칠 수밖에 없습니다. 따라서 개발자는 항상 확장, 유연함과 유지보수를 고려하면서 개발할 필요가 있습니다.\n이번 리팩토링을 통해 특정 해외 물류 업체에 로직이 강하게 종속되지 않고 확장할 수 있게되었고, 추후 비즈니스 변화에도 유연하게 대응할 수 있게 되었습니다.\n이 글이 비슷한 문제로 고민하는 분들께 도움이 되길 바라며, 추상화와 설계 원칙이 시스템 확장성 확보에 어떻게 기여할 수 있는지 고민해보는 계기가 되었으면 좋겠습니다.\n긴글을 읽어주셔서 감사합니다.\nMusinsa CAREER\n함께할 동료를 찾습니다.\n무신사에서는 지속적인 성장을 하기 위해 끊임없이 변화를 추구하고 있습니다. 이런 환경속에서 이번 포스팅과 같은 개발 방향을 고민하고 관심이 있는 분을 모시고 있습니다.\n저희 클레임 개발팀은 클레임과 배송 도메인을 주로 담당하고 있습니다.\n무신사에서 주문 이후의 비즈니스를 책임지고 고객에게 만족스러운 경험을 제공하기 위해 다양한 서비스를 개발 & 운영하고 있습니다. 변화하는 커머스 환경에서 기술적 도전을 즐기고, 지속 가능한 가치를 만들어갈 수 있는 여러분의 지원을 기다립니다.\n전국민이 사용하는 1위 패션 플랫폼 무신사에서 기술로 비즈니스를 성장시키는 경험을 함께하고 싶으시다면 아래 채용 페이지를 통해 지원해주세요!\n🚀 팀 무신사 채용 페이지 (무신사/29CM 전체 포지션 확인이 가능해요)\n🚀 팀 무신사 테크 소식을 받아보는 링크드인\n🚀 무신사 테크 블로그\n🚀 29CM 테크 블로그\n🚀 무신사 테크 유튜브 채널\n채용이 완료되면 공고가 닫힐 수 있으니 빠르게 지원해 주세요!\n\n추상화 & 리팩토링을 통한 해외 물류사 개발 비용 절감 was originally published in MUSINSA tech on Medium, where people are continuing the conversation by highlighting and responding to this story.",
    "dc:creator": "Min Heo",
    "guid": "https://medium.com/p/c2bcc8d9624d",
    "categories": [
      "refactoring",
      "abstraction",
      "backend",
      "java"
    ],
    "isoDate": "2024-12-05T07:08:17.000Z"
  },
  {
    "creator": "Junho Park",
    "title": "나야, 주문 - 주문시스템의 도전과 성장 이야기",
    "link": "https://medium.com/musinsa-tech/%EB%82%98%EC%95%BC-%EC%A3%BC%EB%AC%B8-%EC%A3%BC%EB%AC%B8%EC%8B%9C%EC%8A%A4%ED%85%9C%EC%9D%98-%EB%8F%84%EC%A0%84%EA%B3%BC-%EC%84%B1%EC%9E%A5-%EC%9D%B4%EC%95%BC%EA%B8%B0-744b4bece5b8?source=rss----f107b03c406e---4",
    "pubDate": "Thu, 21 Nov 2024 05:29:38 GMT",
    "content:encoded": "<p>안녕하세요. 무신사에서 주문을 담당하는 백엔드 엔지니어 박준호입니다.</p><p>이번 글에서는 무신사의 주문 시스템이 수많은 변화에 대응하며 대규모 트래픽과 이벤트 시즌에도 안정적인 서비스를 유지하기 위해 어떤 방식으로 변화해왔는지, 그 여정을 공유하고자 합니다.</p><h4>왜 개선이 필요한가?</h4><p>무신사와 같은 커머스 플랫폼에서 주문 시스템은 핵심적인 역할을 담당합니다. 주문 처리 속도와 안정성은 고객 경험에 직접적인 영향을 미치며, 주문 데이터를 신뢰할 수 있어야 모든 비즈니스가 원활하게 운영될 수 있기 때문입니다.</p><p>무신사는 블랙프라이데이(이하 무진장) 시즌마다 최고 매출과 주문 수를 경신하며 놀라운 성장세를 이어가고 있습니다. 이처럼 가파른 성장을 뒷받침하기 위해 주문 시스템도 지속적인 발전이 필요합니다.</p><p>하지만, 주문 도메인은 복잡한 비즈니스 로직을 포함하기 때문에 보수적으로 개발되는 면이 있습니다. 버그나 장애가 발생할 경우 장애 정도나 발생 시간과 관계없이 치명적인 결과로 이어질 수 있기 때문입니다.</p><p>예를 들어 무신사의 경우 2023년 겨울 무진장 기준으로 시간당 평균 주문액이 10억 원 이상으로, 15분의 장애를 가정했을 때 약 2억 5천만 원 이상의 손실이 발생한다는 계산을 할 수 있습니다.</p><p>따라서 주문 도메인을 다룰 때는 안정성과 신뢰성을 유지하면서도 변화와 개선을 추구하는 것이 매우 중요한 과제이고, 이를 위해 강한 책임감을 가지고 시스템을 관리하는 자세가 필요합니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*c2TFOXnuYGYRdO8RhUr2Gw.png\" /><figcaption>무신사 2.0 주문서</figcaption></figure><h3>여정의 시작, 모놀리식 아키텍처와 그 한계</h3><p>초창기 무신사 스토어는 모놀리식 아키텍처 (Monolithic Architecture) 로 구성되어 있었습니다. 하나의 데이터베이스를 공유하며 매거진을 제외한 모든 도메인이 하나의 리포지토리로 관리되어 있었습니다.</p><p>주문 외에도 결제, 재고, 상품, 회원, 쿠폰, 적립금 등 모든 주요 도메인이 하나의 어플리케이션 내에 통합되어 있었기 때문에 매우 복잡하고 유지보수하기 어려운 구조였습니다. 특히 주문 시스템은 스파게티 코드처럼 복잡하게 얽혀 있었고, 콜 스택이 지나치게 깊어 분석이 어려웠습니다. 이러한 복잡성은 새로운 기능 추가나 버그 수정 시 높은 리스크를 동반하게 만들었습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*O3f2cxhTr9-gqsecM5XLpg.png\" /><figcaption>모놀리식 아키텍처 무신사 스토어</figcaption></figure><p>이 시스템은 DB 의존도가 높은 구조로 모든 요청이 데이터베이스를 통해 처리되었기 때문에 성능 문제가 빈번히 발생했습니다. 특히 무진장과 같은 대규모 이벤트 시 요청이 데이터베이스로 몰리면서 사이트가 다운되는 일이 잦았는데, 서버 다운타임과 성능 저하는 매출에 직접적인 영향을 미치는 민감한 문제이므로 이벤트 시즌에는 안정적인 시스템 운영이 필수적이었습니다.</p><p>매년 서버를 증설하고 시스템을 튜닝했지만, 가파르게 증가하는 트래픽을 따라잡지 못해 한계에 도달하기 일쑤였습니다. 최적화되지 않은 코드들이 성능적 한계를 보이면서 근본적인 아키텍처 변화의 필요성이 대두되었습니다.</p><h3>변화의 시작, 리팩토링</h3><p>성능 개선과 유지보수의 용이성, 그리고 개발 생산성 향상은 매우 중요한 개선의 시작이였습니다.</p><p>이를 해결하기 위해 리팩토링을 시작했고, 첫 단계로 시스템의 기본 아키텍처를 개선했습니다. 2020년 초였지만 그 때까지 사용하고 있던 PHP 5.6 버전을 7.4 대로 업그레이드하고, 많은 개발자들이 무신사에 합류하여 작업하게 된 만큼, 일관적인 코드 작성이 가능하도록 PSR(PHP Standards Recommendation)을 채택하여 코드를 표준화하고 Composer를 도입하여 외부 라이브러리 의존성을 관리하기 시작했습니다.</p><p>초기 무신사 시스템은 CodeIgniter 프레임워크 기반으로, 전형적인 MVC 패턴을 따르고 있었습니다. 그래서 Controller 와 Model에 대부분의 비즈니스 로직이 포함되어 있었고, 이로 인해 Controller의 코드 양이 지나치게 많고, 로직이 서로 얽혀있어서 유지보수가 매우 어려운 상태였습니다. 특히, Controller → Model → Model 형태로 연결된 복잡한 흐름은 분석과 디버깅을 어렵게 했습니다. 또, 데이터 타입이 불명확한 배열(Array) 사용이 많아 타입 안정성을 보장할 수 없는 문제가 있었습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1000/1*bIoZBvdbTR54aTvYQCQTfw.jpeg\" /><figcaption>출처 : Netflix 흑백요리사</figcaption></figure><h4><strong>데이터베이스 성능 개선</strong></h4><p>리팩토링 초기 Writer DB의 과도한 부하로 인해 장애가 자주 발생했습니다. Reader DB는 트래픽이 몰릴 경우 스케일 아웃(Scale-Out)을 통해 일부 문제를 해결할 수 있었지만, Writer DB에 과도한 부하가 집중될 때 이를 제어하는 데 한계가 있었습니다. 특히 주문 시스템은 Writer DB 사용 빈도가 높았기 때문에 장애가 반복되곤 했습니다. 이를 해결하기 위해 코드 분석을 통해 불필요한 Writer DB 사용을 줄이고, Reader DB로 쿼리를 분배하여 성능을 최적화했습니다.</p><p>지속적인 모니터링을 통해 슬로우 쿼리(Slow Query)를 개선하고, 주요 칼럼에 적절한 인덱스(Index)를 추가하여 쿼리 성능을 최적화함으로써 Writer DB의 커넥션 점유 시간을 줄이고 DB 부하를 최소화 했습니다.</p><p>또한, 이전 시스템에서는 주문 시 재고 처리를 위해 DB Lock을 사용하는 구조를 채택하고 있었습니다. 이 방식은 여러 사용자가 동시에 주문할 경우, 각 트랜잭션이 동일 자원에 순차적으로 접근하면서 DB Lock을 기다려야 했기 때문에 과주문 방지에는 효과적이었지만, 처리 속도가 느려지고 DB 부하가 집중되어 전체 시스템 성능 저하를 유발하는 주요 원인이 되었습니다. 이를 해결하기 위해 Redis 기반의 분산 락(RedLock)을 도입하여 DB Lock 방식의 한계를 극복하고, 재고 처리와 같은 중요한 리소스에 대한 동시 접근 문제를 해결했습니다. 이를 통해 DB 부하를 분산시키고 시스템의 안정성을 확보할 수 있었습니다.</p><h4><strong>모던 PHP 와 객체지향</strong></h4><p>PHP 7 이상으로 업그레이드한 후, 네임스페이스(namespace) 기반의 모던 PHP 개발 방식을 도입하였고 DTO(Data Transfer Object) 패턴을 적용하여 배열(Array) 대신 명확한 데이터 타입을 사용하고, 객체지향적인 설계를 통해 코드의 일관성과 가독성을 향상시켰습니다.</p><p>처음에는 스파게티 코드를 분리하여 모듈화했으며, 그다음으로 패키지 구조를 변경하여 도메인 주도 설계(Domain Driven Design) 를 도입했습니다. 기존 CI 기반의 패키지 구조를 개선하여 컨트롤러는 그 역할에 충실하고, 나머지 로직은 별도의 네임스페이스로 분리하여 더 깔끔한 구조로 전환했습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*qpqBysFOZAIj6fKQ9HZVkA.png\" /></figure><h4><strong>의존성 주입</strong></h4><p>의존성 주입(Dependency Injection) 을 적극적으로 도입하여 개발의 유연성, 확장성, 재사용성을 크게 향상시켰습니다. 자동 주입을 통해 수동으로 객체를 주입할 필요가 없어졌고, 생산성도 크게 증가했습니다. 또한 양방향 의존성을 피하고 각 패키지 별로 책임 분리를 철저히 하여 높은 응집도와 낮은 결합도를 유지하려고 노력했습니다.</p><h4><strong>ORM 도입</strong></h4><p>기존에는 매번 SQL을 직접 작성해야 했으나, 주문 처리 로직에 ORM을 도입하면서 SQL 작성 없이도 데이터를 손쉽게 처리할 수 있는 환경을 구축했습니다. 필요한 기능을 빠르게 적용할 수 있도록 간단한 EntityManager 라이브러리를 직접 개발했습니다. 이 라이브러리는 Entity 객체와 테이블 매핑, 더티 체킹(Dirty Checking), 1차 캐시 등을 지원하여 데이터 조회, 수정, 등록, 삭제 작업을 간편하게 수행할 수 있도록 했습니다. 이를 통해 개발 생산성이 크게 향상되었습니다.</p><p>이 리팩토링 결과, 주문 시스템의 복잡도가 현저히 낮아졌고, 유지보수성과 생산성이 비약적으로 향상되었습니다. 또한, 버그와 장애 발생 빈도가 크게 줄어들었으며, 개발 효율성 역시 크게 개선되었습니다. 성능 최적화와 시스템 구조 개선 덕분에 주문 시스템은 더 높은 트래픽을 안정적으로 처리할 수 있는 기반을 갖추게 되었습니다.</p><h3>각자의 길을 가다, MSA</h3><p>많은 변화가 있던 시기였습니다. 인프라는 AWS로 이전되었으며, 다양한 도메인을 개발하는 팀들이 빌딩 되었습니다. 많은 리팩토링이 진행되었지만, 여전히 도메인 간의 복잡도와 결합도가 높아 확장성에 문제가 있었습니다. 특히, 배포 시 여러 도메인 서비스에 미치는 영향과 이로 인한 장애 전파는 모든 팀들의 고충이 되었습니다.</p><p>이 문제를 해결하고자, 본격적으로 MSA(Microservice Architecture) 전환을 추진했습니다. 초기에는 회원, 전시, 검색 등 얽혀있던 여러 도메인들이 순차적으로 분리되었고, 이후 결제, 적립금, 상품 등 다양한 도메인들이 추가적으로 분리되었습니다. 거대한 무신사 스토어 시스템은 점차 작은 독립적인 서비스들로 나뉘어 갔습니다. 이로써 각 도메인은 상호 의존성을 줄이고 독립적으로 운영될 수 있게 되었습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*hkboNHgTYf8_qbBgNIBc_g.png\" /><figcaption>무신사 스토어 MSA</figcaption></figure><h4><strong>주문 시스템의 변화와 도메인 분리</strong></h4><p>기존의 주문 시스템은 단일 시스템 내에서 모든 트랜잭션을 처리하고 있었기 때문에, 서비스가 분리되면서 API 기반의 통신 방식으로 변경이 필요했습니다. 이전에는 DB 트랜잭션 방식으로 데이터를 처리했으나, 도메인 분리 후에는 API 통신을 통해 각 서비스 간 데이터 일관성을 유지해야 했습니다. 이 과정에서 주문 시스템은 여러 비즈니스 로직과 데이터 처리 요구 사항을 충족하기 위해 API 통신의 안정성과 트랜잭션 일관성을 유지할 수 있는 설계가 필수적이었습니다.</p><h4><strong>장애 전파 방지 및 SAGA 패턴을 통한 분산 트랜잭션 관리</strong></h4><p>MSA 전환으로 많은 기능이 DB 기반에서 API 중심으로 변경되면서 새로운 문제가 나타났습니다. 특정 API 서비스에 장애가 발생하면 다른 서비스로 전파되어 시스템 전체에 영향을 줄 수 있었습니다. 장애가 발생한 서비스에 지속적으로 요청이 보내지는 것은 비효율적일 뿐 아니라 서비스 복구에도 도움이 되지 않아, 적절한 시점에서의 통제가 필요해졌습니다.</p><p>서킷 브레이커 패턴(Circuit Breaker Pattern)을 도입하면, 장애가 발생한 서비스에 대한 호출을 차단함으로써 다른 서비스의 정상적인 작동을 유지할 수 있습니다. 이를 통해 장애가 다른 서비스로 전파되는 것을 방지하고, 장애가 발생한 서비스를 빠르게 식별하여 복구 작업을 신속하게 시작할 수 있습니다. 또한, 장애 발생 시 문제를 빠르게 파악할 수 있어, 모니터링 시스템에도 유용하게 활용될 수 있습니다.</p><p>MSA 환경에서 중요한 이슈 중 하나는 바로 데이터 일관성이었습니다. 기존의 DB 트랜잭션을 대체하는 분산 트랜잭션 관리 방식으로, SAGA 패턴을 도입하여 문제를 해결했습니다. SAGA 패턴은 트랜잭션의 원자성을 보장하기 위해, 각 서비스가 독립적으로 트랜잭션을 처리하도록 하고, 만약 어떤 서비스에서 실패가 발생할 경우 보상 트랜잭션을 통해 이전 상태로 롤백하는 방식입니다.</p><p>가장 대표적인 예로, 재고 차감 API가 처리된 후 주문 실패 시, 재고 증가 API를 호출하여 복구하는 방식입니다. 이와 같은 방식으로, 보상 트랜잭션을 적용하여 여러 단계의 주문 처리에서 발생할 수 있는 오류를 사전에 방지하고, 데이터 일관성을 확보할 수 있었습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Ob0KBv5sDUJ2XtN9J-SZWQ.png\" /></figure><p>결국, MSA 전환을 통해 서비스 간 의존도를 줄이고, 확장성과 안정성을 강화할 수 있었으며, 장애 발생 시 서비스 복구 시간도 현저히 줄어드는 성과를 얻을 수 있었습니다.</p><h3>이벤트 기반 아키텍처로의 도약, 리소스 분산</h3><p>MSA 도입을 통해 많은 부분이 분리되었지만, 여전히 거대하였고, 다수의 의존성이 남아있었습니다. 주문 시스템도 많은 API 전환을 거쳤지만, 여전히 DB 처리에 대한 로직이 많이 남아있었고, 대량 트래픽 발생 시 DB 쓰기 부하로 인한 장애가 가끔씩 발생했습니다. 때문에 주문 시스템은 이벤트 기반 아키텍처로 리소스를 분산하고 성능 최적화를 진행했습니다.</p><h4><strong>주문 로직의 경량화 및 성능 최적화</strong></h4><p>주문 처리의 성능 최적화를 위해 주문 처리 트랜잭션 내 필수 수행 영역과 비필수 영역을 구분했습니다. 필수 영역을 제외한 비필수 영역은 비동기 처리로 전환하기 위해 AWS의 SNS, SQS, Lambda를 활용한 이벤트 기반 아키텍처를 설계했습니다. 이 과정에서 주문 이벤트에 대해 별도의 SNS 토픽을 생성하여 메시지를 발행하고, SQS 가 이를 구독하여 Lambda 가 트리거 후 비동기적으로 메시지를 처리하는 구조를 추가하여 개선했습니다. 이렇게 구현된 Pub/Sub 모델에서 Lambda는 Consumer 역할을 하며, Python 3.8로 개발된 서버리스 환경에서 운영됩니다.</p><p>SQS는 표준 대기열(Standard Queue)을 사용하여 고가용성과 높은 처리량을 확보했습니다. 메시지 중복 문제를 해결하기 위해, Redis의 SETNX를 사용하여 분산 락을 적용, 중복 메시지를 방지하도록 했습니다. SETNX는 주어진 키가 없을 때만 값을 설정하는 방식으로, 메시지가 중복 처리되지 않도록 보장합니다.</p><p>오류 처리와 장애 대응을 위해, 데드레터 큐(Dead Letter Queue, DLQ)를 연결하여 메시지 처리 중 발생할 수 있는 오류나 예외를 추적하고 실패한 메시지를 별도로 처리할 수 있도록 설계하였습니다. 이로 인해 메시지 처리의 신뢰성을 높이고, 실패한 메시지에 대해 후속 조치를 취할 수 있었습니다. 또한 CloudWatch를 활용하여 시스템의 로깅 및 모니터링을 진행하고 있습니다.</p><p>위에 언급했던 식별된 여러 기능을 주문 로직 내에서 제거하고 주문 이후 이벤트 소비를 통해 프로모션 이벤트 데이터 저장, 최근 결제수단 변경 데이터 저장, 최근 주문한 주소 정보 저장, 로그 저장 등 다양한 기능을 수행함으로써 경량화에 어느 정도 성공을 하였습니다. 이로 인해 대량의 트래픽을 받아도 이전보다 장애가 발생하는 상황이 많이 사라졌습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*grmND1kQmMNVKeQvmcze1g.png\" /><figcaption>AWS 기반 EDA 아키텍처</figcaption></figure><h4><strong>서버리스 아키텍처의 한계와 복잡성 증가</strong></h4><p>서버리스 아키텍처는 확장성과 비용 효율성 면에서 장점이 있지만, 사용하는 AWS 서비스가 많아지면서 관리 포인트가 증가했습니다. 특히 Lambda 기반으로 여러 서버리스 구성 요소를 운영하다 보니, AWS 장애 시 SNS와 SQS 메시지 지연 등 발생 가능한 문제를 진단하는데 있어서 시간이 걸렸습니다.</p><p>결과적으로 주문 처리 시스템의 안정성과 성능은 크게 개선되었지만, 이후 연계된 서버리스 기능들에 대해 모니터링과 트러블슈팅 복잡성이 증가하면서 새로운 고민이 생기게 되었습니다.</p><h3>Java와 Kafka로 미래를 설계하다</h3><p>무신사의 비즈니스가 계속해서 확장되면서 회사는 점점 더 유연하고 빠르게 변화하는 요구에 대응해야 했습니다. MSA 환경에서 다양한 도메인들이 주문 데이터를 필요로 하는 상황이 발생했고, 이에 따라 Kafka를 도입하여 이벤트 스트리밍 기반의 아키텍처로 변화를 이루었습니다. Kafka는 각 서비스가 필요한 데이터를 비동기적으로 소비하고 처리할 수 있게 해주었으며, 이는 시스템의 확장성과 유연성을 크게 향상시켰습니다. 예를 들어, 회원서비스는 등급 산정을 위해 주문 데이터를 활용하고, 재고 서비스는 정합성을 유지하는데 사용되고 있습니다. 또한, 무진장 전광판 서비스에서는 누적 판매 데이터와 누적 할인 금액을 실시간으로 집계하는데 사용되고 있습니다.</p><h4><strong>Kafka 도입을 통한 시스템 확장성 확보</strong></h4><p>주문 시스템은 다양한 Kafka 토픽을 통해 주문 관련 데이터를 실시간으로 제공하고 있습니다. 이를 통해 적립금, 재고, 회원, 후기 등 주문 데이터에 기반한 다양한 비즈니스 로직을 각 도메인에서 독립적으로 처리할 수 있는 환경이 구축되었습니다. 각 서비스가 필요한 데이터를 구독하여 처리할 수 있게 되면서, 시스템 간의 의존성이 줄어들고, 독립적인 서비스 운영이 가능해졌습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*_B5BRRWNd7lZFyzardVj9w.png\" /><figcaption>Kafka 도입</figcaption></figure><h4><strong>Java로의 시스템 전환</strong></h4><p>기존 PHP 기반의 주문 시스템을 Java로 전환하는 작업이 본격적으로 진행되었습니다. 현재 대부분의 API가 Java로 전환되었으며, 일부 주요 로직만 남아있는 상태입니다. Java는 높은 처리량을 처리할 수 있는 성능과 안정성을 제공하여, 대규모 트래픽을 효율적으로 처리하고 시스템의 유지보수성을 향상시키는 데 중요한 역할을 했습니다.</p><p>또한 위에 언급된 AWS 서비스 기반의 아키텍처의 고민을 해결하고자 Lambda 대신 Java Consumer로 개발을 하였고, 이를 통해 모니터링 및 운영 시스템을 일원화하여 효율적인 관리가 가능하도록 했습니다. 이러한 전환은 현재 진행형입니다.</p><p>주문개발팀 내에서는 다양한 주문 관련 시스템을 모듈화하고, 각 시스템을 독립적인 리포지토리로 분리하여 관리하고 있습니다. 예를 들어, 장바구니, 주문, 정산, 프로모션, 글로벌 주문, 오프라인 편집샵 등 각기 다른 서비스들이 별도의 시스템으로 분리되어 개발되고 있습니다. 이를 통해 각 시스템은 독립적으로 배포되고 운영될 수 있으며, 장애 전파를 막고 배포 주기를 단축시킬 수 있게 되었습니다. 또한, 각 팀은 자신의 서비스에 집중할 수 있게 되어, 업무 효율성과 서비스 안정성이 크게 향상되었습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*5_1hmDARppO_a66_k0xX4A.png\" /><figcaption>Java 시스템 전환</figcaption></figure><h3><strong>마치며</strong></h3><p>무신사의 주문 시스템은 위에서 언급한 기술 도입과 개선 외에도 지속적으로 발전하고 있습니다. 예를 들어, Java 17 버전을 도입한 이후 현재는 주문 전 시스템을 Java 21 버전으로 업그레이드하는 등 기술 스택의 최신화 작업이 꾸준히 이루어지고 있습니다</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*P-gvlmVxC2Zw9kE0a8eqPA.png\" /></figure><p>현재 무신사의 주문 시스템은 하루 약 20만 건의 주문을 안정적으로 처리하며, 분당 약 7,000건의 주문 트래픽을 소화할 수 있도록 개발 및 유지되고 있습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*qnojEgOG-iY85S6PtII5pw.png\" /><figcaption>2023년 겨울 마지막날 분당 주문 수</figcaption></figure><p>무신사의 주문 시스템은 트래픽 급증과 비즈니스 요구 변화에 대응하기 위해 끊임없이 아키텍처를 개선해 왔습니다. 초기 모놀리식 구조에서 마이크로서비스 아키텍처, 이벤트 기반 아키텍처, Kafka 기반 스트리밍, 그리고 Java 시스템으로 전환하며 성능과 안정성을 높였습니다. 앞으로도 무신사의 성장을 뒷받침할 핵심 시스템으로 자리잡기 위해 더욱 발전해 나갈 것입니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*jYBM6G0XClak3vnW-wkbvQ.png\" /><figcaption>출처 : Netflix 흑백요리사</figcaption></figure><h3>MUSINSA CAREER</h3><p>함께할 동료를 찾습니다.</p><p>주문개발팀은 무신사의 핵심 시스템인 주문 처리와 정산 시스템을 책임지고 있습니다. 고객이 무신사에서 문제없이 상품을 주문하고 결제할 수 있도록 시스템을 설계하고 운영하며, 무신사페이먼츠와의 결제 연동을 통해 안정적인 결제 프로세스를 지원합니다. 또한, 정확한 매출 데이터를 관리해 정산 업무를 처리하고, 재무팀이 필요한 기반 정보를 제공합니다. 국내 비즈니스뿐만 아니라, 글로벌 무신사의 주문과 정산 시스템도 함께 개발하고 운영하며, 다양한 국가의 고객들에게 최적의 주문 서비스를 제공합니다.</p><p>주문개발팀은 하루 약 20만 건의 주문을 처리하며, 분당 최대 7천건 이상의 주문을 처리할 수 있는 안정적인 시스템을 개발하고 유지하고 있습니다. 또한, 약 50억 건에 이르는 주문 관련 데이터를 다루고 있으며, 대용량 트래픽과 데이터를 안정적으로 처리하는 경험을 바탕으로, 급증하는 트래픽과 주문량에도 끊김 없는 서비스를 제공합니다.</p><p><em>🚀 </em><a href=\"https://corp.musinsa.com/ko/career/\"><em>팀 무신사 채용 페이지</em></a><em> (무신사/29CM 전체 포지션 확인이 가능해요)</em></p><p><em>🚀 </em><a href=\"https://kr.linkedin.com/company/musinsacom\"><em>팀 무신사 테크 소식을 받아보는 링크드인</em></a></p><p><em>🚀 </em><a href=\"https://medium.com/musinsa-tech\"><em>무신사 테크 블로그</em></a></p><p><em>🚀 </em><a href=\"https://medium.com/29cm\"><em>29CM 테크 블로그</em></a></p><p><em>🚀 </em><a href=\"https://www.youtube.com/@MUSINSATECH\"><em>무신사 테크 유튜브 채널</em></a></p><p><em>채용이 완료되면 공고가 닫힐 수 있으니 빠르게 지원해 주세요!</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=744b4bece5b8\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/musinsa-tech/%EB%82%98%EC%95%BC-%EC%A3%BC%EB%AC%B8-%EC%A3%BC%EB%AC%B8%EC%8B%9C%EC%8A%A4%ED%85%9C%EC%9D%98-%EB%8F%84%EC%A0%84%EA%B3%BC-%EC%84%B1%EC%9E%A5-%EC%9D%B4%EC%95%BC%EA%B8%B0-744b4bece5b8\">나야, 주문 - 주문시스템의 도전과 성장 이야기</a> was originally published in <a href=\"https://medium.com/musinsa-tech\">MUSINSA tech</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "content:encodedSnippet": "안녕하세요. 무신사에서 주문을 담당하는 백엔드 엔지니어 박준호입니다.\n이번 글에서는 무신사의 주문 시스템이 수많은 변화에 대응하며 대규모 트래픽과 이벤트 시즌에도 안정적인 서비스를 유지하기 위해 어떤 방식으로 변화해왔는지, 그 여정을 공유하고자 합니다.\n왜 개선이 필요한가?\n무신사와 같은 커머스 플랫폼에서 주문 시스템은 핵심적인 역할을 담당합니다. 주문 처리 속도와 안정성은 고객 경험에 직접적인 영향을 미치며, 주문 데이터를 신뢰할 수 있어야 모든 비즈니스가 원활하게 운영될 수 있기 때문입니다.\n무신사는 블랙프라이데이(이하 무진장) 시즌마다 최고 매출과 주문 수를 경신하며 놀라운 성장세를 이어가고 있습니다. 이처럼 가파른 성장을 뒷받침하기 위해 주문 시스템도 지속적인 발전이 필요합니다.\n하지만, 주문 도메인은 복잡한 비즈니스 로직을 포함하기 때문에 보수적으로 개발되는 면이 있습니다. 버그나 장애가 발생할 경우 장애 정도나 발생 시간과 관계없이 치명적인 결과로 이어질 수 있기 때문입니다.\n예를 들어 무신사의 경우 2023년 겨울 무진장 기준으로 시간당 평균 주문액이 10억 원 이상으로, 15분의 장애를 가정했을 때 약 2억 5천만 원 이상의 손실이 발생한다는 계산을 할 수 있습니다.\n따라서 주문 도메인을 다룰 때는 안정성과 신뢰성을 유지하면서도 변화와 개선을 추구하는 것이 매우 중요한 과제이고, 이를 위해 강한 책임감을 가지고 시스템을 관리하는 자세가 필요합니다.\n무신사 2.0 주문서\n여정의 시작, 모놀리식 아키텍처와 그 한계\n초창기 무신사 스토어는 모놀리식 아키텍처 (Monolithic Architecture) 로 구성되어 있었습니다. 하나의 데이터베이스를 공유하며 매거진을 제외한 모든 도메인이 하나의 리포지토리로 관리되어 있었습니다.\n주문 외에도 결제, 재고, 상품, 회원, 쿠폰, 적립금 등 모든 주요 도메인이 하나의 어플리케이션 내에 통합되어 있었기 때문에 매우 복잡하고 유지보수하기 어려운 구조였습니다. 특히 주문 시스템은 스파게티 코드처럼 복잡하게 얽혀 있었고, 콜 스택이 지나치게 깊어 분석이 어려웠습니다. 이러한 복잡성은 새로운 기능 추가나 버그 수정 시 높은 리스크를 동반하게 만들었습니다.\n모놀리식 아키텍처 무신사 스토어\n이 시스템은 DB 의존도가 높은 구조로 모든 요청이 데이터베이스를 통해 처리되었기 때문에 성능 문제가 빈번히 발생했습니다. 특히 무진장과 같은 대규모 이벤트 시 요청이 데이터베이스로 몰리면서 사이트가 다운되는 일이 잦았는데, 서버 다운타임과 성능 저하는 매출에 직접적인 영향을 미치는 민감한 문제이므로 이벤트 시즌에는 안정적인 시스템 운영이 필수적이었습니다.\n매년 서버를 증설하고 시스템을 튜닝했지만, 가파르게 증가하는 트래픽을 따라잡지 못해 한계에 도달하기 일쑤였습니다. 최적화되지 않은 코드들이 성능적 한계를 보이면서 근본적인 아키텍처 변화의 필요성이 대두되었습니다.\n변화의 시작, 리팩토링\n성능 개선과 유지보수의 용이성, 그리고 개발 생산성 향상은 매우 중요한 개선의 시작이였습니다.\n이를 해결하기 위해 리팩토링을 시작했고, 첫 단계로 시스템의 기본 아키텍처를 개선했습니다. 2020년 초였지만 그 때까지 사용하고 있던 PHP 5.6 버전을 7.4 대로 업그레이드하고, 많은 개발자들이 무신사에 합류하여 작업하게 된 만큼, 일관적인 코드 작성이 가능하도록 PSR(PHP Standards Recommendation)을 채택하여 코드를 표준화하고 Composer를 도입하여 외부 라이브러리 의존성을 관리하기 시작했습니다.\n초기 무신사 시스템은 CodeIgniter 프레임워크 기반으로, 전형적인 MVC 패턴을 따르고 있었습니다. 그래서 Controller 와 Model에 대부분의 비즈니스 로직이 포함되어 있었고, 이로 인해 Controller의 코드 양이 지나치게 많고, 로직이 서로 얽혀있어서 유지보수가 매우 어려운 상태였습니다. 특히, Controller → Model → Model 형태로 연결된 복잡한 흐름은 분석과 디버깅을 어렵게 했습니다. 또, 데이터 타입이 불명확한 배열(Array) 사용이 많아 타입 안정성을 보장할 수 없는 문제가 있었습니다.\n출처 : Netflix 흑백요리사\n데이터베이스 성능 개선\n리팩토링 초기 Writer DB의 과도한 부하로 인해 장애가 자주 발생했습니다. Reader DB는 트래픽이 몰릴 경우 스케일 아웃(Scale-Out)을 통해 일부 문제를 해결할 수 있었지만, Writer DB에 과도한 부하가 집중될 때 이를 제어하는 데 한계가 있었습니다. 특히 주문 시스템은 Writer DB 사용 빈도가 높았기 때문에 장애가 반복되곤 했습니다. 이를 해결하기 위해 코드 분석을 통해 불필요한 Writer DB 사용을 줄이고, Reader DB로 쿼리를 분배하여 성능을 최적화했습니다.\n지속적인 모니터링을 통해 슬로우 쿼리(Slow Query)를 개선하고, 주요 칼럼에 적절한 인덱스(Index)를 추가하여 쿼리 성능을 최적화함으로써 Writer DB의 커넥션 점유 시간을 줄이고 DB 부하를 최소화 했습니다.\n또한, 이전 시스템에서는 주문 시 재고 처리를 위해 DB Lock을 사용하는 구조를 채택하고 있었습니다. 이 방식은 여러 사용자가 동시에 주문할 경우, 각 트랜잭션이 동일 자원에 순차적으로 접근하면서 DB Lock을 기다려야 했기 때문에 과주문 방지에는 효과적이었지만, 처리 속도가 느려지고 DB 부하가 집중되어 전체 시스템 성능 저하를 유발하는 주요 원인이 되었습니다. 이를 해결하기 위해 Redis 기반의 분산 락(RedLock)을 도입하여 DB Lock 방식의 한계를 극복하고, 재고 처리와 같은 중요한 리소스에 대한 동시 접근 문제를 해결했습니다. 이를 통해 DB 부하를 분산시키고 시스템의 안정성을 확보할 수 있었습니다.\n모던 PHP 와 객체지향\nPHP 7 이상으로 업그레이드한 후, 네임스페이스(namespace) 기반의 모던 PHP 개발 방식을 도입하였고 DTO(Data Transfer Object) 패턴을 적용하여 배열(Array) 대신 명확한 데이터 타입을 사용하고, 객체지향적인 설계를 통해 코드의 일관성과 가독성을 향상시켰습니다.\n처음에는 스파게티 코드를 분리하여 모듈화했으며, 그다음으로 패키지 구조를 변경하여 도메인 주도 설계(Domain Driven Design) 를 도입했습니다. 기존 CI 기반의 패키지 구조를 개선하여 컨트롤러는 그 역할에 충실하고, 나머지 로직은 별도의 네임스페이스로 분리하여 더 깔끔한 구조로 전환했습니다.\n\n의존성 주입\n의존성 주입(Dependency Injection) 을 적극적으로 도입하여 개발의 유연성, 확장성, 재사용성을 크게 향상시켰습니다. 자동 주입을 통해 수동으로 객체를 주입할 필요가 없어졌고, 생산성도 크게 증가했습니다. 또한 양방향 의존성을 피하고 각 패키지 별로 책임 분리를 철저히 하여 높은 응집도와 낮은 결합도를 유지하려고 노력했습니다.\nORM 도입\n기존에는 매번 SQL을 직접 작성해야 했으나, 주문 처리 로직에 ORM을 도입하면서 SQL 작성 없이도 데이터를 손쉽게 처리할 수 있는 환경을 구축했습니다. 필요한 기능을 빠르게 적용할 수 있도록 간단한 EntityManager 라이브러리를 직접 개발했습니다. 이 라이브러리는 Entity 객체와 테이블 매핑, 더티 체킹(Dirty Checking), 1차 캐시 등을 지원하여 데이터 조회, 수정, 등록, 삭제 작업을 간편하게 수행할 수 있도록 했습니다. 이를 통해 개발 생산성이 크게 향상되었습니다.\n이 리팩토링 결과, 주문 시스템의 복잡도가 현저히 낮아졌고, 유지보수성과 생산성이 비약적으로 향상되었습니다. 또한, 버그와 장애 발생 빈도가 크게 줄어들었으며, 개발 효율성 역시 크게 개선되었습니다. 성능 최적화와 시스템 구조 개선 덕분에 주문 시스템은 더 높은 트래픽을 안정적으로 처리할 수 있는 기반을 갖추게 되었습니다.\n각자의 길을 가다, MSA\n많은 변화가 있던 시기였습니다. 인프라는 AWS로 이전되었으며, 다양한 도메인을 개발하는 팀들이 빌딩 되었습니다. 많은 리팩토링이 진행되었지만, 여전히 도메인 간의 복잡도와 결합도가 높아 확장성에 문제가 있었습니다. 특히, 배포 시 여러 도메인 서비스에 미치는 영향과 이로 인한 장애 전파는 모든 팀들의 고충이 되었습니다.\n이 문제를 해결하고자, 본격적으로 MSA(Microservice Architecture) 전환을 추진했습니다. 초기에는 회원, 전시, 검색 등 얽혀있던 여러 도메인들이 순차적으로 분리되었고, 이후 결제, 적립금, 상품 등 다양한 도메인들이 추가적으로 분리되었습니다. 거대한 무신사 스토어 시스템은 점차 작은 독립적인 서비스들로 나뉘어 갔습니다. 이로써 각 도메인은 상호 의존성을 줄이고 독립적으로 운영될 수 있게 되었습니다.\n무신사 스토어 MSA\n주문 시스템의 변화와 도메인 분리\n기존의 주문 시스템은 단일 시스템 내에서 모든 트랜잭션을 처리하고 있었기 때문에, 서비스가 분리되면서 API 기반의 통신 방식으로 변경이 필요했습니다. 이전에는 DB 트랜잭션 방식으로 데이터를 처리했으나, 도메인 분리 후에는 API 통신을 통해 각 서비스 간 데이터 일관성을 유지해야 했습니다. 이 과정에서 주문 시스템은 여러 비즈니스 로직과 데이터 처리 요구 사항을 충족하기 위해 API 통신의 안정성과 트랜잭션 일관성을 유지할 수 있는 설계가 필수적이었습니다.\n장애 전파 방지 및 SAGA 패턴을 통한 분산 트랜잭션 관리\nMSA 전환으로 많은 기능이 DB 기반에서 API 중심으로 변경되면서 새로운 문제가 나타났습니다. 특정 API 서비스에 장애가 발생하면 다른 서비스로 전파되어 시스템 전체에 영향을 줄 수 있었습니다. 장애가 발생한 서비스에 지속적으로 요청이 보내지는 것은 비효율적일 뿐 아니라 서비스 복구에도 도움이 되지 않아, 적절한 시점에서의 통제가 필요해졌습니다.\n서킷 브레이커 패턴(Circuit Breaker Pattern)을 도입하면, 장애가 발생한 서비스에 대한 호출을 차단함으로써 다른 서비스의 정상적인 작동을 유지할 수 있습니다. 이를 통해 장애가 다른 서비스로 전파되는 것을 방지하고, 장애가 발생한 서비스를 빠르게 식별하여 복구 작업을 신속하게 시작할 수 있습니다. 또한, 장애 발생 시 문제를 빠르게 파악할 수 있어, 모니터링 시스템에도 유용하게 활용될 수 있습니다.\nMSA 환경에서 중요한 이슈 중 하나는 바로 데이터 일관성이었습니다. 기존의 DB 트랜잭션을 대체하는 분산 트랜잭션 관리 방식으로, SAGA 패턴을 도입하여 문제를 해결했습니다. SAGA 패턴은 트랜잭션의 원자성을 보장하기 위해, 각 서비스가 독립적으로 트랜잭션을 처리하도록 하고, 만약 어떤 서비스에서 실패가 발생할 경우 보상 트랜잭션을 통해 이전 상태로 롤백하는 방식입니다.\n가장 대표적인 예로, 재고 차감 API가 처리된 후 주문 실패 시, 재고 증가 API를 호출하여 복구하는 방식입니다. 이와 같은 방식으로, 보상 트랜잭션을 적용하여 여러 단계의 주문 처리에서 발생할 수 있는 오류를 사전에 방지하고, 데이터 일관성을 확보할 수 있었습니다.\n\n결국, MSA 전환을 통해 서비스 간 의존도를 줄이고, 확장성과 안정성을 강화할 수 있었으며, 장애 발생 시 서비스 복구 시간도 현저히 줄어드는 성과를 얻을 수 있었습니다.\n이벤트 기반 아키텍처로의 도약, 리소스 분산\nMSA 도입을 통해 많은 부분이 분리되었지만, 여전히 거대하였고, 다수의 의존성이 남아있었습니다. 주문 시스템도 많은 API 전환을 거쳤지만, 여전히 DB 처리에 대한 로직이 많이 남아있었고, 대량 트래픽 발생 시 DB 쓰기 부하로 인한 장애가 가끔씩 발생했습니다. 때문에 주문 시스템은 이벤트 기반 아키텍처로 리소스를 분산하고 성능 최적화를 진행했습니다.\n주문 로직의 경량화 및 성능 최적화\n주문 처리의 성능 최적화를 위해 주문 처리 트랜잭션 내 필수 수행 영역과 비필수 영역을 구분했습니다. 필수 영역을 제외한 비필수 영역은 비동기 처리로 전환하기 위해 AWS의 SNS, SQS, Lambda를 활용한 이벤트 기반 아키텍처를 설계했습니다. 이 과정에서 주문 이벤트에 대해 별도의 SNS 토픽을 생성하여 메시지를 발행하고, SQS 가 이를 구독하여 Lambda 가 트리거 후 비동기적으로 메시지를 처리하는 구조를 추가하여 개선했습니다. 이렇게 구현된 Pub/Sub 모델에서 Lambda는 Consumer 역할을 하며, Python 3.8로 개발된 서버리스 환경에서 운영됩니다.\nSQS는 표준 대기열(Standard Queue)을 사용하여 고가용성과 높은 처리량을 확보했습니다. 메시지 중복 문제를 해결하기 위해, Redis의 SETNX를 사용하여 분산 락을 적용, 중복 메시지를 방지하도록 했습니다. SETNX는 주어진 키가 없을 때만 값을 설정하는 방식으로, 메시지가 중복 처리되지 않도록 보장합니다.\n오류 처리와 장애 대응을 위해, 데드레터 큐(Dead Letter Queue, DLQ)를 연결하여 메시지 처리 중 발생할 수 있는 오류나 예외를 추적하고 실패한 메시지를 별도로 처리할 수 있도록 설계하였습니다. 이로 인해 메시지 처리의 신뢰성을 높이고, 실패한 메시지에 대해 후속 조치를 취할 수 있었습니다. 또한 CloudWatch를 활용하여 시스템의 로깅 및 모니터링을 진행하고 있습니다.\n위에 언급했던 식별된 여러 기능을 주문 로직 내에서 제거하고 주문 이후 이벤트 소비를 통해 프로모션 이벤트 데이터 저장, 최근 결제수단 변경 데이터 저장, 최근 주문한 주소 정보 저장, 로그 저장 등 다양한 기능을 수행함으로써 경량화에 어느 정도 성공을 하였습니다. 이로 인해 대량의 트래픽을 받아도 이전보다 장애가 발생하는 상황이 많이 사라졌습니다.\nAWS 기반 EDA 아키텍처\n서버리스 아키텍처의 한계와 복잡성 증가\n서버리스 아키텍처는 확장성과 비용 효율성 면에서 장점이 있지만, 사용하는 AWS 서비스가 많아지면서 관리 포인트가 증가했습니다. 특히 Lambda 기반으로 여러 서버리스 구성 요소를 운영하다 보니, AWS 장애 시 SNS와 SQS 메시지 지연 등 발생 가능한 문제를 진단하는데 있어서 시간이 걸렸습니다.\n결과적으로 주문 처리 시스템의 안정성과 성능은 크게 개선되었지만, 이후 연계된 서버리스 기능들에 대해 모니터링과 트러블슈팅 복잡성이 증가하면서 새로운 고민이 생기게 되었습니다.\nJava와 Kafka로 미래를 설계하다\n무신사의 비즈니스가 계속해서 확장되면서 회사는 점점 더 유연하고 빠르게 변화하는 요구에 대응해야 했습니다. MSA 환경에서 다양한 도메인들이 주문 데이터를 필요로 하는 상황이 발생했고, 이에 따라 Kafka를 도입하여 이벤트 스트리밍 기반의 아키텍처로 변화를 이루었습니다. Kafka는 각 서비스가 필요한 데이터를 비동기적으로 소비하고 처리할 수 있게 해주었으며, 이는 시스템의 확장성과 유연성을 크게 향상시켰습니다. 예를 들어, 회원서비스는 등급 산정을 위해 주문 데이터를 활용하고, 재고 서비스는 정합성을 유지하는데 사용되고 있습니다. 또한, 무진장 전광판 서비스에서는 누적 판매 데이터와 누적 할인 금액을 실시간으로 집계하는데 사용되고 있습니다.\nKafka 도입을 통한 시스템 확장성 확보\n주문 시스템은 다양한 Kafka 토픽을 통해 주문 관련 데이터를 실시간으로 제공하고 있습니다. 이를 통해 적립금, 재고, 회원, 후기 등 주문 데이터에 기반한 다양한 비즈니스 로직을 각 도메인에서 독립적으로 처리할 수 있는 환경이 구축되었습니다. 각 서비스가 필요한 데이터를 구독하여 처리할 수 있게 되면서, 시스템 간의 의존성이 줄어들고, 독립적인 서비스 운영이 가능해졌습니다.\nKafka 도입\nJava로의 시스템 전환\n기존 PHP 기반의 주문 시스템을 Java로 전환하는 작업이 본격적으로 진행되었습니다. 현재 대부분의 API가 Java로 전환되었으며, 일부 주요 로직만 남아있는 상태입니다. Java는 높은 처리량을 처리할 수 있는 성능과 안정성을 제공하여, 대규모 트래픽을 효율적으로 처리하고 시스템의 유지보수성을 향상시키는 데 중요한 역할을 했습니다.\n또한 위에 언급된 AWS 서비스 기반의 아키텍처의 고민을 해결하고자 Lambda 대신 Java Consumer로 개발을 하였고, 이를 통해 모니터링 및 운영 시스템을 일원화하여 효율적인 관리가 가능하도록 했습니다. 이러한 전환은 현재 진행형입니다.\n주문개발팀 내에서는 다양한 주문 관련 시스템을 모듈화하고, 각 시스템을 독립적인 리포지토리로 분리하여 관리하고 있습니다. 예를 들어, 장바구니, 주문, 정산, 프로모션, 글로벌 주문, 오프라인 편집샵 등 각기 다른 서비스들이 별도의 시스템으로 분리되어 개발되고 있습니다. 이를 통해 각 시스템은 독립적으로 배포되고 운영될 수 있으며, 장애 전파를 막고 배포 주기를 단축시킬 수 있게 되었습니다. 또한, 각 팀은 자신의 서비스에 집중할 수 있게 되어, 업무 효율성과 서비스 안정성이 크게 향상되었습니다.\nJava 시스템 전환\n마치며\n무신사의 주문 시스템은 위에서 언급한 기술 도입과 개선 외에도 지속적으로 발전하고 있습니다. 예를 들어, Java 17 버전을 도입한 이후 현재는 주문 전 시스템을 Java 21 버전으로 업그레이드하는 등 기술 스택의 최신화 작업이 꾸준히 이루어지고 있습니다\n\n현재 무신사의 주문 시스템은 하루 약 20만 건의 주문을 안정적으로 처리하며, 분당 약 7,000건의 주문 트래픽을 소화할 수 있도록 개발 및 유지되고 있습니다.\n2023년 겨울 마지막날 분당 주문 수\n무신사의 주문 시스템은 트래픽 급증과 비즈니스 요구 변화에 대응하기 위해 끊임없이 아키텍처를 개선해 왔습니다. 초기 모놀리식 구조에서 마이크로서비스 아키텍처, 이벤트 기반 아키텍처, Kafka 기반 스트리밍, 그리고 Java 시스템으로 전환하며 성능과 안정성을 높였습니다. 앞으로도 무신사의 성장을 뒷받침할 핵심 시스템으로 자리잡기 위해 더욱 발전해 나갈 것입니다.\n출처 : Netflix 흑백요리사\nMUSINSA CAREER\n함께할 동료를 찾습니다.\n주문개발팀은 무신사의 핵심 시스템인 주문 처리와 정산 시스템을 책임지고 있습니다. 고객이 무신사에서 문제없이 상품을 주문하고 결제할 수 있도록 시스템을 설계하고 운영하며, 무신사페이먼츠와의 결제 연동을 통해 안정적인 결제 프로세스를 지원합니다. 또한, 정확한 매출 데이터를 관리해 정산 업무를 처리하고, 재무팀이 필요한 기반 정보를 제공합니다. 국내 비즈니스뿐만 아니라, 글로벌 무신사의 주문과 정산 시스템도 함께 개발하고 운영하며, 다양한 국가의 고객들에게 최적의 주문 서비스를 제공합니다.\n주문개발팀은 하루 약 20만 건의 주문을 처리하며, 분당 최대 7천건 이상의 주문을 처리할 수 있는 안정적인 시스템을 개발하고 유지하고 있습니다. 또한, 약 50억 건에 이르는 주문 관련 데이터를 다루고 있으며, 대용량 트래픽과 데이터를 안정적으로 처리하는 경험을 바탕으로, 급증하는 트래픽과 주문량에도 끊김 없는 서비스를 제공합니다.\n🚀 팀 무신사 채용 페이지 (무신사/29CM 전체 포지션 확인이 가능해요)\n🚀 팀 무신사 테크 소식을 받아보는 링크드인\n🚀 무신사 테크 블로그\n🚀 29CM 테크 블로그\n🚀 무신사 테크 유튜브 채널\n채용이 완료되면 공고가 닫힐 수 있으니 빠르게 지원해 주세요!\n\n나야, 주문 - 주문시스템의 도전과 성장 이야기 was originally published in MUSINSA tech on Medium, where people are continuing the conversation by highlighting and responding to this story.",
    "dc:creator": "Junho Park",
    "guid": "https://medium.com/p/744b4bece5b8",
    "categories": [
      "tech",
      "backend",
      "musinsa",
      "architecture",
      "refactoring"
    ],
    "isoDate": "2024-11-21T05:29:38.000Z"
  },
  {
    "creator": "Hangjun Cho",
    "title": "무신사 성장과 함께 거대해져온 600줄짜리 쿠폰 쿼리와의 아름다운 이별",
    "link": "https://medium.com/musinsa-tech/%EB%AC%B4%EC%8B%A0%EC%82%AC-%EC%84%B1%EC%9E%A5%EA%B3%BC-%ED%95%A8%EA%BB%98-%EA%B1%B0%EB%8C%80%ED%95%B4%EC%A0%B8%EC%98%A8-600%EC%A4%84%EC%A7%9C%EB%A6%AC-%EC%BF%A0%ED%8F%B0-%EC%BF%BC%EB%A6%AC%EC%99%80%EC%9D%98-%EC%95%84%EB%A6%84%EB%8B%A4%EC%9A%B4-%EC%9D%B4%EB%B3%84-e689d7d932b5?source=rss----f107b03c406e---4",
    "pubDate": "Wed, 13 Nov 2024 10:34:43 GMT",
    "content:encoded": "<p>안녕하세요, 무신사 캠페인개발팀에서 무신사의 최대 이벤트인 무진장과 여러 이벤트들을 담당하고 있는 조항준입니다. 이번 글에서는 오래된 레거시 시스템 중 하나였던 쿠폰 시스템을 개선하는 과정에서의 실패와 성공 사례를 공유하고자 합니다.</p><p><strong>왜 개선이 필요한가?</strong></p><p>과거에는 사용자가 쿠폰을 사용할 때만 자신이 보유한 쿠폰을 확인하는 구조였습니다. 그러나 Musinsa 2.0이 도입되면서, Musinsa에서 가장 트래픽이 높은 페이지인 상품 상세 페이지에서 <strong>최적의 쿠폰을 실시간으로 제공</strong>해야 하는 요구가 생겼습니다.</p><p>Musinsa의 빠른 성장과 함께 트래픽이 폭발적으로 증가했고, 단순했던 쿠폰 쿼리가 점점 복잡해져 결국 600줄에 달하는 거대한 코드로 변모했습니다. 이러한 거대한 쿼리는 더 이상 높은 트래픽을 감당하기 어려웠고, 계속해서 유지하기 위해 막대한 인프라 비용을 지불해야 한다는 문제가 있었습니다.</p><p>지금까지 거대해지면서도 버티고 있었던 개인적인 생각은<br>막대한 인프라 비용과 오래 함께한 팀원들의 노력으로 유지 되고 있었다고 생각합니다.</p><p><strong>어떻게 개선하기로 했나?</strong></p><p><strong>1. 쿼리 분할</strong></p><ul><li>단일책임원칙은 객체지향 프로그래밍의 SOLID 원칙 중 하나로, 각 클래스나 함수가 하나의 책임만 가져야 한다는 원칙입니다. 각각의 기능이 고유한 책임을 가질 때 코드의 재사용성과 유지보수성이 높아지는 것이 핵심입니다.</li><li>긴 쿼리를 하나의 책임을 가진 여러 개의 짧은 쿼리로 분리하는 작업을 단일책임원칙으로 비유할 수 있습니다. 즉, 각 쿼리는 특정한 데이터나 기능에만 집중하도록 설계하여 효율적으로 필요한 데이터만을 반환할 수 있게 만드는 것입니다.</li><li>위와 같은 이유로 600줄짜리 거대한 쿼리를 여러개의 짧은 쿼리로 쪼개어 자신만의 데이터를 가져오고 그 데이터를 조합하여 <strong>Application에서 계산</strong>하도록 변경했습니다. 이를 통해 쿼리의 복잡성을 줄이고, 처리 속도를 크게 개선할 수 있었습니다.</li></ul><p><strong>2. 캐싱 처리</strong></p><ul><li><strong>캐싱을 통한 조회 최적화 모델 구성:</strong> 캐싱을 적극적으로 활용해 변화가 많치않고 자주 요청되는 데이터는 캐시된 결과를 반환하도록 최적화, 데이터베이스에는 변화가 적은 데이터만 접근하도록 하여, 데이터베이스 부하를 줄이고 응답 속도를 높히는데 집중을 했습니다.</li></ul><p><strong>3. Application 자원 활용</strong></p><ul><li><strong>데이터베이스의 책임</strong>은 주로 데이터를 안전하게 저장하고, 빠르고 효율적으로 조회하는 데 집중</li><li><strong>애플리케이션의 책임</strong>은 비즈니스 로직과 관련된 연산과 데이터 조작을 담당하며, 데이터베이스로부터 데이터를 받아 필요한 연산을 수행하는 역할</li><li>각 자원의 특성을 생각해서 데이터베이스는 안전하고 빠르게 데이터를 주는데 집중하고 Application은 연산을 집중적으로 담당해서 처리 하였습니다.</li><li>Application 자원의 사용은 비용이 더 저렴하고, 운영 측면에서도 더 효율적이라는 장점이 있었습니다.</li></ul><p><strong>실패 사례</strong></p><p><strong>첫 번째 시도: S3</strong></p><ul><li>모든 쿠폰 데이터를 배치로 생성해 S3에 업로드</li><li>문제점: 다운로드 지연과 네트워크 비용 증가 <br>(1480개 쿠폰 데이터 용량: 5MB)</li></ul><p><strong>두 번째 시도: Memcached</strong></p><ul><li>Memcached에 쿠폰 데이터를 저장</li><li>문제점: 단일 Key 최대 용량 1MB의 한계에 부딪힘</li></ul><p><strong>세 번째 시도: Memcached 분산 저장</strong></p><ul><li>데이터를 분산해 저장한 후 조합</li><li>문제점: 조합 과정의 시간 소요 증가 및 호출 횟수 증가</li></ul><p><strong>최종 개선 방향: 최소한의 DB 사용</strong></p><p><strong>1. 쿼리 구성 요소 분석</strong></p><ul><li><strong>쿼리 구성 요소</strong>: JOIN, SubQuery, UNION, GROUP BY, ORDER BY, CASE등<br>쿼리를 짜면서 볼수있는 모든것의 집합</li><li><strong>문제점</strong>: 쿼리의 복잡성과 무거운 구조가 성능 저하와 유지보수 비용을 증가</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/791/1*bJnJvV0c3I1BEJIM_WtLIg.png\" /></figure><p><strong>2. 개선 방안</strong></p><ul><li><strong>쿼리 분리</strong>: 가벼운 쿼리로 나누어 개별 성능 향상.</li><li><strong>캐싱 처리</strong>: 캐싱할 수 있는 데이터는 최대한 캐싱.</li><li><strong>로직 처리</strong>: 특정 조건은 쿼리가 아닌 로직으로 처리해 복잡성 경량화.</li></ul><p><strong>3. 구체적 개선 방법</strong></p><ul><li><strong>사용 가능한 쿠폰 필터링 및 캐싱</strong>: 쿠폰 유형으로 필터링한 후 캐싱하여 효율성을 높임</li><li><strong>회원 등급 캐싱</strong>: 개별 회원의 등급 정보를 캐싱하여 조회 성능을 향상</li><li><strong>쿠폰 조건 로직 처리</strong>: 온라인 쿠폰 및 등급 쿠폰의 다양한 조건을 로직으로 처리하여 복잡성을 경감</li><li><strong>회원 보유 쿠폰 처리</strong>: 회원이 보유한 쿠폰을 실시간으로 조회하고 로직을 통해 필터링</li></ul><p><strong>4. 결과</strong></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*hlf7DS4MD3jHpB8kGcTpnA.png\" /></figure><ul><li>평균 Latency가 110ms → 83ms 로 응답속도 약 24.55% 향상</li><li>최초 진입시에는 약 150ms로 기존로직 약 110ms로 기존 보다 조금 느려졌지만 이후 캐싱을 통해서 서비스하면서 속도가 향상됨.</li><li>쿼리를 단순화시켜서 여러번 수행시키면서 기존 사용가능한 쿠폰의 데이터를 구하는 쿼리가 60ms걸렸지만 개선 후 20ms로 쿼리가 가벼워짐</li><li>동일한 로직을 수행시 기존 DB사용량의 변화<br>- 개선 전 로직 DB 사용률 72.5% <br>- 개선 후 최초 호출시 49.5%<br>- 캐싱이 된 후에는 13.6%</li></ul><p>쿼리를 분할하고 Application 로직을 사용하면서 데이터베이스 부하는 줄어들고, 인프라 비용 역시 절감할 수 있었습니다. 이 과정을 통해 무거웠던 600줄짜리 레거시 쿼리와의 아름다운 이별이 가능했습니다.</p><p><strong>마무리</strong></p><p>이번 개선 작업을 통해 쿼리의 속도가 빠르다고 해서 그 복잡성이 괜찮다고 여기는 것은 잘못된 생각이라는 것을 깨달았습니다. 단순히 성능만을 추구하는 것이 아닌, 더 효율적이고 관리하기 쉬운(유지보수가 용이한) 대안을 찾아야 한다는 중요한 교훈을 얻었습니다.</p><p>이번 개선에서의 경험은 다양한 접근 방식이 존재한다는 것을 느끼게 해주었고, 복잡한 쿼리를 단순화하는 것이 기술적 부채를 줄이고 향후 유지보수를 용이하게 한다는 것을 느끼게 해준 개선 작업이었습니다.</p><p>앞으로도 다양한 방법과 유지보수가 용이하도록 지속적인 개선을 통해 더 나은 시스템을 구축해 나갈 것입니다.</p><h3>MUSINSA CAREER</h3><p>함께할 동료를 찾습니다.</p><p>무신사의 폭발적인 성장에는 매년 말 진행 했던 무진장(블랙프라이데이)가 큰 역할을 했습니다. 해당 이벤트로 매출 부스팅 및 신규 유저를 확보 했고, 이렇게 증가된 매출과 유저로 인해 매년 높은 매출 성장을 이뤘으며, 지금의 무신사가 만들어졌습니다. 캠페인개발팀은 무신사의 핵심 행사인 무진장(블랙프라이데이) 캠페인을 주도적으로 개발 및 운영지원 하고 있습니다. 그 외에도 무신사에서 전개 하는 다양한 캠페인 및 이벤트를 적극적으로 지원하고 있습니다. 우리는 팀원 각자 전문성과 열정을 바탕으로 다양한 캠페인을 성공적으로 오픈하고 있으며, 파트너와 함께 고민하며 성장하기 위해 최선을 다하고 있습니다.</p><p><em>🚀 </em><a href=\"https://corp.musinsa.com/ko/career/\"><em>팀 무신사 채용 페이지</em></a><em> (무신사/29CM 전체 포지션 확인이 가능해요)</em></p><p><em>🚀 </em><a href=\"https://kr.linkedin.com/company/musinsacom\"><em>팀 무신사 테크 소식을 받아보는 링크드인</em></a></p><p><em>🚀 </em><a href=\"https://medium.com/musinsa-tech\"><em>무신사 테크 블로그</em></a></p><p><em>🚀 </em><a href=\"https://medium.com/29cm\"><em>29CM 테크 블로그</em></a></p><p><em>🚀 </em><a href=\"https://www.youtube.com/@MUSINSATECH\"><em>무신사 테크 유튜브 채널</em></a></p><p><em>채용이 완료되면 공고가 닫힐 수 있으니 빠르게 지원해 주세요!</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e689d7d932b5\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/musinsa-tech/%EB%AC%B4%EC%8B%A0%EC%82%AC-%EC%84%B1%EC%9E%A5%EA%B3%BC-%ED%95%A8%EA%BB%98-%EA%B1%B0%EB%8C%80%ED%95%B4%EC%A0%B8%EC%98%A8-600%EC%A4%84%EC%A7%9C%EB%A6%AC-%EC%BF%A0%ED%8F%B0-%EC%BF%BC%EB%A6%AC%EC%99%80%EC%9D%98-%EC%95%84%EB%A6%84%EB%8B%A4%EC%9A%B4-%EC%9D%B4%EB%B3%84-e689d7d932b5\">무신사 성장과 함께 거대해져온 600줄짜리 쿠폰 쿼리와의 아름다운 이별</a> was originally published in <a href=\"https://medium.com/musinsa-tech\">MUSINSA tech</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "content:encodedSnippet": "안녕하세요, 무신사 캠페인개발팀에서 무신사의 최대 이벤트인 무진장과 여러 이벤트들을 담당하고 있는 조항준입니다. 이번 글에서는 오래된 레거시 시스템 중 하나였던 쿠폰 시스템을 개선하는 과정에서의 실패와 성공 사례를 공유하고자 합니다.\n왜 개선이 필요한가?\n과거에는 사용자가 쿠폰을 사용할 때만 자신이 보유한 쿠폰을 확인하는 구조였습니다. 그러나 Musinsa 2.0이 도입되면서, Musinsa에서 가장 트래픽이 높은 페이지인 상품 상세 페이지에서 최적의 쿠폰을 실시간으로 제공해야 하는 요구가 생겼습니다.\nMusinsa의 빠른 성장과 함께 트래픽이 폭발적으로 증가했고, 단순했던 쿠폰 쿼리가 점점 복잡해져 결국 600줄에 달하는 거대한 코드로 변모했습니다. 이러한 거대한 쿼리는 더 이상 높은 트래픽을 감당하기 어려웠고, 계속해서 유지하기 위해 막대한 인프라 비용을 지불해야 한다는 문제가 있었습니다.\n지금까지 거대해지면서도 버티고 있었던 개인적인 생각은\n막대한 인프라 비용과 오래 함께한 팀원들의 노력으로 유지 되고 있었다고 생각합니다.\n어떻게 개선하기로 했나?\n1. 쿼리 분할\n\n단일책임원칙은 객체지향 프로그래밍의 SOLID 원칙 중 하나로, 각 클래스나 함수가 하나의 책임만 가져야 한다는 원칙입니다. 각각의 기능이 고유한 책임을 가질 때 코드의 재사용성과 유지보수성이 높아지는 것이 핵심입니다.\n긴 쿼리를 하나의 책임을 가진 여러 개의 짧은 쿼리로 분리하는 작업을 단일책임원칙으로 비유할 수 있습니다. 즉, 각 쿼리는 특정한 데이터나 기능에만 집중하도록 설계하여 효율적으로 필요한 데이터만을 반환할 수 있게 만드는 것입니다.\n위와 같은 이유로 600줄짜리 거대한 쿼리를 여러개의 짧은 쿼리로 쪼개어 자신만의 데이터를 가져오고 그 데이터를 조합하여 Application에서 계산하도록 변경했습니다. 이를 통해 쿼리의 복잡성을 줄이고, 처리 속도를 크게 개선할 수 있었습니다.\n\n2. 캐싱 처리\n\n캐싱을 통한 조회 최적화 모델 구성: 캐싱을 적극적으로 활용해 변화가 많치않고 자주 요청되는 데이터는 캐시된 결과를 반환하도록 최적화, 데이터베이스에는 변화가 적은 데이터만 접근하도록 하여, 데이터베이스 부하를 줄이고 응답 속도를 높히는데 집중을 했습니다.\n\n3. Application 자원 활용\n\n데이터베이스의 책임은 주로 데이터를 안전하게 저장하고, 빠르고 효율적으로 조회하는 데 집중\n애플리케이션의 책임은 비즈니스 로직과 관련된 연산과 데이터 조작을 담당하며, 데이터베이스로부터 데이터를 받아 필요한 연산을 수행하는 역할\n각 자원의 특성을 생각해서 데이터베이스는 안전하고 빠르게 데이터를 주는데 집중하고 Application은 연산을 집중적으로 담당해서 처리 하였습니다.\nApplication 자원의 사용은 비용이 더 저렴하고, 운영 측면에서도 더 효율적이라는 장점이 있었습니다.\n\n실패 사례\n첫 번째 시도: S3\n\n모든 쿠폰 데이터를 배치로 생성해 S3에 업로드\n문제점: 다운로드 지연과 네트워크 비용 증가 \n(1480개 쿠폰 데이터 용량: 5MB)\n\n두 번째 시도: Memcached\n\nMemcached에 쿠폰 데이터를 저장\n문제점: 단일 Key 최대 용량 1MB의 한계에 부딪힘\n\n세 번째 시도: Memcached 분산 저장\n\n데이터를 분산해 저장한 후 조합\n문제점: 조합 과정의 시간 소요 증가 및 호출 횟수 증가\n\n최종 개선 방향: 최소한의 DB 사용\n1. 쿼리 구성 요소 분석\n\n쿼리 구성 요소: JOIN, SubQuery, UNION, GROUP BY, ORDER BY, CASE등\n쿼리를 짜면서 볼수있는 모든것의 집합\n문제점: 쿼리의 복잡성과 무거운 구조가 성능 저하와 유지보수 비용을 증가\n\n2. 개선 방안\n\n쿼리 분리: 가벼운 쿼리로 나누어 개별 성능 향상.\n캐싱 처리: 캐싱할 수 있는 데이터는 최대한 캐싱.\n로직 처리: 특정 조건은 쿼리가 아닌 로직으로 처리해 복잡성 경량화.\n\n3. 구체적 개선 방법\n\n사용 가능한 쿠폰 필터링 및 캐싱: 쿠폰 유형으로 필터링한 후 캐싱하여 효율성을 높임\n회원 등급 캐싱: 개별 회원의 등급 정보를 캐싱하여 조회 성능을 향상\n쿠폰 조건 로직 처리: 온라인 쿠폰 및 등급 쿠폰의 다양한 조건을 로직으로 처리하여 복잡성을 경감\n회원 보유 쿠폰 처리: 회원이 보유한 쿠폰을 실시간으로 조회하고 로직을 통해 필터링\n\n4. 결과\n\n평균 Latency가 110ms → 83ms 로 응답속도 약 24.55% 향상\n최초 진입시에는 약 150ms로 기존로직 약 110ms로 기존 보다 조금 느려졌지만 이후 캐싱을 통해서 서비스하면서 속도가 향상됨.\n쿼리를 단순화시켜서 여러번 수행시키면서 기존 사용가능한 쿠폰의 데이터를 구하는 쿼리가 60ms걸렸지만 개선 후 20ms로 쿼리가 가벼워짐\n동일한 로직을 수행시 기존 DB사용량의 변화\n- 개선 전 로직 DB 사용률 72.5% \n- 개선 후 최초 호출시 49.5%\n- 캐싱이 된 후에는 13.6%\n\n쿼리를 분할하고 Application 로직을 사용하면서 데이터베이스 부하는 줄어들고, 인프라 비용 역시 절감할 수 있었습니다. 이 과정을 통해 무거웠던 600줄짜리 레거시 쿼리와의 아름다운 이별이 가능했습니다.\n마무리\n이번 개선 작업을 통해 쿼리의 속도가 빠르다고 해서 그 복잡성이 괜찮다고 여기는 것은 잘못된 생각이라는 것을 깨달았습니다. 단순히 성능만을 추구하는 것이 아닌, 더 효율적이고 관리하기 쉬운(유지보수가 용이한) 대안을 찾아야 한다는 중요한 교훈을 얻었습니다.\n이번 개선에서의 경험은 다양한 접근 방식이 존재한다는 것을 느끼게 해주었고, 복잡한 쿼리를 단순화하는 것이 기술적 부채를 줄이고 향후 유지보수를 용이하게 한다는 것을 느끼게 해준 개선 작업이었습니다.\n앞으로도 다양한 방법과 유지보수가 용이하도록 지속적인 개선을 통해 더 나은 시스템을 구축해 나갈 것입니다.\nMUSINSA CAREER\n함께할 동료를 찾습니다.\n무신사의 폭발적인 성장에는 매년 말 진행 했던 무진장(블랙프라이데이)가 큰 역할을 했습니다. 해당 이벤트로 매출 부스팅 및 신규 유저를 확보 했고, 이렇게 증가된 매출과 유저로 인해 매년 높은 매출 성장을 이뤘으며, 지금의 무신사가 만들어졌습니다. 캠페인개발팀은 무신사의 핵심 행사인 무진장(블랙프라이데이) 캠페인을 주도적으로 개발 및 운영지원 하고 있습니다. 그 외에도 무신사에서 전개 하는 다양한 캠페인 및 이벤트를 적극적으로 지원하고 있습니다. 우리는 팀원 각자 전문성과 열정을 바탕으로 다양한 캠페인을 성공적으로 오픈하고 있으며, 파트너와 함께 고민하며 성장하기 위해 최선을 다하고 있습니다.\n🚀 팀 무신사 채용 페이지 (무신사/29CM 전체 포지션 확인이 가능해요)\n🚀 팀 무신사 테크 소식을 받아보는 링크드인\n🚀 무신사 테크 블로그\n🚀 29CM 테크 블로그\n🚀 무신사 테크 유튜브 채널\n채용이 완료되면 공고가 닫힐 수 있으니 빠르게 지원해 주세요!\n\n무신사 성장과 함께 거대해져온 600줄짜리 쿠폰 쿼리와의 아름다운 이별 was originally published in MUSINSA tech on Medium, where people are continuing the conversation by highlighting and responding to this story.",
    "dc:creator": "Hangjun Cho",
    "guid": "https://medium.com/p/e689d7d932b5",
    "categories": [
      "query",
      "backend",
      "쿼리",
      "musinsa",
      "memcached"
    ],
    "isoDate": "2024-11-13T10:34:43.000Z"
  },
  {
    "creator": "CHANMUL",
    "title": "재고 서비스의 진화와 혁신: 지속적인 개선을 통한 안정성과 확장성 강화",
    "link": "https://medium.com/musinsa-tech/%EC%9E%AC%EA%B3%A0-%EC%84%9C%EB%B9%84%EC%8A%A4%EC%9D%98-%EC%A7%84%ED%99%94%EC%99%80-%ED%98%81%EC%8B%A0-%EC%A7%80%EC%86%8D%EC%A0%81%EC%9D%B8-%EA%B0%9C%EC%84%A0%EC%9D%84-%ED%86%B5%ED%95%9C-%EC%95%88%EC%A0%95%EC%84%B1%EA%B3%BC-%ED%99%95%EC%9E%A5%EC%84%B1-%EA%B0%95%ED%99%94-cb851f1ff782?source=rss----f107b03c406e---4",
    "pubDate": "Wed, 23 Oct 2024 02:08:13 GMT",
    "content:encoded": "<p>안녕하세요.</p><p>저는 무신사 재고 서비스의 백엔드 개발을 담당하고 있는 백승호입니다.</p><p>이 포스트에서는 재고 시스템의 개선을 위한 구조적 변화와 다운타임을 최소화하기 위해서 정기적인 유지보수, 백업 시스템, 고가용성(HA) 아키텍처, 재난 복구 계획 등의 전략을 세우고 개선하는 것에 대해 소개하고자 합니다.</p><h3><strong>재고 서비스의 과거와 변화</strong></h3><p>재고 관리 서비스는 기업의 운영에서 핵심적인 역할을 담당하며, 특히 고객 만족도를 직접적으로 영향을 미치는 요소로 자리잡고 있습니다. 초기에는 재고 관리가 다소 소홀히 다루어져 왔으며, 운영자들은 창고에서 직접 재고를 파악하고, 수기로 어드민 시스템에 재고 입고 및 출고 정보를 입력하는 방식으로 진행했습니다. 이러한 수작업 중심의 관리 방식은 효율성이 낮고 오류 발생 가능성이 높아, 재고의 정확성 유지와 신속한 처리가 어렵다는 문제를 안고 있었습니다. 출고 바코드 관리 또한 수동으로 이루어져, 재고 관리의 중요성은 상대적으로 낮게 평가되었습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*MzW9e4UDDMykQ8qN\" /></figure><h4><strong>재고 서비스의 초기 운영</strong></h4><p>초기 재고 관리 시스템은 <strong>모놀리식 PHP-MySQL</strong> 구조로 운영되었고, 재고 처리는 여러 서비스에 분산되어 있었으며, 이로 인해 유지보수가 어려운 구조를 띄었습니다. 운영자들이 수작업으로 재고를 관리하고 출고 바코드를 입력하는 방식은 오류 발생의 위험을 높였고, 빠르게 변화하는 시장 환경에 신속히 대응하기 어려웠습니다. 특히, 수동 처리로 인해 품절 취소율이 높아져 고객 서비스에도 부정적인 영향을 미쳤습니다.</p><p>이러한 상황 속에서 재고 서비스의 개편 필요성이 대두되었습니다. PM인 <strong>이선화</strong>님과 서비스 운영과 개발을 하던 <strong>황두리</strong>님, 지금은 퇴사했지만 당시 함께 일하던 <strong>함지선</strong>님과 함께 이러한 변화에 대응하기 위해 재고 서비스의 전반적인 개편 작업을 시작하게 되었습니다.</p><h4><strong>ERP 도입과 재고 관리의 변화</strong></h4><p>ERP 시스템의 도입은 재고 관리 방식에 큰 변화를 가져왔습니다. 재고 관리의 중요성이 부각 되었고, 물류 운영, 회계, 매장 등 여러 부서에서 사용되던 재고 관리 방식이 ERP 시스템에 통합되어 재고의 정확성과 일관성을 확보할 필요성이 생겼습니다. 재고 도메인은 주문 서비스에서 분리되어 독립된 팀으로 관리 되었으며, 재고 서비스는 물류와 함께 더욱 중요한 역할을 맡게 되었습니다.</p><h3><strong>재고 서비스의 개편과 주요 기술 도입</strong></h3><h4><strong>재고 서비스의 구조적 변화</strong></h4><p>재고 서비스의 개편은 점진적인 코드 이전과 신규 재고 서비스 도입으로 시작되었습니다. 초기에는 모든 재고 처리 로직이 다양한 서비스에 산재되어 있었기 때문에, 이 로직들을 일원화하고 단일 애플리케이션에서 여러 느슨하게 결합된 독립적인 서비스로 전환하는 작업이 필요했습니다. 이 작업은 상당한 리스크를 동반했지만, 재고 처리의 일관성을 확보하기 위해서는 반드시 필요한 과정이었습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*YH0nQkuSdXOtY7UP\" /></figure><h4><strong>ERP 시스템 도입</strong></h4><p>ERP 시스템의 도입은 재고 관리 방식을 완전히 바꾸었습니다. ERP 시스템은 매입 및 오프라인 재고의 입출고를 자동화하여 실물 재고와 시스템상의 재고 간의 오차를 0.03% 이내로 줄이는 데 기여했습니다. 또한, 재고 관리 시스템은 물류 센터와의 동기화를 통해 재고 정보의 실시간 갱신을 가능하게 했으며, 이로 인해 미할당건중 재고가 부족하여 품절 취소된 케이스 개선전 전체 재고 처리량의 최대 피크 3.4% 발생하였으나 개선 후 3개월 평균 0.54%를 유지하고 개선 전 대비 평균 86%를 감소하는 성과를 달성할 수 있었습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*VVPmtki_W1vfDNIf\" /></figure><h4><strong>Redis와 Kafka의 도입</strong></h4><p>재고 서비스의 개선 과정에서 중요한 기술적 요소로 <strong>Redis</strong>와 <strong>Kafka</strong>가 도입되었습니다. Redis는 재고 처리의 동시성 문제를 해결하기 위해 사용되었으며, 재고 데이터의 빠른 접근과 처리 속도를 보장하는 데 중요한 역할을 했습니다. Redis를 통해 재고 데이터는 실시간으로 캐싱되었으며, 재고 처리 결과는 비동기적으로 Kafka 이벤트를 통해 전달되었습니다. 이러한 비동기 처리는 재고 데이터의 일관성을 유지하면서도 처리 속도를 높이는 데 기여했습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*Y1ExWCq-CWLDWyXt\" /></figure><p>Redis의 데이터 타입은 hash를 이용하였으며 hash를 이용할 경우 장점은 스캔 시 value도 함께 조회하여 데이터 조회에 O(n)으로 처리 가능합니다.</p><p>Redis 에 적재한 재고 데이터는 데일리 대사를 진행하고 병렬처리 방식으로 전체 재고의 대사를 진행합니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*1nwE5fMKuiOzGCj7\" /></figure><p>Redis에 재고를 적재하고 처리할 경우 고려해야 할 사항은 트랜잭션 롤백입니다. RDB를 이용한 처리의 경우 트랜잭션 보장으로 Request 단위의 원자성과 일관성을 유지하지만 Redis는 구현하여 대응할 수 있습니다. redis의 watch를 이용한 트랜잭션 처리도 가능합니다.</p><p>재고 처리 정보 로드 서킷 적용하여 Elasticsearch의 순단이나 장애가 발생하였을 경우 데이터베이스에서 정보를 로드하여 재고 처리가 가능하도록 적용하였습니다.</p><h3><strong>주요 성과 및 개선 사항</strong></h3><h4><strong>재고 서비스의 마이크로서비스화</strong></h4><p>재고 서비스의 마이크로서비스화는 기존의 단일 애플리케이션에서 느슨하게 결합된 독립적인 서비스로의 전환을 성공적으로 이끌었습니다. 이를 통해 재고 처리 로직의 일원화가 이루어졌으며, 다양한 클라이언트의 요구에 따라 정책이 변화하는 복잡한 재고 관리 환경에서도 효율적인 대응이 가능해졌습니다. 이로 인해 재고 서비스는 독립된 서비스로써의 역할을 제대로 수행할 수 있게 되었으며, 재고 처리의 일관성과 신뢰성을 확보할 수 있었습니다.</p><h4><strong>배치 처리 및 성능 개선</strong></h4><p>재고 서비스의 배치 처리 성능도 크게 개선되었습니다. 최대 5000개의 SKU를 한 번에 처리할 수 있는 배치성 API를 통해 옵션 생성 및 수정 작업의 지연 시간을 기존의 p95 10초에서 p95 900ms 이내로 단축시켰습니다. 이러한 성능 개선은 재고 관리 시스템의 효율성을 크게 향상시켰으며, 실시간으로 재고 데이터를 갱신할 수 있는 능력을 확보하게 했습니다.</p><h4><strong>재고 처리의 동시성 및 안정성 강화</strong></h4><p>Redis를 활용한 재고 처리 방식은 재고 서비스의 동시성과 안정성을 크게 강화했습니다. Redis는 단일 스레드로 동작하기 때문에 동시성을 보장하면서도, 전체 자원의 약 4%만을 사용하여 재고 처리의 정합성을 99.9%로 유지할 수 있었습니다. 이러한 성과는 재고 처리 오류로 인한 과주문 및 품절 취소를 현저히 줄이는 데 기여하였습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*COjVKBPeAHBsrcmr\" /></figure><p>재고 서비스의 데이터베이스와 Redis 간의 데이터 동기화는 Kafka 이벤트를 통해 준 실시간으로 이루어졌습니다. 이를 통해 재고 데이터의 최신성을 유지하면서도, 재고 처리 결과를 빠르게 반영할 수 있었습니다. 이러한 동기화 작업은 데이터베이스의 일관성을 유지하는 데 중요한 역할을 했습니다.</p><p>여름 블랙프라이데이, 이벤트 등으로 인한 스파이크 트래픽에도 안정적인 latency를 유지하여 주문 및 재고 처리에 안정성을 확보 하였습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*O_03Xvknm81DXGv4\" /></figure><h3><strong>향후 계획 및 추가 개선 사항</strong></h3><h4><strong>API 개선 및 성능 최적화</strong></h4><p>재고 처리와 관련된 API를 재설계하여 성능과 안정성을 더욱 강화할 계획입니다. 현재 재고 처리 시 필요로 하는 부가 정보, 예를 들어 상품, 옵션, 매장 정보 등을 RDB에서 가져오고 있지만, 이를 <strong>NoSQL 서비스</strong>로 캐싱하는 방안을 도입하여 데이터 제공의 안정성을 높이고 처리 시간을 단축하고자 합니다. 이 방식은 재고 처리의 신속성과 정확성을 향상시키는 데 중요한 역할을 할 것입니다.</p><p>단일 SKU에 대한 처리 속도를 더욱 안정적으로 높이기 위해 재고 처리 내역에 대한 순서 보장 로직을 정리하고 최적화할 예정입니다. 이를 통해 재고 관리 시스템의 성능을 극대화하면서도, 동시성 문제를 최소화할 수 있을 것입니다.</p><h4><strong>추가 자원 최적화 및 모니터링 강화</strong></h4><p>Redis의 사용량을 최적화하면서도 최대의 효율을 달성할 수 있는 방법을 모색하고 있습니다. 특히, Redis 메모리 관리와 교체 후에도 서비스의 안정적인 운영을 보장할 수 있는 인프라를 구축하였습니다. 또한, 실시간 모니터링 시스템을 도입하여 재고 처리의 모든 단계를 철저히 관리하고, 잠재적인 문제를 사전에 감지하여 대응할 수 있도록 할 것입니다. 이러한 모니터링 강화는 재고 관리 시스템의 안정성과 신뢰성을 한층 더 높일 것입니다.</p><h3><strong>재고 서비스의 운영과 안정성 보장</strong></h3><h4><strong>Redis 장애 대응 및 메모리 관리</strong></h4><p>Redis의 인프라 장애는 재고 서비스의 전면 장애로 이어질 수 있는 중요한 문제입니다. 이를 해결하기 위해 Redis의 메모리 초기화 및 교체 후에도 데이터베이스의 재고를 기준으로 서비스 운영이 가능하도록 개발을 진행하였습니다. 이러한 장애 대응 방안은 재고 관리 시스템의 안정성 및 다운 타임을 보장하는 데 중요한 역할을 합니다.</p><h4><strong>비동기 데이터 업데이트 및 실시간 동기화</strong></h4><p>재고 처리 결과 데이터의 업데이트는 비동기적으로 진행되며, 이를 통해 재고 데이터의 최신성을 유지하면서도 처리 속도를 높일 수 있었습니다. 또한, Kafka 이벤트를 통한 외부 연동 서비스(물류, ERP 등)와 비동기 데이터 업데이트 및 실시간 동기화하여 재고 관리의 정확성을 높이는 데 기여했습니다. 이러한 비동기 처리 방식은 재고 관리 시스템의 효율성을 극대화하는 데 중요한 역할을 하였습니다.</p><h3><strong>결론: 재고 서비스의 혁신과 미래</strong></h3><p>재고 관리 시스템의 혁신은 ERP 시스템 도입과 함께 시작되었습니다. 수작업으로 이루어지던 재고 관리는 자동화 시스템으로 대체되었으며, Redis와 Kafka를 통한 개선 작업은 재고 서비스의 효율성과 안정성을 높이는 데 기여했습니다. 이러한 변화는 품절 취소율을 감소시키고, 재고 관리 시스템의 신뢰성을 확보하는 데 큰 도움이 되었습니다.</p><p>재고 서비스의 향후 계획은 API 개선, 성능 최적화, 모니터링 강화 등을 포함하며, 이러한 노력이 재고 관리 시스템의 지속적인 개선과 혁신으로 이어질 것입니다. 지속적인 기술 발전과 프로세스 개선을 통해 재고 관리의 중요성을 더욱 높이고, 고객 만족을 극대화할 수 있는 기반을 다질 예정입니다.</p><p>함께 고생한 <strong>이선화</strong>님, <strong>황두리</strong>님, <strong>함지선</strong>님 그리고 상품 개발 파트 동료 분들, 주문팀, ERP팀, 물류팀 등 도움 주신 모든 분들 감사드립니다.</p><h3>MUSINSA CAREER</h3><p>함께할 동료를 찾습니다.</p><p>무신사 상품 개발팀은 고객에게 최고의 가치를 제공할 수 있는 <strong>상품</strong>을 개발하는 핵심 부서입니다. 우리는 다양한 상품에 대한 <strong>기준 정보</strong>를 구축하고 이를 바탕으로 제품의 일관성과 품질을 유지합니다. 상품의 세부 사항을 꼼꼼하게 관리하며, <strong>재고</strong> 상태와 연동된 정보를 통해 효율적인 운영이 가능하도록 합니다.</p><p>또한, <strong>상품 상세</strong> 페이지를 설계하여 고객이 상품에 대한 정확한 정보를 얻을 수 있도록 하며, 이 모든 과정을 <strong>코어</strong> 시스템과 긴밀하게 연계해 개발 전반에 걸쳐 높은 트래픽에도 안정적이고 효율적인 프로세스를 구현합니다.</p><p>상품 개발팀은 창의성과 기술적 역량을 바탕으로 시장의 요구에 부합하는 혁신적인 제품을 지속적으로 선보이며, 회사의 성공적인 성장을 이끄는 중요한 역할을 하고 있습니다.</p><p><em>🚀 </em><a href=\"https://corp.musinsa.com/ko/career/\"><em>팀 무신사 채용 페이지</em></a><em> (무신사/29CM 전체 포지션 확인이 가능해요)</em></p><p><em>🚀 </em><a href=\"https://kr.linkedin.com/company/musinsacom\"><em>팀 무신사 테크 소식을 받아보는 링크드인</em></a></p><p><em>🚀 </em><a href=\"https://medium.com/musinsa-tech\"><em>무신사 테크 블로그</em></a></p><p><em>🚀 </em><a href=\"https://medium.com/29cm\"><em>29CM 테크 블로그</em></a></p><p><em>🚀 </em><a href=\"https://www.youtube.com/@MUSINSATECH\"><em>무신사 테크 유튜브 채널</em></a></p><p><em>채용이 완료되면 공고가 닫힐 수 있으니 빠르게 지원해 주세요!</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=cb851f1ff782\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/musinsa-tech/%EC%9E%AC%EA%B3%A0-%EC%84%9C%EB%B9%84%EC%8A%A4%EC%9D%98-%EC%A7%84%ED%99%94%EC%99%80-%ED%98%81%EC%8B%A0-%EC%A7%80%EC%86%8D%EC%A0%81%EC%9D%B8-%EA%B0%9C%EC%84%A0%EC%9D%84-%ED%86%B5%ED%95%9C-%EC%95%88%EC%A0%95%EC%84%B1%EA%B3%BC-%ED%99%95%EC%9E%A5%EC%84%B1-%EA%B0%95%ED%99%94-cb851f1ff782\">재고 서비스의 진화와 혁신: 지속적인 개선을 통한 안정성과 확장성 강화</a> was originally published in <a href=\"https://medium.com/musinsa-tech\">MUSINSA tech</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "content:encodedSnippet": "안녕하세요.\n저는 무신사 재고 서비스의 백엔드 개발을 담당하고 있는 백승호입니다.\n이 포스트에서는 재고 시스템의 개선을 위한 구조적 변화와 다운타임을 최소화하기 위해서 정기적인 유지보수, 백업 시스템, 고가용성(HA) 아키텍처, 재난 복구 계획 등의 전략을 세우고 개선하는 것에 대해 소개하고자 합니다.\n재고 서비스의 과거와 변화\n재고 관리 서비스는 기업의 운영에서 핵심적인 역할을 담당하며, 특히 고객 만족도를 직접적으로 영향을 미치는 요소로 자리잡고 있습니다. 초기에는 재고 관리가 다소 소홀히 다루어져 왔으며, 운영자들은 창고에서 직접 재고를 파악하고, 수기로 어드민 시스템에 재고 입고 및 출고 정보를 입력하는 방식으로 진행했습니다. 이러한 수작업 중심의 관리 방식은 효율성이 낮고 오류 발생 가능성이 높아, 재고의 정확성 유지와 신속한 처리가 어렵다는 문제를 안고 있었습니다. 출고 바코드 관리 또한 수동으로 이루어져, 재고 관리의 중요성은 상대적으로 낮게 평가되었습니다.\n\n재고 서비스의 초기 운영\n초기 재고 관리 시스템은 모놀리식 PHP-MySQL 구조로 운영되었고, 재고 처리는 여러 서비스에 분산되어 있었으며, 이로 인해 유지보수가 어려운 구조를 띄었습니다. 운영자들이 수작업으로 재고를 관리하고 출고 바코드를 입력하는 방식은 오류 발생의 위험을 높였고, 빠르게 변화하는 시장 환경에 신속히 대응하기 어려웠습니다. 특히, 수동 처리로 인해 품절 취소율이 높아져 고객 서비스에도 부정적인 영향을 미쳤습니다.\n이러한 상황 속에서 재고 서비스의 개편 필요성이 대두되었습니다. PM인 이선화님과 서비스 운영과 개발을 하던 황두리님, 지금은 퇴사했지만 당시 함께 일하던 함지선님과 함께 이러한 변화에 대응하기 위해 재고 서비스의 전반적인 개편 작업을 시작하게 되었습니다.\nERP 도입과 재고 관리의 변화\nERP 시스템의 도입은 재고 관리 방식에 큰 변화를 가져왔습니다. 재고 관리의 중요성이 부각 되었고, 물류 운영, 회계, 매장 등 여러 부서에서 사용되던 재고 관리 방식이 ERP 시스템에 통합되어 재고의 정확성과 일관성을 확보할 필요성이 생겼습니다. 재고 도메인은 주문 서비스에서 분리되어 독립된 팀으로 관리 되었으며, 재고 서비스는 물류와 함께 더욱 중요한 역할을 맡게 되었습니다.\n재고 서비스의 개편과 주요 기술 도입\n재고 서비스의 구조적 변화\n재고 서비스의 개편은 점진적인 코드 이전과 신규 재고 서비스 도입으로 시작되었습니다. 초기에는 모든 재고 처리 로직이 다양한 서비스에 산재되어 있었기 때문에, 이 로직들을 일원화하고 단일 애플리케이션에서 여러 느슨하게 결합된 독립적인 서비스로 전환하는 작업이 필요했습니다. 이 작업은 상당한 리스크를 동반했지만, 재고 처리의 일관성을 확보하기 위해서는 반드시 필요한 과정이었습니다.\n\nERP 시스템 도입\nERP 시스템의 도입은 재고 관리 방식을 완전히 바꾸었습니다. ERP 시스템은 매입 및 오프라인 재고의 입출고를 자동화하여 실물 재고와 시스템상의 재고 간의 오차를 0.03% 이내로 줄이는 데 기여했습니다. 또한, 재고 관리 시스템은 물류 센터와의 동기화를 통해 재고 정보의 실시간 갱신을 가능하게 했으며, 이로 인해 미할당건중 재고가 부족하여 품절 취소된 케이스 개선전 전체 재고 처리량의 최대 피크 3.4% 발생하였으나 개선 후 3개월 평균 0.54%를 유지하고 개선 전 대비 평균 86%를 감소하는 성과를 달성할 수 있었습니다.\n\nRedis와 Kafka의 도입\n재고 서비스의 개선 과정에서 중요한 기술적 요소로 Redis와 Kafka가 도입되었습니다. Redis는 재고 처리의 동시성 문제를 해결하기 위해 사용되었으며, 재고 데이터의 빠른 접근과 처리 속도를 보장하는 데 중요한 역할을 했습니다. Redis를 통해 재고 데이터는 실시간으로 캐싱되었으며, 재고 처리 결과는 비동기적으로 Kafka 이벤트를 통해 전달되었습니다. 이러한 비동기 처리는 재고 데이터의 일관성을 유지하면서도 처리 속도를 높이는 데 기여했습니다.\n\nRedis의 데이터 타입은 hash를 이용하였으며 hash를 이용할 경우 장점은 스캔 시 value도 함께 조회하여 데이터 조회에 O(n)으로 처리 가능합니다.\nRedis 에 적재한 재고 데이터는 데일리 대사를 진행하고 병렬처리 방식으로 전체 재고의 대사를 진행합니다.\n\nRedis에 재고를 적재하고 처리할 경우 고려해야 할 사항은 트랜잭션 롤백입니다. RDB를 이용한 처리의 경우 트랜잭션 보장으로 Request 단위의 원자성과 일관성을 유지하지만 Redis는 구현하여 대응할 수 있습니다. redis의 watch를 이용한 트랜잭션 처리도 가능합니다.\n재고 처리 정보 로드 서킷 적용하여 Elasticsearch의 순단이나 장애가 발생하였을 경우 데이터베이스에서 정보를 로드하여 재고 처리가 가능하도록 적용하였습니다.\n주요 성과 및 개선 사항\n재고 서비스의 마이크로서비스화\n재고 서비스의 마이크로서비스화는 기존의 단일 애플리케이션에서 느슨하게 결합된 독립적인 서비스로의 전환을 성공적으로 이끌었습니다. 이를 통해 재고 처리 로직의 일원화가 이루어졌으며, 다양한 클라이언트의 요구에 따라 정책이 변화하는 복잡한 재고 관리 환경에서도 효율적인 대응이 가능해졌습니다. 이로 인해 재고 서비스는 독립된 서비스로써의 역할을 제대로 수행할 수 있게 되었으며, 재고 처리의 일관성과 신뢰성을 확보할 수 있었습니다.\n배치 처리 및 성능 개선\n재고 서비스의 배치 처리 성능도 크게 개선되었습니다. 최대 5000개의 SKU를 한 번에 처리할 수 있는 배치성 API를 통해 옵션 생성 및 수정 작업의 지연 시간을 기존의 p95 10초에서 p95 900ms 이내로 단축시켰습니다. 이러한 성능 개선은 재고 관리 시스템의 효율성을 크게 향상시켰으며, 실시간으로 재고 데이터를 갱신할 수 있는 능력을 확보하게 했습니다.\n재고 처리의 동시성 및 안정성 강화\nRedis를 활용한 재고 처리 방식은 재고 서비스의 동시성과 안정성을 크게 강화했습니다. Redis는 단일 스레드로 동작하기 때문에 동시성을 보장하면서도, 전체 자원의 약 4%만을 사용하여 재고 처리의 정합성을 99.9%로 유지할 수 있었습니다. 이러한 성과는 재고 처리 오류로 인한 과주문 및 품절 취소를 현저히 줄이는 데 기여하였습니다.\n\n재고 서비스의 데이터베이스와 Redis 간의 데이터 동기화는 Kafka 이벤트를 통해 준 실시간으로 이루어졌습니다. 이를 통해 재고 데이터의 최신성을 유지하면서도, 재고 처리 결과를 빠르게 반영할 수 있었습니다. 이러한 동기화 작업은 데이터베이스의 일관성을 유지하는 데 중요한 역할을 했습니다.\n여름 블랙프라이데이, 이벤트 등으로 인한 스파이크 트래픽에도 안정적인 latency를 유지하여 주문 및 재고 처리에 안정성을 확보 하였습니다.\n\n향후 계획 및 추가 개선 사항\nAPI 개선 및 성능 최적화\n재고 처리와 관련된 API를 재설계하여 성능과 안정성을 더욱 강화할 계획입니다. 현재 재고 처리 시 필요로 하는 부가 정보, 예를 들어 상품, 옵션, 매장 정보 등을 RDB에서 가져오고 있지만, 이를 NoSQL 서비스로 캐싱하는 방안을 도입하여 데이터 제공의 안정성을 높이고 처리 시간을 단축하고자 합니다. 이 방식은 재고 처리의 신속성과 정확성을 향상시키는 데 중요한 역할을 할 것입니다.\n단일 SKU에 대한 처리 속도를 더욱 안정적으로 높이기 위해 재고 처리 내역에 대한 순서 보장 로직을 정리하고 최적화할 예정입니다. 이를 통해 재고 관리 시스템의 성능을 극대화하면서도, 동시성 문제를 최소화할 수 있을 것입니다.\n추가 자원 최적화 및 모니터링 강화\nRedis의 사용량을 최적화하면서도 최대의 효율을 달성할 수 있는 방법을 모색하고 있습니다. 특히, Redis 메모리 관리와 교체 후에도 서비스의 안정적인 운영을 보장할 수 있는 인프라를 구축하였습니다. 또한, 실시간 모니터링 시스템을 도입하여 재고 처리의 모든 단계를 철저히 관리하고, 잠재적인 문제를 사전에 감지하여 대응할 수 있도록 할 것입니다. 이러한 모니터링 강화는 재고 관리 시스템의 안정성과 신뢰성을 한층 더 높일 것입니다.\n재고 서비스의 운영과 안정성 보장\nRedis 장애 대응 및 메모리 관리\nRedis의 인프라 장애는 재고 서비스의 전면 장애로 이어질 수 있는 중요한 문제입니다. 이를 해결하기 위해 Redis의 메모리 초기화 및 교체 후에도 데이터베이스의 재고를 기준으로 서비스 운영이 가능하도록 개발을 진행하였습니다. 이러한 장애 대응 방안은 재고 관리 시스템의 안정성 및 다운 타임을 보장하는 데 중요한 역할을 합니다.\n비동기 데이터 업데이트 및 실시간 동기화\n재고 처리 결과 데이터의 업데이트는 비동기적으로 진행되며, 이를 통해 재고 데이터의 최신성을 유지하면서도 처리 속도를 높일 수 있었습니다. 또한, Kafka 이벤트를 통한 외부 연동 서비스(물류, ERP 등)와 비동기 데이터 업데이트 및 실시간 동기화하여 재고 관리의 정확성을 높이는 데 기여했습니다. 이러한 비동기 처리 방식은 재고 관리 시스템의 효율성을 극대화하는 데 중요한 역할을 하였습니다.\n결론: 재고 서비스의 혁신과 미래\n재고 관리 시스템의 혁신은 ERP 시스템 도입과 함께 시작되었습니다. 수작업으로 이루어지던 재고 관리는 자동화 시스템으로 대체되었으며, Redis와 Kafka를 통한 개선 작업은 재고 서비스의 효율성과 안정성을 높이는 데 기여했습니다. 이러한 변화는 품절 취소율을 감소시키고, 재고 관리 시스템의 신뢰성을 확보하는 데 큰 도움이 되었습니다.\n재고 서비스의 향후 계획은 API 개선, 성능 최적화, 모니터링 강화 등을 포함하며, 이러한 노력이 재고 관리 시스템의 지속적인 개선과 혁신으로 이어질 것입니다. 지속적인 기술 발전과 프로세스 개선을 통해 재고 관리의 중요성을 더욱 높이고, 고객 만족을 극대화할 수 있는 기반을 다질 예정입니다.\n함께 고생한 이선화님, 황두리님, 함지선님 그리고 상품 개발 파트 동료 분들, 주문팀, ERP팀, 물류팀 등 도움 주신 모든 분들 감사드립니다.\nMUSINSA CAREER\n함께할 동료를 찾습니다.\n무신사 상품 개발팀은 고객에게 최고의 가치를 제공할 수 있는 상품을 개발하는 핵심 부서입니다. 우리는 다양한 상품에 대한 기준 정보를 구축하고 이를 바탕으로 제품의 일관성과 품질을 유지합니다. 상품의 세부 사항을 꼼꼼하게 관리하며, 재고 상태와 연동된 정보를 통해 효율적인 운영이 가능하도록 합니다.\n또한, 상품 상세 페이지를 설계하여 고객이 상품에 대한 정확한 정보를 얻을 수 있도록 하며, 이 모든 과정을 코어 시스템과 긴밀하게 연계해 개발 전반에 걸쳐 높은 트래픽에도 안정적이고 효율적인 프로세스를 구현합니다.\n상품 개발팀은 창의성과 기술적 역량을 바탕으로 시장의 요구에 부합하는 혁신적인 제품을 지속적으로 선보이며, 회사의 성공적인 성장을 이끄는 중요한 역할을 하고 있습니다.\n🚀 팀 무신사 채용 페이지 (무신사/29CM 전체 포지션 확인이 가능해요)\n🚀 팀 무신사 테크 소식을 받아보는 링크드인\n🚀 무신사 테크 블로그\n🚀 29CM 테크 블로그\n🚀 무신사 테크 유튜브 채널\n채용이 완료되면 공고가 닫힐 수 있으니 빠르게 지원해 주세요!\n\n재고 서비스의 진화와 혁신: 지속적인 개선을 통한 안정성과 확장성 강화 was originally published in MUSINSA tech on Medium, where people are continuing the conversation by highlighting and responding to this story.",
    "dc:creator": "CHANMUL",
    "guid": "https://medium.com/p/cb851f1ff782",
    "categories": [
      "상품개발",
      "microservice-architecture",
      "재고서비스",
      "혁신과-미래"
    ],
    "isoDate": "2024-10-23T02:08:13.000Z"
  },
  {
    "creator": "최영민",
    "title": "AI와 함께하는 패션 큐레이션 — 무신사 2.0 시나리오 기반 추천 시스템 개발",
    "link": "https://medium.com/musinsa-tech/ai%EC%99%80-%ED%95%A8%EA%BB%98%ED%95%98%EB%8A%94-%ED%8C%A8%EC%85%98-%ED%81%90%EB%A0%88%EC%9D%B4%EC%85%98-%EB%AC%B4%EC%8B%A0%EC%82%AC-2-0-%EC%8B%9C%EB%82%98%EB%A6%AC%EC%98%A4-%EA%B8%B0%EB%B0%98-%EC%B6%94%EC%B2%9C-%EC%8B%9C%EC%8A%A4%ED%85%9C-%EA%B0%9C%EB%B0%9C-db7020b20b68?source=rss----f107b03c406e---4",
    "pubDate": "Fri, 11 Oct 2024 04:43:32 GMT",
    "content:encoded": "<h3>AI와 함께하는 패션 큐레이션 — 무신사 2.0 시나리오 기반 추천 시스템 개발</h3><p>안녕하세요. 무신사 데이터사이언스 팀에서 추천 시스템 관련 분석과 모델링 업무를 하고 있는 <a href=\"https://www.linkedin.com/in/%EC%98%81%EB%AF%BC-%EC%B5%9C-23552688/\"><strong>최영민</strong></a><strong>, </strong><a href=\"https://www.linkedin.com/in/%EB%AA%85%ED%9C%98-%EC%9D%B4-5813a0176/\"><strong>이명휘</strong></a> 입니다.</p><p>최근 <strong>무신사 2.0</strong>이라는 타이틀로 엔지니어링, 기획, 디자인 등 서비스 전반에서 많은 개편이 있었습니다. 저희 데이터사이언스 팀에서도 추천 시스템을 필두로 여러 개편작업을 진행하게 되었고, 이를 달성하기 위해 고민했던 부분을 공유하고자 합니다.</p><h3><strong>무신사 2.0, 새로워진 추천영역: 패션 큐레이션의 진화</strong></h3><p>무신사를 비롯한 이커머스의 추천 시스템은 기업이 추구하는 비즈니스 방향성과 밀접하게 연관되어 있으며, 각기 다른 <strong>사용자 경험</strong>과 <strong>정량적인 성과</strong>를 창출합니다. 이는 개인화된 상품만을 지속적으로 큐레이션 하는 무한 스크롤 형태의 개인화 추천 방식, 실시간 트렌드를 반영한 인기 상품 추천 방식, 그리고 다양한 시나리오를 풍부하게 노출하는 설명 가능한 추천 방식<strong> </strong>등으로 나눠 볼 수 있습니다. 이렇게 다양한 추천 방식들은 사용자들의 선호 요소와 행동 패턴을 더욱 정교하게 반영하여 더욱 개인화된 쇼핑 경험을 제공하는 역할을 합니다.</p><p>무신사에서는 최근 대규모 개편(무신사 2.0)을 통해 “<strong>시나리오 기반의 설명 가능한 추천 시스템”</strong>을 구축하여 이전 추천 시스템에서 해결하기 어려웠던 문제점들을 해결하고자 했습니다. 이를 위해 데이터사이언스 팀은 사용자 “만족도”와 “매출 증대”라는 두 마리 토끼를 잡기 위한 여정을 시작했습니다. 구현 설명에 들어가기에 앞서 기존 추천 구조와 영역에 관해서 말씀드리도록 하겠습니다.</p><p>기존 무신사의 추천 시스템에서는 4개의 고정 영역에 유저 데모그래픽 세그먼트 인기 상품을 추천하고 있었습니다. 반복적인 A/B 테스트를 통해 유저의 관심사를 포착한 후 적절한 상품들을 노출 시키기 위해 많은 노력을 기울였으나 노출 영역의 한계점과 적은 상품 속성 정보로 인하여 다수의 유저를 만족 시키기 어려웠습니다. 데모그래픽 정보로 만으로는 유저들의 개개인 취향과 선호 요소를 충분히 반영하지 못했고, 상품의 속성이 세세하지 못하여 매력적인 개인화 상품을 제시하기 어려웠습니다.</p><p>무신사 내 추천 시스템 관련 부서는 무신사 2.0 도입에 맞춰 위의 문제점들을 해결하고자 밑바닥부터 기획을 진행했습니다. 아래는 새로운 추천 시스템에 대한 목표를 간추린 주요 기획 내용입니다.</p><ul><li><strong>상호작용과 행동 패턴 기반의 추천 시스템</strong> — 비교적 단순한 유저의 데모그래픽 정보를 넘어 관심 상품에 대한 행동 패턴과 상호작용을 기반으로 60개 이상의 설명 가능한 추천 시나리오를 제공하는 것을 목표</li><li><strong>개인화 및 최적화된 시나리오를 노출</strong> — 개별화된 추천 시나리오의 세부 내용은 유저의 행동 로그를 기반으로 끊임없이 변화하며, 시나리오에 대한 노출 순서도 정렬 알고리즘을 통해 최적화</li><li><strong>콜드스타트 유저 대응</strong> — 트렌드 및 계절성 기반의 시나리오를 강화하여 처음 가입한 유저 및 장기간 활동이 없던 유저에게 상품 탐색의 흥미를 부여</li><li><strong>설명 가능한 추천 시나리오 적용</strong> — 추천 시나리오에 납득 가능한 타이틀을 부여하여 추천 결과에 대한 신뢰성 향상</li><li><strong>추천 시나리오 생성 및 수정</strong> — 추천 시나리오의 생성 및 수정 그리고 실험 체계를 모듈화하여 새로운 시나리오 대응 및 A/B 테스트의 용이성 향상</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*9YSxyYWzBW1AV50K\" /><figcaption>이미지1 무신사 1.0과 2.0 추천판 비교</figcaption></figure><p>무신사에서 기획한 설명 가능한 추천 시스템에서는 유저의 취향을 반영한 각기 다른 60여 개의 시나리오가 맞물려 거대한 취향 스토리를 구성하게 됩니다. 그만큼 시나리오 구성에 대한 중요도가 높았으며, 해당 내용이 유저의 취향과 밀접한 관련이 있어야 했습니다.</p><h4><strong>시나리오의 종류와 생성 방법</strong></h4><p>무신사 2.0에서는 홈(메인) 지면에 대한 크고 작은 UI/UX 변화가 있었습니다. 뷰티, 스포츠, 아울렛 등의 작은 관(Department) 단위로 구성 되어 있던 부분이 멀티 스토어 개념으로 확장 되었습니다.</p><p>[이미지 2] 처럼 무신사에서는 뷰티, 아울렛, 스니커, 플레이어, 부티크, 키즈 총 6개의 차별화된 스토어가 신설 되었고, 각 스토어의 특색에 맞는 상품 추천과 정보를 제공해야 했습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*8O6PKrsie8vJKzlz\" /><figcaption>이미지2 개편 된 무신사 추천판 UI/UX</figcaption></figure><p>새로 기획된 취지에 맞춰 유관 부서와 오랜 기간 고민을 하였고, 각기 다른 특징과 목적을 갖는 총 4개의 설명 가능한 시나리오 그룹이 탄생했습니다.</p><ul><li><strong>실시간 반응형 시나리오</strong> — 유저가 상품을 탐색 중인 경우, 탐색 중인 상품과 유사한 상품 &amp; 브랜드를 실시간으로 추천하기 위해 기획되었습니다. 상품 &amp; 브랜드 상세 페이지에 진입한 후, 다시 홈 화면으로 복귀한 경우 노출됩니다. 실시간 반응형 시나리오는 유저의 탐색 경험을 극대화하는 요소 중 하나이기 때문에 비교적 상위 영역에 노출됩니다. 이러한 이유로 여러 시나리오 중에서 높은 클릭률(CTR) 및 전환율(CVR)을 보장받을 수 있습니다.</li><li><strong>액션 기반 시나리오 — </strong>유저가 활동한 행동 패턴을 기반으로 생성됩니다. 당연하게도 앞선 실시간 반응형 시나리오보다는 장기간의 로그 데이터를 활용합니다. 단순한 상품 “클릭”과 같은 유저의 암묵적(implicit) 로그 외에도 “좋아요”, “구매”, “장바구니” 같은 명시적(explicit) 로그를 함께 이용합니다. 과거에 조회했던 유사한 상품을 비롯해 동일한 스타일 및 카테고리의 상품들을 다양하게 추천받을 수 있습니다.</li><li><strong>유저 선호도 기반 시나리오</strong> — 고도화된 추천 알고리즘으로 개별 유저의 선호도를 측정하여 상품, 브랜드, 스타일, 카테고리 등의 요소들의 선호 순위를 추출합니다. 선호도는 장기 취향(Long-term preference)과 단기 취향(Short-term preference)을 모두 반영하게 되며, 무신사 내 중요 요소(상품, 브랜드, 카테고리 등)를 임베딩하여 개인화된 순위(Top-k)를 만들게 됩니다.</li><li><strong>트렌드 기반 시나리오</strong> — 무신사를 비롯한 이커머스 플랫폼에서는 계절성을 띄는 인기 상품 및 카테고리를 관리하는 것이 중요합니다. 트렌드 기반 시나리오는 유저 본인에게 알맞은 트렌디한 인기 상품들을 만나 볼 수 있습니다. 예를 들어 “비슷한 연령대가 좋아할 만한 브랜드”, “비 오는 날 어울리는 데이트 룩” 등 데모그래픽, TPO(Time, Place, Occation)를 기반으로 후보군이 생성됩니다. 이외에도 신규 회원, 오랜만에 접속한(콜드스타트) 유저에 대한 대응책으로도 기획되었습니다.</li></ul><p>이러한 시나리오들은 개별적인 생성 프로세스와 특징들을 보유하고 있기 때문에 이를 잘 표현해 줄 수 있는 설명 가능한 타이틀이 필요합니다. 아래는 기획한 시나리오의 종류와 타이틀에 대한 예시 자료입니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*C62ZGjq7LPw3RExPt_f-7Q.jpeg\" /><figcaption>이미지3 설명 가능한 추천 시스템 시나리오 예시</figcaption></figure><h3><strong>시나리오 생성을 위한 AI/ML 기술</strong></h3><p>이제 본격적으로 시나리오 생성을 위한 기술적인 부분에 대해서 말씀드리도록 하겠습니다. 약 60여개의 시나리오에 대한 각기 다른 모델(알고리즘)을 생성하는 것은 성능 대비 리소스 낭비로 이어질 수 있습니다. 데이터사이언스 팀은 시나리오 생성에 있어서 가장 효율적인 방법을 탐색하였고, 아래 3가지 공통 모델들을 제작 후에 이를 최대한 활용하는 방향으로 개발을 진행했습니다. 코어(백본) 모델을 기반으로 하위 작업(Downstream task)에 응용하여 개발의 유연성과 효율성을 높이도록 했습니다.</p><ul><li><strong>코어(백본) 모델</strong> — 유저와 요소의 상호작용을 기반으로 학습되는 코어 모델. 중요 요소 임베딩 추출에 근본이 되며, 유저 선호 후보군 생성 및 유사 요소(상품) 계산에 활용</li><li><strong>요소(상품) 재정렬 모델</strong> — 각 시나리오 대한 요소(상품)의 최종 순서를 결정 (Ranker)</li><li><strong>시나리오 정렬 모델</strong> — 유저 개별 시나리오 노출 순서를 최적하여 정렬</li></ul><h4><strong>코어(백본) 모델</strong></h4><p>유저의 개별 선호도를 추출하고 예측하기 위해서 장기간 / 단기간 로그 데이터를 위한 모델을 각각 두 가지로 나누어 구성했습니다. 장기간 학습 모델은 유저와 요소(상품, 브랜드 등)의 장기간 상호작용 로그를 이용하게 됩니다. 시간에 따른 가중치를 적용한 Sequential 모델을 사용 하거나 시간 패턴(Time-aware)을 고려하지 않는 그래프 기반의 PinSage 알고리즘을 무신사에 맞게 학습하였습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*EGIT_ovHcHueTg8o\" /><figcaption>이미지4 유저 선호도 추출 결과 예시</figcaption></figure><p><strong>PinSage</strong>는 Pinterest에서 개발한 대규모 그래프 기반의 신경망 모델로, 수십억개의 노드와 엣지를 가지는 대규모 그래프에 효율적으로 대응하기 위해 개발되었습니다. 데이터사이언스 팀은 무신사 상품의 메타 데이터(브랜드, 카테고리, 스타일 등)를 노드 특성에 첨가하여 학습하였고, 이를 통해 고품질 요소(상품) 임베딩을 추출할 수 있었습니다. (<a href=\"https://www.dgl.ai/\">dgl library</a> 사용)</p><p><strong>그래프 구성 코드</strong></p><pre>graph_builder = PandasGraphBuilder()<br>graph_builder.add_entities(new_users, &#39;uid&#39;, &#39;user&#39;)<br>graph_builder.add_entities(new_goods, &#39;goods_no&#39;, &#39;goods&#39;)<br>graph_builder.add_binary_relations(new_ratings, &#39;uid&#39;, &#39;goods_no&#39;, &#39;rated&#39;)<br>graph_builder.add_binary_relations(new_ratings, &#39;goods_no&#39;, &#39;uid&#39;, &#39;rated-by&#39;)<br>g = graph_builder.build()<br>node_dict = {<br>&#39;user&#39;: [new_users, [&#39;uid&#39;,&#39;userfeature1&#39;,&#39;userfeature2&#39;,&#39;userfeature3&#39;], [&#39;cat&#39;, &#39;cat&#39;, &#39;cat&#39;, &#39;cat&#39;] ],<br>&#39;goods&#39;: [new_goods, [ &#39;goods_no&#39;,&#39;goodsfeature1&#39;,&#39;goodsfeature2&#39;,&#39;goodsfeature3&#39;, &#39;goodsfeature4&#39;], [&#39;cat&#39;, &#39;cat&#39;, &#39;cat&#39;,&#39;cat&#39;, &#39;cat&#39;]]<br>}<br>edge_dict = {<br>&#39;rated&#39;: [new_ratings, [&#39;score&#39;, &#39;timestamp&#39;]],<br>&#39;rated-by&#39;: [new_ratings, [&#39;score&#39;, &#39;timestamp&#39;]]<br>}</pre><p>단기간 로그 데이터를 위한 모델은 Bert4Rec을 활용했습니다. Bert4rec은 언어모델 Bert 기반의 추천 알고리즘으로 Sequential한 유저 행동을 양방향으로 분석하는 방식입니다. 국내 많은 기업이 해당 모델을 통해 개인화된 상품군을 추천하거나, 모델 내에서 발생한 부가적 정보를 가공하여 유저 임베딩을 생성하기도 합니다.</p><p>위의 2개의 모델을 결합하여 각 요소에 대한 임베딩을 추출하였고, 이를 아래 2개의 Downstream task에 응용하였습니다.</p><p><strong>Downstream task1. 유저 선호도 추출</strong></p><p>&lt;이미지 5&gt;는 유저와 브랜드 간의 유사도를 구하는 예시입니다. 유저 브랜드 선호도를 구하기 위해서 우선 유저와 브랜드들의 임베딩을 추출하게 됩니다. 유저 임베딩은 유저가 상호작용했던 히스토리 상품들의 임베딩 정보를 집계 후 평균 내어 계산합니다. 그리고 브랜드 임베딩은 브랜드를 대표 할수 있는 상품을 확정 하여 해당 상품 임베딩의 집계 정보를 이용합니다. 이를 통해 모든 유저와 모든 브랜드의 임베딩을 만든 후, 유저별로 가장 가까운 브랜드를 코사인 유사도를 통해 추려낼 수 있습니다. 이는 카테고리, 스타일 정보 등으로 확장할 수도 있습니다. 하지만, 상품의 특성과 카테고리가 달라지는 경우, 임베딩 평균 집계 정보가 훼손될 수 있습니다. 이를 해결하기 위해 상의, 하의, 뷰티 등의 대분류 카테고리로 나누어 다시 평균 집계하였습니다.</p><p>현재는 데이터 수집 기간, 부가 특성(Side Information) 도입 등 테스트가 진행되고 있습니다. 그리고 더 개선된 모델을 위해서 데이터셋 입력 단위를 상품 상호 작용뿐만 아니라 브랜드 및 카테고리 상호작용으로 확장하는 방안도 구상 중입니다. 이외에도 행동 이력이 부족하여 선호도 추출 대상에서 제외된 유저들도 명시적(고객의 직접 정보를 받음) 데이터 등을 활용함으로써 커버리지를 확대하고 있습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*V2FzMCD_Tizt9wCGPCOgZw.jpeg\" /><figcaption>이미지5 유저별 선호 브랜드 추출 예시</figcaption></figure><p><strong>Downstream task 2. 유사 요소(상품/브랜드/카테고리/스타일) 추출</strong></p><p>유사 요소도 결국 상품-상품, 브랜드-브랜드, 카테고리-카테고리의 임베딩에 대한 유사도를 구하는 방식으로 접근할 수 있습니다. 물론 상품 수가 많아 계산 복잡도가 높기 때문에 <a href=\"https://github.com/facebookresearch/faiss\">faiss</a>를 활용했습니다.</p><p><strong>요소(상품) 재정렬 모델</strong></p><p>유저 선호도 및 유사 요소에 대한 후보군은 앞서 설명해 드렸던 백본 모델을 통해 신속하게 생성한 후, 재정렬 모델을 한 번 더 거쳐 순위를 매기게 됩니다. DeepFM 기반의 CTR 예측 모델을 이용하여 유저가 개별로 추천할 만한 요소를 먼저 선별하게 됩니다. 이외에도 해당 모델에는 유저에게 반영되는 결과 품질을 최종적으로 필터하고 순서를 재조정하는 기능이 있습니다. 예를 들어, 동일한 상품이지만 색상이 달라 다른 상품으로 인식되는 경우, 사용자에게는 동일 상품이 반복적으로 노출될 수 있습니다. 이를 방지하기 위한 특성들이 재정렬 모델에 녹아들어 있기 때문에 최종적으로는 자연스러운 추천 결과를 만나볼 수 있습니다. 재정렬 모델에 대한 상세한 설명이 궁금하시다면, 과거에 작성했던 <a href=\"https://medium.com/musinsa-tech/%EB%AC%B4%EC%8B%A0%EC%82%AC%EA%B0%80-%EC%B9%B4%ED%85%8C%EA%B3%A0%EB%A6%AC%EC%88%8D-%EC%B6%94%EC%B2%9C%EC%9D%84-%ED%95%98%EB%8A%94-%EB%B0%A9%EB%B2%95-a45b219685ea\">블로그</a> 내용을 참고해 주세요.</p><h4><strong>시나리오 순서 최적화 모델</strong></h4><p>시나리오를 다양하게 생성하는 것도 중요하지만, “관심 있을 만한” 시나리오들을 최적화하여 보여주는 것 또한<strong> </strong>저희 팀의 큰 목표 중 하나 입니다. 시나리오의 종류와 시나리오를 구성하는 상품 후보 및 정렬이 개인화되어있기 때문에, 각 유저 마다 노출가능한 시나리오 안에서 가장 적합한 노출 위해 <strong>MAB </strong>기반의 최적화 방법을 적용 예정입니다.</p><p>이 방식을 통해 유저의 긍정적 반응이 나타난 시나리오를 상단에 우선 노출하거고 기존에 보지 못했거나 새로 생성된 시나리오를 적절하게 노출하여 선순환 구조를 만들어 낼 것으로 기대합니다.</p><h3><strong>설명 가능한 시나리오를 위한 무신사 추천 시스템 구조</strong></h3><p>총 4가지의 테마(실시간 반응, 액션, 선호도, 트렌드)로 구성된 수많은 시나리오를 효율적으로 생성 및 서빙하기 위해서 많은 고민과 논의를 했습니다. 시나리오 메타 정보 생성, 필터링 및 후처리, 가중치 부여 등의 추천 품질을 높이기 위한 작업을 시나리오마다 올바르게 적용해야 했습니다. 추가로, “오랜 기간 고착화된 추천 결과가 유저에게 노출되거나” “시나리오별로 제작된 설명들이 추천 결과와 매칭되지 않는 문제”, 그리고 “원치 않은 시나리오를 관리자 페이지에서 즉각 제거” 하는 등의 필수 기능들을 제공하기 위해서는 올바른 아키텍처와 검증 체계가 필요했습니다. [이미지6]는 무신사 2.0에서 개편된 설명 가능한 추천 시나리오를 위한 엔지니어링 아키텍처입니다. 이제 각 파트에 대한 부가 설명을 하겠습니다. (AI/ML파트에 대한 설명은 앞서 기재했기 때문에 제외하도록 하겠습니다)</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*x9vQps0EqJuH5R6ba8vCxQ.jpeg\" /><figcaption>이미지6 무신사 추천 시스템 2.0 아키텍처</figcaption></figure><h4><strong>로그 &amp; 메타 정보 수집 파트</strong></h4><p>유저의 상호 작용 로그 데이터는 추천 시스템에서 매우 중요한 역할을 하게됩니다. 개인의 취향과 선호도를 파악하기 위해서 필수로 수집 해야하는 정보이며, 추천 알고리즘의 근간이 되는 역할을 하게 됩니다. 무신사 데이터엔지니어링 팀에서는 유저 행동으로 발생하는 방대한 암묵적, 명시적 로그들을 수집 및 집계하고 있습니다. 프론트엔드단부터 시작된 유저 활동 로그들은 추천 시스템에 활용할 수 있도록 짧은 기간내에 무신사 카탈로그에 적재됩니다. 이외에도 실시간 반응형 추천 시나리오를 위해서 Spark Streaming을 사용하고 있습니다. 유저가 현재 탐색하고 있는 상품, 브랜드, 카테고리 정보 등이 실시간으로 데이터베이스(MongoDB)에 제공되며, 이를 추천에 즉각 적용하게 됩니다.</p><p>상품의 메타 정보는 추천의 품질을 높이는 중요 요소입니다. 기존에는 입점 업체가 등록한 색상, 상품명 등의 기본 정보만을 추천 시스템에 적용하였습니다. 추천 시스템의 다각화를 위해 “<a href=\"https://medium.com/musinsa-tech/%EC%86%8D%EC%84%B1%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-%EC%B6%94%EC%B2%9C-%EA%B3%A0%EB%8F%84%ED%99%94-063ac9881801\"><strong>속성을 활용한 추천 고도화</strong></a>” 처럼 이미지 기반 예측을 통한 상품을 스타일, 속성 정보를 응용 하고 있습니다. 이를 통해 추천 시스템에 설명 가능한 정보를 효율적으로 수집하며 기존보다 세분화 된 추천을 제공할 수 있습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*joNKdnM1k332UuQi6-vpVw.jpeg\" /><figcaption>이미지7 상품 메타 정보 예시</figcaption></figure><h4><strong>시나리오 생성 파트</strong></h4><p>대부분의 시나리오별 결과물은 [이미지8]처럼 여러 과정을 통해 정제됩니다. 시나리오 생성을 위해 상품, 카테고리, 브랜드, 스타일 등의 개인화된 선호도를 1차로 추출하게 됩니다. 이는 수 많은 추천 후보군 중에서 관련있는 결과만 빠르게 필터하는 과정입니다. 그 후 정렬모델을 거쳐 유저가 클릭할 만한 시나리오별 상품들을 재구성하게 됩니다. 아마 추천 시스템에 관심이 많으신 분들은 [이미지8]의 흐름도가 익숙하실 거라 생각됩니다. 상세한 설명을 위해서 “유저 선호 스타일의 시나리오” 에 대한 예시를 아래와 같이 작성하였습니다.</p><ul><li><strong>시나리오명 : “OOO님이 좋아할 만한 스타일의 상품모음”</strong></li><li>1단계: 유저 선호도 모델에서 개인화된 선호 스타일 추출</li><li>2단계: 해당 스타일의 상품 후보군을 생성</li><li>3단계: CTR예측 정렬 모델을 통해 상품의 랭킹을 재구성</li><li>4단계: 추천 결과의 카테고리, 브랜드 등의 메타 요소에 대한 다양성을 부여하기 위해 부스팅/디케잉 등의 후처리 적용</li><li>5단계: 최종으로 정렬된 상품들을 추천 구좌에 노출</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*7gHttg6NZRD67-S8B4pGtA.jpeg\" /><figcaption>이미지8 시나리오 노출 구성도</figcaption></figure><p>API 영역</p><p>개인화된 시나리오가 고착화 되어 노출되는 것을 방지하고, 효율적인 시나리오 서빙을 위해 API 영역은 크게 2단계로 구성했습니다.</p><ul><li>1단계 Gate API : 유저별 시나리오 리스트와 순서를 제공하는 API 컴포넌트</li><li>2단계 Scenario API : 시나리오별 상품 리스트를 제공하는 API 컴포넌트</li></ul><p>추천 결과가 노출되는 추천판(홈 화면)에 유저가 접근하면 Gate API는 개인화된 시나리오 리스트와 시나리오 순서 정보를 제공하게 됩니다. 물론 시나리오마다 순서를 위한 스코어가 존재하며, 시나리오 순서의 최적화를 염두에 두어 A/B 테스트가 가능하도록 개발하였습니다. Gate API에서 제공된 응답을 토대로 시나리오들에 대한 결과를 다시 요청하게 됩니다. 유저별로 60여 개의 시나리오에 대한 응답들을 모두 집계해야 하므로 짧은 지연시간 내에 서빙 하는 것 또한 풀어내야 할 문제였습니다. (이를 해결해 주신 엔지니어분들께 감사드립니다.)</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*O7LxDBZGdFZXGEt0DqBXqQ.jpeg\" /><figcaption>이미지9 A/B테스트 시나리오 버킷 구성 샘플</figcaption></figure><h3>성과 및 지표</h3><p>지난 8월 오픈 이후 무신사 2.0 추천판에서는<strong> PDP 전환율과 인당 클릭 수</strong>가 상승하여 각각 기존 대비 <strong>156%, 129%</strong>를 향상 되었습니다. 또한, <strong>노출된 상품과 브랜드 수가 증가</strong>하여 각각 기존 대비 <strong>148%, 157%</strong>를 향상 되었습니다. 이는 상품(<strong>+48%</strong>) 다양성 및 브랜드 다양성(<strong>+57%</strong>)이 증가한 결과를 만들어냈습니다.</p><p>무신사의 스토어별로 신규 시나리오들이 앞으로도 다양하게 추가될 예정입니다. 현재 진행하고 있는 시나리오 순서 최적화가 완료되면 더 긍정적인 지표 추이가 예상됩니다.</p><ul><li>PDP 전환율: 상품 클릭 수 / 방문 유저 수</li><li>인당 클릭수: 상품 클릭 수 / 클릭 유저 수</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*vFB3ocB7cC6soO2Ls8jP_A.png\" /><figcaption>이미지10 무신사 추천 시스템 지표1</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*IYhCRQhOeXRh9JAyCRBzSg.png\" /><figcaption>이미지11 무신사 추천 시스템 지표2</figcaption></figure><h3><strong>모니터링</strong></h3><p>성과 측정은 Databricks에 내장된 시각화 모듈과 스케줄링 기능을 이용해 지표를 모니터링하고 있습니다. 주요 모니터링 대상 및 지표는 아래와 같습니다.</p><ul><li>스토어별 전체(Total) 성과 지표</li><li>스토어 내 세부 시나리오별 지표</li></ul><p>최근 A/B 테스트 기반의 실험 환경 구성이 화두인 만큼 주요 지표 트래킹뿐만 아니라, 시나리오별 A/B 버킷 지표의 비교를 통해 추천판 최적화에 힘쓰고 있습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/913/0*kbFnwoO2jqgVQEHP\" /><figcaption>이미지12 추천 지표 모니터링 대시보드</figcaption></figure><h3>마치며</h3><p>오픈 후 얼마 되지 않은 시점이기 때문에 부족한 부분도 많고, 스토어 별로 시나리오 수의 편차가 큽니다. 무신사 데이터사이언스 팀으로서 아래와 같이 부족한 부분을 개선하고 고도화할 작업이 많이 남아 있습니다.</p><ul><li>상품 신규 속성 발굴 및 속성 모델 고도화</li><li>명시적 데이터 등을 활용한 유저 선호 모델 고도화</li><li>유저 활동 데이터 분석을 통한 신규 시나리오 발굴</li><li>A/B 테스트 환경 고도화를 통한 시나리오 순서 및 개별 시나리오 자동 최적화</li></ul><p>마지막으로, 팀의 성장과 업무에 방향의 대해 항상 고민이 많으시고, 원활한 진행을 위해 애쓰시는 이원지,김준호님, 기획 초기부터 함께 고생하신 시니어 같은 막내 김지윤님, 혹시나 하는 마음으로 요청드린 기능도 뚝딱 만들어주신 최두석님, 김건희님, 궂은일 마다 않으시고 팀의 바탕이 되어 주시는 남상준님, 상품의 정보를 풍부하고 재밌게 만들어 주신 서정민님, 앞으로 더 많은 작업을 해주실 조준형님, 황의린님, 전혜윤님, 아직 입사하신지 얼마 되지 않았지만 잘 적응 해 나가고 계신 윤성민님, 최재혁님 감사드립니다.</p><h3>MUSINSA CAREER</h3><p>함께할 동료를 찾습니다.</p><p>무신사 데이터사이언스 팀은 무신사 고객에게 최적화된 쇼핑 경험을 제공하기 위해 다양한 AI 모델을 기획하고 운영합니다. 이외에도 무신사 및 그룹사의 플랫폼 데이터를 기반으로 추천 및 검색 시스템 등의 다양한 데이터 프로덕트를 제공합니다. 전국민이 사용하는 1위 패션 플랫폼 무신사에서 기술로 비즈니스를 성장시키는 경험을 함께하고 싶으시다면 아래 채용 페이지를 통해 지원해 주세요!</p><p><em>🚀 </em><a href=\"https://corp.musinsa.com/ko/career/\"><em>팀 무신사 채용 페이지</em></a><em> (무신사/29CM 전체 포지션 확인이 가능해요)</em></p><p><em>🚀 </em><a href=\"https://kr.linkedin.com/company/musinsacom\"><em>팀 무신사 테크 소식을 받아보는 링크드인</em></a></p><p><em>🚀 </em><a href=\"https://medium.com/musinsa-tech\"><em>무신사 테크 블로그</em></a></p><p><em>🚀 </em><a href=\"https://medium.com/29cm\"><em>29CM 테크 블로그</em></a></p><p><em>🚀 </em><a href=\"https://www.youtube.com/@MUSINSATECH\"><em>무신사 테크 유튜브 채널</em></a></p><p><em>채용이 완료되면 공고가 닫힐 수 있으니 빠르게 지원해 주세요!</em></p><h3>References</h3><ul><li>https://medium.com/musinsa-tech/속성을-활용한-추천-고도화-063ac9881801</li><li><a href=\"https://medium.com/musinsa-tech/%EC%86%8D%EC%84%B1%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-%EC%B6%94%EC%B2%9C-%EA%B3%A0%EB%8F%84%ED%99%94-part-2-%EB%AC%B4%EC%8B%A0%EC%82%AC%EA%B0%80-%EA%B0%9C%EC%9D%B8%ED%99%94-%EC%B6%94%EC%B2%9C%EC%9D%84-%ED%95%98%EB%8A%94-%EB%B0%A9%EB%B2%95-71f7aeb1dc5d\">https://medium.com/musinsa-tech/속성을-활용한-추천-고도화-part-2-무신사가-개인화-추천을-하는-방법-71f7aeb1dc5d</a></li><li><a href=\"https://medium.com/musinsa-tech/%EB%AC%B4%EC%8B%A0%EC%82%AC%EA%B0%80-%EC%B9%B4%ED%85%8C%EA%B3%A0%EB%A6%AC%EC%88%8D-%EC%B6%94%EC%B2%9C%EC%9D%84-%ED%95%98%EB%8A%94-%EB%B0%A9%EB%B2%95-a45b219685ea\">https://medium.com/musinsa-tech/무신사가-카테고리숍-추천을-하는-방법-a45b219685ea</a></li><li><a href=\"https://arxiv.org/abs/1806.01973\">https://arxiv.org/abs/1806.01973</a></li><li><a href=\"https://arxiv.org/abs/1904.06690\">https://arxiv.org/abs/1904.06690</a></li><li><a href=\"https://arxiv.org/abs/1003.0146\">https://arxiv.org/abs/1003.0146</a></li><li><a href=\"https://medium.com/naver-place-dev/%EB%B9%84%EC%8A%B7%ED%95%9C-%EC%9D%8C%EC%8B%9D%EC%A0%90-%EC%B7%A8%ED%96%A5-%EC%9C%A0%EC%A0%80-%EC%B6%94%EC%B2%9C-%EB%AA%A8%EB%8D%B8-%EA%B0%9C%EB%B0%9C%EA%B8%B0-9b33bb77c94d\">https://medium.com/naver-place-dev/비슷한-음식점-취향-유저-추천-모델-개발기-9b33bb77c94d</a></li><li><a href=\"https://speakerdeck.com/kakao/sequential-recommendation-system-kakao-seobiseu-jeogyonggi?slide=33\">https://speakerdeck.com/kakao/sequential-recommendation-system-kakao-seobiseu-jeogyonggi?slide=33</a></li><li><a href=\"https://greeksharifa.github.io/machine_learning/2021/02/21/Pin-Sage/\">https://greeksharifa.github.io/machine_learning/2021/02/21/Pin-Sage/</a></li><li><a href=\"https://static.googleusercontent.com/media/research.google.com/ko//pubs/archive/45530.pdf\">https://static.googleusercontent.com/media/research.google.com/ko//pubs/archive/45530.pdf</a></li></ul><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=db7020b20b68\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/musinsa-tech/ai%EC%99%80-%ED%95%A8%EA%BB%98%ED%95%98%EB%8A%94-%ED%8C%A8%EC%85%98-%ED%81%90%EB%A0%88%EC%9D%B4%EC%85%98-%EB%AC%B4%EC%8B%A0%EC%82%AC-2-0-%EC%8B%9C%EB%82%98%EB%A6%AC%EC%98%A4-%EA%B8%B0%EB%B0%98-%EC%B6%94%EC%B2%9C-%EC%8B%9C%EC%8A%A4%ED%85%9C-%EA%B0%9C%EB%B0%9C-db7020b20b68\">AI와 함께하는 패션 큐레이션 — 무신사 2.0 시나리오 기반 추천 시스템 개발</a> was originally published in <a href=\"https://medium.com/musinsa-tech\">MUSINSA tech</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "content:encodedSnippet": "AI와 함께하는 패션 큐레이션 — 무신사 2.0 시나리오 기반 추천 시스템 개발\n안녕하세요. 무신사 데이터사이언스 팀에서 추천 시스템 관련 분석과 모델링 업무를 하고 있는 최영민, 이명휘 입니다.\n최근 무신사 2.0이라는 타이틀로 엔지니어링, 기획, 디자인 등 서비스 전반에서 많은 개편이 있었습니다. 저희 데이터사이언스 팀에서도 추천 시스템을 필두로 여러 개편작업을 진행하게 되었고, 이를 달성하기 위해 고민했던 부분을 공유하고자 합니다.\n무신사 2.0, 새로워진 추천영역: 패션 큐레이션의 진화\n무신사를 비롯한 이커머스의 추천 시스템은 기업이 추구하는 비즈니스 방향성과 밀접하게 연관되어 있으며, 각기 다른 사용자 경험과 정량적인 성과를 창출합니다. 이는 개인화된 상품만을 지속적으로 큐레이션 하는 무한 스크롤 형태의 개인화 추천 방식, 실시간 트렌드를 반영한 인기 상품 추천 방식, 그리고 다양한 시나리오를 풍부하게 노출하는 설명 가능한 추천 방식 등으로 나눠 볼 수 있습니다. 이렇게 다양한 추천 방식들은 사용자들의 선호 요소와 행동 패턴을 더욱 정교하게 반영하여 더욱 개인화된 쇼핑 경험을 제공하는 역할을 합니다.\n무신사에서는 최근 대규모 개편(무신사 2.0)을 통해 “시나리오 기반의 설명 가능한 추천 시스템”을 구축하여 이전 추천 시스템에서 해결하기 어려웠던 문제점들을 해결하고자 했습니다. 이를 위해 데이터사이언스 팀은 사용자 “만족도”와 “매출 증대”라는 두 마리 토끼를 잡기 위한 여정을 시작했습니다. 구현 설명에 들어가기에 앞서 기존 추천 구조와 영역에 관해서 말씀드리도록 하겠습니다.\n기존 무신사의 추천 시스템에서는 4개의 고정 영역에 유저 데모그래픽 세그먼트 인기 상품을 추천하고 있었습니다. 반복적인 A/B 테스트를 통해 유저의 관심사를 포착한 후 적절한 상품들을 노출 시키기 위해 많은 노력을 기울였으나 노출 영역의 한계점과 적은 상품 속성 정보로 인하여 다수의 유저를 만족 시키기 어려웠습니다. 데모그래픽 정보로 만으로는 유저들의 개개인 취향과 선호 요소를 충분히 반영하지 못했고, 상품의 속성이 세세하지 못하여 매력적인 개인화 상품을 제시하기 어려웠습니다.\n무신사 내 추천 시스템 관련 부서는 무신사 2.0 도입에 맞춰 위의 문제점들을 해결하고자 밑바닥부터 기획을 진행했습니다. 아래는 새로운 추천 시스템에 대한 목표를 간추린 주요 기획 내용입니다.\n\n상호작용과 행동 패턴 기반의 추천 시스템 — 비교적 단순한 유저의 데모그래픽 정보를 넘어 관심 상품에 대한 행동 패턴과 상호작용을 기반으로 60개 이상의 설명 가능한 추천 시나리오를 제공하는 것을 목표\n개인화 및 최적화된 시나리오를 노출 — 개별화된 추천 시나리오의 세부 내용은 유저의 행동 로그를 기반으로 끊임없이 변화하며, 시나리오에 대한 노출 순서도 정렬 알고리즘을 통해 최적화\n콜드스타트 유저 대응 — 트렌드 및 계절성 기반의 시나리오를 강화하여 처음 가입한 유저 및 장기간 활동이 없던 유저에게 상품 탐색의 흥미를 부여\n설명 가능한 추천 시나리오 적용 — 추천 시나리오에 납득 가능한 타이틀을 부여하여 추천 결과에 대한 신뢰성 향상\n추천 시나리오 생성 및 수정 — 추천 시나리오의 생성 및 수정 그리고 실험 체계를 모듈화하여 새로운 시나리오 대응 및 A/B 테스트의 용이성 향상\n이미지1 무신사 1.0과 2.0 추천판 비교\n무신사에서 기획한 설명 가능한 추천 시스템에서는 유저의 취향을 반영한 각기 다른 60여 개의 시나리오가 맞물려 거대한 취향 스토리를 구성하게 됩니다. 그만큼 시나리오 구성에 대한 중요도가 높았으며, 해당 내용이 유저의 취향과 밀접한 관련이 있어야 했습니다.\n시나리오의 종류와 생성 방법\n무신사 2.0에서는 홈(메인) 지면에 대한 크고 작은 UI/UX 변화가 있었습니다. 뷰티, 스포츠, 아울렛 등의 작은 관(Department) 단위로 구성 되어 있던 부분이 멀티 스토어 개념으로 확장 되었습니다.\n[이미지 2] 처럼 무신사에서는 뷰티, 아울렛, 스니커, 플레이어, 부티크, 키즈 총 6개의 차별화된 스토어가 신설 되었고, 각 스토어의 특색에 맞는 상품 추천과 정보를 제공해야 했습니다.\n이미지2 개편 된 무신사 추천판 UI/UX\n새로 기획된 취지에 맞춰 유관 부서와 오랜 기간 고민을 하였고, 각기 다른 특징과 목적을 갖는 총 4개의 설명 가능한 시나리오 그룹이 탄생했습니다.\n\n실시간 반응형 시나리오 — 유저가 상품을 탐색 중인 경우, 탐색 중인 상품과 유사한 상품 & 브랜드를 실시간으로 추천하기 위해 기획되었습니다. 상품 & 브랜드 상세 페이지에 진입한 후, 다시 홈 화면으로 복귀한 경우 노출됩니다. 실시간 반응형 시나리오는 유저의 탐색 경험을 극대화하는 요소 중 하나이기 때문에 비교적 상위 영역에 노출됩니다. 이러한 이유로 여러 시나리오 중에서 높은 클릭률(CTR) 및 전환율(CVR)을 보장받을 수 있습니다.\n액션 기반 시나리오 — 유저가 활동한 행동 패턴을 기반으로 생성됩니다. 당연하게도 앞선 실시간 반응형 시나리오보다는 장기간의 로그 데이터를 활용합니다. 단순한 상품 “클릭”과 같은 유저의 암묵적(implicit) 로그 외에도 “좋아요”, “구매”, “장바구니” 같은 명시적(explicit) 로그를 함께 이용합니다. 과거에 조회했던 유사한 상품을 비롯해 동일한 스타일 및 카테고리의 상품들을 다양하게 추천받을 수 있습니다.\n유저 선호도 기반 시나리오 — 고도화된 추천 알고리즘으로 개별 유저의 선호도를 측정하여 상품, 브랜드, 스타일, 카테고리 등의 요소들의 선호 순위를 추출합니다. 선호도는 장기 취향(Long-term preference)과 단기 취향(Short-term preference)을 모두 반영하게 되며, 무신사 내 중요 요소(상품, 브랜드, 카테고리 등)를 임베딩하여 개인화된 순위(Top-k)를 만들게 됩니다.\n트렌드 기반 시나리오 — 무신사를 비롯한 이커머스 플랫폼에서는 계절성을 띄는 인기 상품 및 카테고리를 관리하는 것이 중요합니다. 트렌드 기반 시나리오는 유저 본인에게 알맞은 트렌디한 인기 상품들을 만나 볼 수 있습니다. 예를 들어 “비슷한 연령대가 좋아할 만한 브랜드”, “비 오는 날 어울리는 데이트 룩” 등 데모그래픽, TPO(Time, Place, Occation)를 기반으로 후보군이 생성됩니다. 이외에도 신규 회원, 오랜만에 접속한(콜드스타트) 유저에 대한 대응책으로도 기획되었습니다.\n\n이러한 시나리오들은 개별적인 생성 프로세스와 특징들을 보유하고 있기 때문에 이를 잘 표현해 줄 수 있는 설명 가능한 타이틀이 필요합니다. 아래는 기획한 시나리오의 종류와 타이틀에 대한 예시 자료입니다.\n이미지3 설명 가능한 추천 시스템 시나리오 예시\n시나리오 생성을 위한 AI/ML 기술\n이제 본격적으로 시나리오 생성을 위한 기술적인 부분에 대해서 말씀드리도록 하겠습니다. 약 60여개의 시나리오에 대한 각기 다른 모델(알고리즘)을 생성하는 것은 성능 대비 리소스 낭비로 이어질 수 있습니다. 데이터사이언스 팀은 시나리오 생성에 있어서 가장 효율적인 방법을 탐색하였고, 아래 3가지 공통 모델들을 제작 후에 이를 최대한 활용하는 방향으로 개발을 진행했습니다. 코어(백본) 모델을 기반으로 하위 작업(Downstream task)에 응용하여 개발의 유연성과 효율성을 높이도록 했습니다.\n\n코어(백본) 모델 — 유저와 요소의 상호작용을 기반으로 학습되는 코어 모델. 중요 요소 임베딩 추출에 근본이 되며, 유저 선호 후보군 생성 및 유사 요소(상품) 계산에 활용\n요소(상품) 재정렬 모델 — 각 시나리오 대한 요소(상품)의 최종 순서를 결정 (Ranker)\n시나리오 정렬 모델 — 유저 개별 시나리오 노출 순서를 최적하여 정렬\n\n코어(백본) 모델\n유저의 개별 선호도를 추출하고 예측하기 위해서 장기간 / 단기간 로그 데이터를 위한 모델을 각각 두 가지로 나누어 구성했습니다. 장기간 학습 모델은 유저와 요소(상품, 브랜드 등)의 장기간 상호작용 로그를 이용하게 됩니다. 시간에 따른 가중치를 적용한 Sequential 모델을 사용 하거나 시간 패턴(Time-aware)을 고려하지 않는 그래프 기반의 PinSage 알고리즘을 무신사에 맞게 학습하였습니다.\n이미지4 유저 선호도 추출 결과 예시\nPinSage는 Pinterest에서 개발한 대규모 그래프 기반의 신경망 모델로, 수십억개의 노드와 엣지를 가지는 대규모 그래프에 효율적으로 대응하기 위해 개발되었습니다. 데이터사이언스 팀은 무신사 상품의 메타 데이터(브랜드, 카테고리, 스타일 등)를 노드 특성에 첨가하여 학습하였고, 이를 통해 고품질 요소(상품) 임베딩을 추출할 수 있었습니다. (dgl library 사용)\n그래프 구성 코드\ngraph_builder = PandasGraphBuilder()\ngraph_builder.add_entities(new_users, 'uid', 'user')\ngraph_builder.add_entities(new_goods, 'goods_no', 'goods')\ngraph_builder.add_binary_relations(new_ratings, 'uid', 'goods_no', 'rated')\ngraph_builder.add_binary_relations(new_ratings, 'goods_no', 'uid', 'rated-by')\ng = graph_builder.build()\nnode_dict = {\n'user': [new_users, ['uid','userfeature1','userfeature2','userfeature3'], ['cat', 'cat', 'cat', 'cat'] ],\n'goods': [new_goods, [ 'goods_no','goodsfeature1','goodsfeature2','goodsfeature3', 'goodsfeature4'], ['cat', 'cat', 'cat','cat', 'cat']]\n}edge_dict = {\n'rated': [new_ratings, ['score', 'timestamp']],\n'rated-by': [new_ratings, ['score', 'timestamp']]\n}\n단기간 로그 데이터를 위한 모델은 Bert4Rec을 활용했습니다. Bert4rec은 언어모델 Bert 기반의 추천 알고리즘으로 Sequential한 유저 행동을 양방향으로 분석하는 방식입니다. 국내 많은 기업이 해당 모델을 통해 개인화된 상품군을 추천하거나, 모델 내에서 발생한 부가적 정보를 가공하여 유저 임베딩을 생성하기도 합니다.\n위의 2개의 모델을 결합하여 각 요소에 대한 임베딩을 추출하였고, 이를 아래 2개의 Downstream task에 응용하였습니다.\nDownstream task1. 유저 선호도 추출\n<이미지 5>는 유저와 브랜드 간의 유사도를 구하는 예시입니다. 유저 브랜드 선호도를 구하기 위해서 우선 유저와 브랜드들의 임베딩을 추출하게 됩니다. 유저 임베딩은 유저가 상호작용했던 히스토리 상품들의 임베딩 정보를 집계 후 평균 내어 계산합니다. 그리고 브랜드 임베딩은 브랜드를 대표 할수 있는 상품을 확정 하여 해당 상품 임베딩의 집계 정보를 이용합니다. 이를 통해 모든 유저와 모든 브랜드의 임베딩을 만든 후, 유저별로 가장 가까운 브랜드를 코사인 유사도를 통해 추려낼 수 있습니다. 이는 카테고리, 스타일 정보 등으로 확장할 수도 있습니다. 하지만, 상품의 특성과 카테고리가 달라지는 경우, 임베딩 평균 집계 정보가 훼손될 수 있습니다. 이를 해결하기 위해 상의, 하의, 뷰티 등의 대분류 카테고리로 나누어 다시 평균 집계하였습니다.\n현재는 데이터 수집 기간, 부가 특성(Side Information) 도입 등 테스트가 진행되고 있습니다. 그리고 더 개선된 모델을 위해서 데이터셋 입력 단위를 상품 상호 작용뿐만 아니라 브랜드 및 카테고리 상호작용으로 확장하는 방안도 구상 중입니다. 이외에도 행동 이력이 부족하여 선호도 추출 대상에서 제외된 유저들도 명시적(고객의 직접 정보를 받음) 데이터 등을 활용함으로써 커버리지를 확대하고 있습니다.\n이미지5 유저별 선호 브랜드 추출 예시\nDownstream task 2. 유사 요소(상품/브랜드/카테고리/스타일) 추출\n유사 요소도 결국 상품-상품, 브랜드-브랜드, 카테고리-카테고리의 임베딩에 대한 유사도를 구하는 방식으로 접근할 수 있습니다. 물론 상품 수가 많아 계산 복잡도가 높기 때문에 faiss를 활용했습니다.\n요소(상품) 재정렬 모델\n유저 선호도 및 유사 요소에 대한 후보군은 앞서 설명해 드렸던 백본 모델을 통해 신속하게 생성한 후, 재정렬 모델을 한 번 더 거쳐 순위를 매기게 됩니다. DeepFM 기반의 CTR 예측 모델을 이용하여 유저가 개별로 추천할 만한 요소를 먼저 선별하게 됩니다. 이외에도 해당 모델에는 유저에게 반영되는 결과 품질을 최종적으로 필터하고 순서를 재조정하는 기능이 있습니다. 예를 들어, 동일한 상품이지만 색상이 달라 다른 상품으로 인식되는 경우, 사용자에게는 동일 상품이 반복적으로 노출될 수 있습니다. 이를 방지하기 위한 특성들이 재정렬 모델에 녹아들어 있기 때문에 최종적으로는 자연스러운 추천 결과를 만나볼 수 있습니다. 재정렬 모델에 대한 상세한 설명이 궁금하시다면, 과거에 작성했던 블로그 내용을 참고해 주세요.\n시나리오 순서 최적화 모델\n시나리오를 다양하게 생성하는 것도 중요하지만, “관심 있을 만한” 시나리오들을 최적화하여 보여주는 것 또한 저희 팀의 큰 목표 중 하나 입니다. 시나리오의 종류와 시나리오를 구성하는 상품 후보 및 정렬이 개인화되어있기 때문에, 각 유저 마다 노출가능한 시나리오 안에서 가장 적합한 노출 위해 MAB 기반의 최적화 방법을 적용 예정입니다.\n이 방식을 통해 유저의 긍정적 반응이 나타난 시나리오를 상단에 우선 노출하거고 기존에 보지 못했거나 새로 생성된 시나리오를 적절하게 노출하여 선순환 구조를 만들어 낼 것으로 기대합니다.\n설명 가능한 시나리오를 위한 무신사 추천 시스템 구조\n총 4가지의 테마(실시간 반응, 액션, 선호도, 트렌드)로 구성된 수많은 시나리오를 효율적으로 생성 및 서빙하기 위해서 많은 고민과 논의를 했습니다. 시나리오 메타 정보 생성, 필터링 및 후처리, 가중치 부여 등의 추천 품질을 높이기 위한 작업을 시나리오마다 올바르게 적용해야 했습니다. 추가로, “오랜 기간 고착화된 추천 결과가 유저에게 노출되거나” “시나리오별로 제작된 설명들이 추천 결과와 매칭되지 않는 문제”, 그리고 “원치 않은 시나리오를 관리자 페이지에서 즉각 제거” 하는 등의 필수 기능들을 제공하기 위해서는 올바른 아키텍처와 검증 체계가 필요했습니다. [이미지6]는 무신사 2.0에서 개편된 설명 가능한 추천 시나리오를 위한 엔지니어링 아키텍처입니다. 이제 각 파트에 대한 부가 설명을 하겠습니다. (AI/ML파트에 대한 설명은 앞서 기재했기 때문에 제외하도록 하겠습니다)\n이미지6 무신사 추천 시스템 2.0 아키텍처\n로그 & 메타 정보 수집 파트\n유저의 상호 작용 로그 데이터는 추천 시스템에서 매우 중요한 역할을 하게됩니다. 개인의 취향과 선호도를 파악하기 위해서 필수로 수집 해야하는 정보이며, 추천 알고리즘의 근간이 되는 역할을 하게 됩니다. 무신사 데이터엔지니어링 팀에서는 유저 행동으로 발생하는 방대한 암묵적, 명시적 로그들을 수집 및 집계하고 있습니다. 프론트엔드단부터 시작된 유저 활동 로그들은 추천 시스템에 활용할 수 있도록 짧은 기간내에 무신사 카탈로그에 적재됩니다. 이외에도 실시간 반응형 추천 시나리오를 위해서 Spark Streaming을 사용하고 있습니다. 유저가 현재 탐색하고 있는 상품, 브랜드, 카테고리 정보 등이 실시간으로 데이터베이스(MongoDB)에 제공되며, 이를 추천에 즉각 적용하게 됩니다.\n상품의 메타 정보는 추천의 품질을 높이는 중요 요소입니다. 기존에는 입점 업체가 등록한 색상, 상품명 등의 기본 정보만을 추천 시스템에 적용하였습니다. 추천 시스템의 다각화를 위해 “속성을 활용한 추천 고도화” 처럼 이미지 기반 예측을 통한 상품을 스타일, 속성 정보를 응용 하고 있습니다. 이를 통해 추천 시스템에 설명 가능한 정보를 효율적으로 수집하며 기존보다 세분화 된 추천을 제공할 수 있습니다.\n이미지7 상품 메타 정보 예시\n시나리오 생성 파트\n대부분의 시나리오별 결과물은 [이미지8]처럼 여러 과정을 통해 정제됩니다. 시나리오 생성을 위해 상품, 카테고리, 브랜드, 스타일 등의 개인화된 선호도를 1차로 추출하게 됩니다. 이는 수 많은 추천 후보군 중에서 관련있는 결과만 빠르게 필터하는 과정입니다. 그 후 정렬모델을 거쳐 유저가 클릭할 만한 시나리오별 상품들을 재구성하게 됩니다. 아마 추천 시스템에 관심이 많으신 분들은 [이미지8]의 흐름도가 익숙하실 거라 생각됩니다. 상세한 설명을 위해서 “유저 선호 스타일의 시나리오” 에 대한 예시를 아래와 같이 작성하였습니다.\n\n시나리오명 : “OOO님이 좋아할 만한 스타일의 상품모음”\n1단계: 유저 선호도 모델에서 개인화된 선호 스타일 추출\n2단계: 해당 스타일의 상품 후보군을 생성\n3단계: CTR예측 정렬 모델을 통해 상품의 랭킹을 재구성\n4단계: 추천 결과의 카테고리, 브랜드 등의 메타 요소에 대한 다양성을 부여하기 위해 부스팅/디케잉 등의 후처리 적용\n5단계: 최종으로 정렬된 상품들을 추천 구좌에 노출\n이미지8 시나리오 노출 구성도\nAPI 영역\n개인화된 시나리오가 고착화 되어 노출되는 것을 방지하고, 효율적인 시나리오 서빙을 위해 API 영역은 크게 2단계로 구성했습니다.\n\n1단계 Gate API : 유저별 시나리오 리스트와 순서를 제공하는 API 컴포넌트\n2단계 Scenario API : 시나리오별 상품 리스트를 제공하는 API 컴포넌트\n\n추천 결과가 노출되는 추천판(홈 화면)에 유저가 접근하면 Gate API는 개인화된 시나리오 리스트와 시나리오 순서 정보를 제공하게 됩니다. 물론 시나리오마다 순서를 위한 스코어가 존재하며, 시나리오 순서의 최적화를 염두에 두어 A/B 테스트가 가능하도록 개발하였습니다. Gate API에서 제공된 응답을 토대로 시나리오들에 대한 결과를 다시 요청하게 됩니다. 유저별로 60여 개의 시나리오에 대한 응답들을 모두 집계해야 하므로 짧은 지연시간 내에 서빙 하는 것 또한 풀어내야 할 문제였습니다. (이를 해결해 주신 엔지니어분들께 감사드립니다.)\n이미지9 A/B테스트 시나리오 버킷 구성 샘플\n성과 및 지표\n지난 8월 오픈 이후 무신사 2.0 추천판에서는 PDP 전환율과 인당 클릭 수가 상승하여 각각 기존 대비 156%, 129%를 향상 되었습니다. 또한, 노출된 상품과 브랜드 수가 증가하여 각각 기존 대비 148%, 157%를 향상 되었습니다. 이는 상품(+48%) 다양성 및 브랜드 다양성(+57%)이 증가한 결과를 만들어냈습니다.\n무신사의 스토어별로 신규 시나리오들이 앞으로도 다양하게 추가될 예정입니다. 현재 진행하고 있는 시나리오 순서 최적화가 완료되면 더 긍정적인 지표 추이가 예상됩니다.\n\nPDP 전환율: 상품 클릭 수 / 방문 유저 수\n인당 클릭수: 상품 클릭 수 / 클릭 유저 수\n이미지10 무신사 추천 시스템 지표1이미지11 무신사 추천 시스템 지표2\n모니터링\n성과 측정은 Databricks에 내장된 시각화 모듈과 스케줄링 기능을 이용해 지표를 모니터링하고 있습니다. 주요 모니터링 대상 및 지표는 아래와 같습니다.\n\n스토어별 전체(Total) 성과 지표\n스토어 내 세부 시나리오별 지표\n\n최근 A/B 테스트 기반의 실험 환경 구성이 화두인 만큼 주요 지표 트래킹뿐만 아니라, 시나리오별 A/B 버킷 지표의 비교를 통해 추천판 최적화에 힘쓰고 있습니다.\n이미지12 추천 지표 모니터링 대시보드\n마치며\n오픈 후 얼마 되지 않은 시점이기 때문에 부족한 부분도 많고, 스토어 별로 시나리오 수의 편차가 큽니다. 무신사 데이터사이언스 팀으로서 아래와 같이 부족한 부분을 개선하고 고도화할 작업이 많이 남아 있습니다.\n\n상품 신규 속성 발굴 및 속성 모델 고도화\n명시적 데이터 등을 활용한 유저 선호 모델 고도화\n유저 활동 데이터 분석을 통한 신규 시나리오 발굴\nA/B 테스트 환경 고도화를 통한 시나리오 순서 및 개별 시나리오 자동 최적화\n\n마지막으로, 팀의 성장과 업무에 방향의 대해 항상 고민이 많으시고, 원활한 진행을 위해 애쓰시는 이원지,김준호님, 기획 초기부터 함께 고생하신 시니어 같은 막내 김지윤님, 혹시나 하는 마음으로 요청드린 기능도 뚝딱 만들어주신 최두석님, 김건희님, 궂은일 마다 않으시고 팀의 바탕이 되어 주시는 남상준님, 상품의 정보를 풍부하고 재밌게 만들어 주신 서정민님, 앞으로 더 많은 작업을 해주실 조준형님, 황의린님, 전혜윤님, 아직 입사하신지 얼마 되지 않았지만 잘 적응 해 나가고 계신 윤성민님, 최재혁님 감사드립니다.\nMUSINSA CAREER\n함께할 동료를 찾습니다.\n무신사 데이터사이언스 팀은 무신사 고객에게 최적화된 쇼핑 경험을 제공하기 위해 다양한 AI 모델을 기획하고 운영합니다. 이외에도 무신사 및 그룹사의 플랫폼 데이터를 기반으로 추천 및 검색 시스템 등의 다양한 데이터 프로덕트를 제공합니다. 전국민이 사용하는 1위 패션 플랫폼 무신사에서 기술로 비즈니스를 성장시키는 경험을 함께하고 싶으시다면 아래 채용 페이지를 통해 지원해 주세요!\n🚀 팀 무신사 채용 페이지 (무신사/29CM 전체 포지션 확인이 가능해요)\n🚀 팀 무신사 테크 소식을 받아보는 링크드인\n🚀 무신사 테크 블로그\n🚀 29CM 테크 블로그\n🚀 무신사 테크 유튜브 채널\n채용이 완료되면 공고가 닫힐 수 있으니 빠르게 지원해 주세요!\nReferences\n\nhttps://medium.com/musinsa-tech/속성을-활용한-추천-고도화-063ac9881801\nhttps://medium.com/musinsa-tech/속성을-활용한-추천-고도화-part-2-무신사가-개인화-추천을-하는-방법-71f7aeb1dc5d\nhttps://medium.com/musinsa-tech/무신사가-카테고리숍-추천을-하는-방법-a45b219685ea\nhttps://arxiv.org/abs/1806.01973\nhttps://arxiv.org/abs/1904.06690\nhttps://arxiv.org/abs/1003.0146\nhttps://medium.com/naver-place-dev/비슷한-음식점-취향-유저-추천-모델-개발기-9b33bb77c94d\nhttps://speakerdeck.com/kakao/sequential-recommendation-system-kakao-seobiseu-jeogyonggi?slide=33\nhttps://greeksharifa.github.io/machine_learning/2021/02/21/Pin-Sage/\nhttps://static.googleusercontent.com/media/research.google.com/ko//pubs/archive/45530.pdf\n\nAI와 함께하는 패션 큐레이션 — 무신사 2.0 시나리오 기반 추천 시스템 개발 was originally published in MUSINSA tech on Medium, where people are continuing the conversation by highlighting and responding to this story.",
    "dc:creator": "최영민",
    "guid": "https://medium.com/p/db7020b20b68",
    "categories": [
      "ai",
      "recommendations",
      "sequential-model",
      "graph-model",
      "fashion"
    ],
    "isoDate": "2024-10-11T04:43:32.000Z"
  },
  {
    "creator": "Soyoun Kim",
    "title": "무진장을 맞아, 후기 응답속도를 개선해보자",
    "link": "https://medium.com/musinsa-tech/%EB%AC%B4%EC%A7%84%EC%9E%A5%EC%9D%84-%EB%A7%9E%EC%95%84-%ED%9B%84%EA%B8%B0-%EC%9D%91%EB%8B%B5%EC%86%8D%EB%8F%84%EB%A5%BC-%EA%B0%9C%EC%84%A0%ED%95%B4%EB%B3%B4%EC%9E%90-c92e0ae60f1e?source=rss----f107b03c406e---4",
    "pubDate": "Wed, 25 Sep 2024 04:41:52 GMT",
    "content:encoded": "<p>안녕하세요. 무신사 구매개발실 회원개발팀에서 백엔드 엔지니어로 일하고 있는 김소연 입니다.</p><p>무신사에서 회원 및 메시지, 후기까지 다양한 도메인을 경험하고 있습니다.</p><p>이 글에서는 후기 도메인을 담당하면서 경험한 성능 개선 사례를 공유합니다.</p><h3>장애 없는 무진장 만들기</h3><h3>무진장</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*LFcA48Du7ml8peaUBH4NoQ.png\" /><figcaption>2024 무진장 여름 블랙프라이데이</figcaption></figure><p>무신사에서 가장 큰 이벤트를 꼽으라면 단연 무진장일 것입니다. <br>다양한 프로모션으로 많은 고객들이 이 시기에 무신사를 찾아주고 있습니다.</p><p>지난 <strong>‘</strong>2024 무진장 여름 블랙프라이데이(이하 무진장 여름 블프)’는 2024년 6월 23일 오후 7시부터 7월 3일 자정까지 590만개의 상품을 판매했고 총 누적 판매액 2000억원을 기록했습니다.</p><p>이렇게 많은 상품이 판매되는 만큼 트래픽도 굉장히 많이 늘어납니다. <br>평소보다 트래픽이 배로 늘어나는 상황에 고객 불편을 최소화하고 성공적인 무진장 여름 블프 진행을 위해서, 사전에 미리 서비스 안정성을 꼼꼼히 확인하고 있습니다.</p><p>후기 도메인의 서비스도 마찬가지로 안정성 검토를 진행했습니다.</p><h3>서비스 규모 예상</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/300/1*7vQZ0uNmJBgn5G4JAh9Hxg.png\" /><figcaption>상품상세 페이지 내 후기 노출 지면</figcaption></figure><p>후기 서비스는 EKS로 서비스 되고 있습니다. <br>트래픽이 줄어들고 늘어남에 따라 pods의 갯수는 auto-scaling 됩니다.</p><p>이런 환경에서, 무진장 여름 블프의 특가 시간에 고객들이 편하게 리뷰 확인을 할 수 있으려면 pods가 몇 개나 있어야 할까요?</p><blockquote><em>트래픽 목표 기준 = 평소 피크 트래픽 x 3</em></blockquote><p>피크 타임에 제일 많이 증설되었을 때의, 후기 pods는 30대까지 scale-out 되었으며, 단순하게 산술적으로 계산했을 때 90대의 pods가 필요해 보입니다. <br>후기 서비스를 구성하고 있는 DB, Elasticsearch, 타 연관 도메인 API Server등을 고려하면, 단순히 + 60개의 pods 그 이상의 비용이 필요해 보였습니다.</p><p>특히 요청이 늘어날 때 Elasticsearch에서 후기 데이터를 가져 오고 있는 상황에서 ES 부하가 발생하면 ES 노드의 증설도 필요해 지는데, 이 부분에서 비용이 큰 상황이었습니다.</p><p>특가 시간대 안정적인 운영을 위해서는 개선이 꼭 필요해 보였습니다.</p><h3>개선 과정</h3><h3>서비스 분석</h3><p>개선을 위해서 후기 서비스를 분석해봅시다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/956/1*1QESHBHA3lLwu-k-X07BHQ.png\" /><figcaption>한달 이내 가장 트래픽이 높았던 한시간 기준(5월 26일 22시 30분~23시 30분) 기준, Datadog APM &gt; 관련 지표 호출 통계</figcaption></figure><p>후기 서비스에서 가장 요청이 많은 API는 아래와 같았습니다.</p><ol><li>전체 후기 목록 페이지</li><li>만족도 관련 페이지</li><li>후기 좋아요 갯수 API</li></ol><p>위 API들은 무신사에서 가장 요청이 많은 상품 상세 페이지에서 발생하는 호출로, 상품상세의 요청량과 비례하여 증가합니다.</p><p>이중 1번 전체 후기 목록 페이지의 평균 응답지연시간이 250ms으로, 타 요청 대비 압도적으로 긴 상황이었습니다.</p><ol><li><strong><em>전체 후기 목록 페이지 300ms</em></strong></li><li>만족도 관련 페이지 16ms</li><li>후기 좋아요 갯수 API 5ms</li></ol><h3>후기 목록만 빨리 나오면 되겠는걸?</h3><p>후기 목록 API가 상대적으로 느린데는 슬픈 사연이 있습니다.</p><ul><li>SSR</li><li>많은 처리 내용</li></ul><p>처리 내용을 조금 더 자세히 살펴보겠습니다.</p><pre>- 상품 정보 조회<br>- 동일/연관 상품 정보 조회<br>- 검색 필터 정보 생성<br>- 상품 옵션 조회<br>- 상품 기준, 작성 가능한 후기 타입 조회<br>- 키워드 검색 필터 생성<br>- 나의 필터 조건 생성<br>    - 마이사이즈 조회<br>    - 마이사이즈 기준 필터 생성<br>- 상품 후기 데이터 조회<br>    - 후기 조회<br>        - 후기 작성 유저 정보 조회<br>        - 후기 이미지 조회<br>        - 댓글 조회<br>        - 댓글 갯수 조회<br>        - 데이터 후처리<br>            - 키워드 검색 필터를 위한 키워드 하이라이트 설정<br>            - …<br>    - “후기 조회” 에서 조회된 후기 목록 중 API 요청 고객이 숨김/차단한 회원을 조회하여 후기 필터링<br>    - 내 댓글 여부 표시<br>- 상품의 전체 후기 사진 조회<br>    - 동일/연관 상품 정보 조회<br>    - 동일/연관 상품 사진 조회<br>    - 조회된 사진 목록 중 API 요청 고객이 숨김/차단한 회원을 조회하여 후기 필터링<br>- 연관 상품 목록의 품절 상품에 품절 표시<br>(…더보기)</pre><p>호출 위치도 파악해 봅니다.</p><p>상품상세 페이지에서 상품 일련번호를 기준으로 호출되고 있었습니다.</p><blockquote><em>/api/goods/v3/review-html?상품번호=4146845&amp;유사상품번호=0&amp;선택유사상품번호=4146845&amp;정렬=new</em></blockquote><p>nginx access log로 유입되는 url 패턴을 확인해본 결과, 상품번호, 선택유사상품번호가 상품번호로 동일하게 들어오고 있었으며, 다른 query parameter는 의미 없는 것으로 파악 되었습니다.</p><p>이를 통해, 상품 단위의 캐싱 전략을 사용해 볼 수 있겠다는 생각이 들었습니다.</p><h3>개선 방향</h3><p>개선방향은 크게 아래 세가지에 집중했습니다.</p><ol><li>상품 단위의 캐싱 전략 사용을 위해, 개인화 호출 부분과 상품 정보 조회 및 후기 목록 조회 부분을 식별하여 분리</li><li>중복 로직 개선</li><li>불필요 로직 제거</li></ol><p>개선 후의 처리 내용을 한번 봅시다.</p><pre>- 후기 데이터 조회<br>    - 상품 정보 조회<br>    - 동일/연관 상품 정보 조회<br>    - 검색 필터 정보 생성<br>    - 상품 옵션 조회<br>    - 상품 기준, 작성 가능한 후기 타입 조회<br>    - 상품 후기 데이터 조회<br>        - 후기 조회<br>            - 후기 작성 유저 정보 조회<br>            - 후기 이미지 조회<br>            - 댓글 갯수 조회<br>    - 상품의 전체 후기 사진 조회<br>        - 동일/연관 상품 사진 조회<br>    - 연관 상품 목록의 품절 상품에 품절 표시<br>- 개인화 정보 조회 및 처리<br>    - 나의 필터 조건 생성<br>        - 마이사이즈 조회<br>        - 마이사이즈 기준 필터 생성<br>- API 요청 고객이 숨김/차단한 회원을 조회하여 후기 필터링<br>    - 사진 목록 필터링<br>    - 후기 목록 필터링</pre><p>여전히 할 일은 많지만 후기 데이터 조회를 상품번호 키를 기준으로 캐싱 처리를 할 수 있게 되었고, 캐시 처리 된 상태에서는 아래와 같이 줄일 수 있습니다.</p><pre>- 후기 데이터 조회 (cache hit)<br>- 개인화 정보 조회 및 처리<br>    - 나의 필터 조건 생성<br>        - 마이사이즈 조회<br>        - 마이사이즈 기준 필터 생성<br>- API 요청 고객이 숨김/차단한 회원을 조회하여 후기 필터링<br>    - 사진 목록 필터링<br>    - 후기 목록 필터링</pre><h3>30개의 pods</h3><p>개선 작업 후, k6 부하 테스트를 진행했습니다.</p><p>위에서 언급한 요청량이 제일 많은 top3 API를 대상으로, 테스트를 진행했습니다.</p><p>테스트 결과, pods 1대당 370 req/s 의 요청 수행이 가능했으며, 평균 응답지연속도는 11ms로 측정 되었습니다.</p><p>이를 바탕으로 목표 요청량을 수용하기 위해서는 pods가 15대 필요했으며, autoscaling 을 위해 설정한 KEDA의 CPU 60% 임계 증설 설정, 제한적인 테스트 내용을 감안한 여유있는 증설을 고려하면, 30개의 pods로 목표 트래픽 수용이 가능할 것으로 판단했습니다.</p><p>개선 배포 후 pods 갯수 모니터링 결과, 개선 전 23대 수준에서 운영되던 것이 개선 후 14로 감소하였습니다.</p><p>무진장 여름 블프에도 상시 14대 수준에서 운영이 가능했으며, 전체를 통틀어 최대 피크타임인 마지막날 오후 9시 부터 자정까지 30대 고정 증설하여 운영하였고, 30대를 초과하지 않고 cpu 사용수준을 증설 기준 미만으로 유지하며 안정적인 서버 상황 유지가 가능했습니다.</p><h3>개선된 전체 후기 목록 페이지</h3><p>배포 후 개선된 API의 평균 응답 속도를 확인해본 결과, (피크 요청량 기준) 아래와 같은 개선 수준을 확인할 수 있었으며,</p><ul><li><strong><em>평균응답지연속도는 기존 300ms &gt; 100ms로 1/3 으로 감소.</em></strong></li><li><strong><em>p50 250ms &gt; 60ms로 감소</em></strong></li></ul><p>특히 캐싱 구조상, 요청이 많으면 많을 수록 응답지연시간이 줄어드는 구조로, 특가등 요청이 많을 때 더 효과적인 성능을 보여줄 수 있었습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/723/1*j4Lg0864wmA2pnRTJOk61w.png\" /><figcaption>개선 전/후 지표 비교</figcaption></figure><h3>좋은 영향력</h3><p>후기 데이터 조회에서 호출하고 있는 여러가지 API들이 있었습니다.</p><p>예를 들면, 상품 정보 조회 API, DB, Elasticsearch, Redis 정도가 있을 것 같네요.</p><p>이 부분이 캐싱 처리 되면서 자연스럽게 호출이 줄어들었고, 지난 무진장에서는 scale-up을 고려했어야 했던 리소스들의 스펙을 유지하면서도 안정적인 서비스가 가능했습니다. 비용을 줄이는 효과를 얻기도 했습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/957/1*Sa7xVvg1NClG4GUR5OY6UQ.png\" /><figcaption>개선 전/후 지표 비교</figcaption></figure><h3>나는 아직 배고프다</h3><p>무진장 직전, 일주일 정도의 짧은 시간을 들여 개선된 내용이라, 코드 품질과 유지보수성을 챙기지는 못했습니다. <br>다행히도, 무신사 2.0 개편을 앞두고 있던 시점이라 무진장 여름 블프만을 위한 개선이었던 터라 단기적인 개선에 집중한 결과입니다.</p><p>캐시를 이용하여 응답속도를 개선하긴 했으나 여전히 비즈니스 로직 자체에 비효율성이 존재한다는 느낌이 들었습니다. <br>위 개선에서는 캐시 만료 시간을 고정적으로 하여 개선이 진행이 되었는데요. 사실 시간이 좀 더 여유가 있었다면, 캐시 데이터 셋과 캐시 만료를 고정 시간이 아닌, 적절한 만료 정책을 정해서 캐시를 좀 더 효율적으로 사용할 수 있었을텐데 하는 아쉬움이 남았습니다.</p><h3>맺음말</h3><p>개인적으로는, 무진장이라는 회사에서 가장 중요한 이벤트의 원활한 진행에 기여하여 매우 보람 있는 경험이었습니다. <br>무진장 기간에는 모든 백엔드 개발자들이 본인이 담당한 서비스가 안정적으로 운영되고 있는지 예의주시하는 기간인데요. 저희 팀 모두가 장애 걱정을 덜고, 무진장 기간동안 긴장도를 줄여서 지낼 수 있게 되었습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/958/1*XEqkj6H4FydLITJzhM1HXg.png\" /><figcaption>많은 트래픽도 여유롭게 넘길 수 있게 된 평화로운 회원개발팀</figcaption></figure><p>개선에서 제일 중요한 로직은 캐시 적용이었습니다. 당시 후기의 상황에서 적용해서 좋은 효과를 볼 수 있었는데요. 다만, 캐시 적용이 모든 걸 해결해주지는 않습니다. <br>캐시는 빠른 만큼 비쌉니다. 빠른 성능을 유지하기 위해서는 꼭 필요한 곳에 잘 사용해야 합니다.</p><p>마지막으로, 서비스 개선을 위해서는 내가 개발/운영하는 서비스에 지속적인 관심이 필요합니다. <br>서비스의 개선은, 어느 날 갑자기 ‘개선 좀 해볼까?’ 마음먹는다고 해서 실행할 수 있는 일이 아닙니다. 계속해서 코드와 APM등의 모니터링 을 들여다보면서, 어떤 곳에서 비효율이 발생하는지 알아야 개선도 경험할 수 있습니다.</p><p>자기의 성과는 자기가 스스로 만듭니다.</p><h3>MUSINSA CAREER</h3><blockquote><strong><em>함께할 동료를 찾습니다.</em></strong></blockquote><blockquote>무신사 회원개발팀은 1500만 명의 회원과 MAU 500만 명을 보유한 플랫폼의 핵심 서비스를 안정적으로 운영하고 있습니다. <br>회원 관리, 멤버십, 인증, 메시지, 적립금 등의 서비스를 책임지며, 제품의 모든 요구사항을 신속하고 효율적으로 해결하여 안정성과 확장성을 보장합니다.</blockquote><blockquote><em>전국민이 사용하는 1위 패션 플랫폼 무신사에서 기술로 비즈니스를 성장시키는 경험을 함께하고 싶으시다면 아래 채용 페이지를 통해 지원해 주세요!</em></blockquote><blockquote><em>🚀 </em><a href=\"https://corp.musinsa.com/ko/career/\"><em>팀 무신사 채용 페이지</em></a><em> (무신사/29CM 전체 포지션 확인이 가능해요)</em></blockquote><blockquote><em>🚀 </em><a href=\"https://kr.linkedin.com/company/musinsacom\"><em>팀 무신사 테크 소식을 받아보는 링크드인</em></a></blockquote><blockquote><em>🚀 </em><a href=\"https://medium.com/musinsa-tech\"><em>무신사 테크 블로그</em></a></blockquote><blockquote><em>🚀 </em><a href=\"https://medium.com/29cm\"><em>29CM 테크 블로그</em></a></blockquote><blockquote><em>🚀 </em><a href=\"https://www.youtube.com/@MUSINSATECH\"><em>무신사 테크 유튜브 채널</em></a></blockquote><blockquote><em>채용이 완료되면 공고가 닫힐 수 있으니 빠르게 지원해 주세요!</em></blockquote><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c92e0ae60f1e\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/musinsa-tech/%EB%AC%B4%EC%A7%84%EC%9E%A5%EC%9D%84-%EB%A7%9E%EC%95%84-%ED%9B%84%EA%B8%B0-%EC%9D%91%EB%8B%B5%EC%86%8D%EB%8F%84%EB%A5%BC-%EA%B0%9C%EC%84%A0%ED%95%B4%EB%B3%B4%EC%9E%90-c92e0ae60f1e\">무진장을 맞아, 후기 응답속도를 개선해보자</a> was originally published in <a href=\"https://medium.com/musinsa-tech\">MUSINSA tech</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "content:encodedSnippet": "안녕하세요. 무신사 구매개발실 회원개발팀에서 백엔드 엔지니어로 일하고 있는 김소연 입니다.\n무신사에서 회원 및 메시지, 후기까지 다양한 도메인을 경험하고 있습니다.\n이 글에서는 후기 도메인을 담당하면서 경험한 성능 개선 사례를 공유합니다.\n장애 없는 무진장 만들기\n무진장\n2024 무진장 여름 블랙프라이데이\n무신사에서 가장 큰 이벤트를 꼽으라면 단연 무진장일 것입니다. \n다양한 프로모션으로 많은 고객들이 이 시기에 무신사를 찾아주고 있습니다.\n지난 ‘2024 무진장 여름 블랙프라이데이(이하 무진장 여름 블프)’는 2024년 6월 23일 오후 7시부터 7월 3일 자정까지 590만개의 상품을 판매했고 총 누적 판매액 2000억원을 기록했습니다.\n이렇게 많은 상품이 판매되는 만큼 트래픽도 굉장히 많이 늘어납니다. \n평소보다 트래픽이 배로 늘어나는 상황에 고객 불편을 최소화하고 성공적인 무진장 여름 블프 진행을 위해서, 사전에 미리 서비스 안정성을 꼼꼼히 확인하고 있습니다.\n후기 도메인의 서비스도 마찬가지로 안정성 검토를 진행했습니다.\n서비스 규모 예상\n상품상세 페이지 내 후기 노출 지면\n후기 서비스는 EKS로 서비스 되고 있습니다. \n트래픽이 줄어들고 늘어남에 따라 pods의 갯수는 auto-scaling 됩니다.\n이런 환경에서, 무진장 여름 블프의 특가 시간에 고객들이 편하게 리뷰 확인을 할 수 있으려면 pods가 몇 개나 있어야 할까요?\n트래픽 목표 기준 = 평소 피크 트래픽 x 3\n피크 타임에 제일 많이 증설되었을 때의, 후기 pods는 30대까지 scale-out 되었으며, 단순하게 산술적으로 계산했을 때 90대의 pods가 필요해 보입니다. \n후기 서비스를 구성하고 있는 DB, Elasticsearch, 타 연관 도메인 API Server등을 고려하면, 단순히 + 60개의 pods 그 이상의 비용이 필요해 보였습니다.\n특히 요청이 늘어날 때 Elasticsearch에서 후기 데이터를 가져 오고 있는 상황에서 ES 부하가 발생하면 ES 노드의 증설도 필요해 지는데, 이 부분에서 비용이 큰 상황이었습니다.\n특가 시간대 안정적인 운영을 위해서는 개선이 꼭 필요해 보였습니다.\n개선 과정\n서비스 분석\n개선을 위해서 후기 서비스를 분석해봅시다.\n한달 이내 가장 트래픽이 높았던 한시간 기준(5월 26일 22시 30분~23시 30분) 기준, Datadog APM > 관련 지표 호출 통계\n후기 서비스에서 가장 요청이 많은 API는 아래와 같았습니다.\n\n전체 후기 목록 페이지\n만족도 관련 페이지\n후기 좋아요 갯수 API\n\n위 API들은 무신사에서 가장 요청이 많은 상품 상세 페이지에서 발생하는 호출로, 상품상세의 요청량과 비례하여 증가합니다.\n이중 1번 전체 후기 목록 페이지의 평균 응답지연시간이 250ms으로, 타 요청 대비 압도적으로 긴 상황이었습니다.\n\n전체 후기 목록 페이지 300ms\n만족도 관련 페이지 16ms\n후기 좋아요 갯수 API 5ms\n\n후기 목록만 빨리 나오면 되겠는걸?\n후기 목록 API가 상대적으로 느린데는 슬픈 사연이 있습니다.\n\nSSR\n많은 처리 내용\n\n처리 내용을 조금 더 자세히 살펴보겠습니다.\n- 상품 정보 조회\n- 동일/연관 상품 정보 조회\n- 검색 필터 정보 생성\n- 상품 옵션 조회\n- 상품 기준, 작성 가능한 후기 타입 조회\n- 키워드 검색 필터 생성\n- 나의 필터 조건 생성\n    - 마이사이즈 조회\n    - 마이사이즈 기준 필터 생성\n- 상품 후기 데이터 조회\n    - 후기 조회\n        - 후기 작성 유저 정보 조회\n        - 후기 이미지 조회\n        - 댓글 조회\n        - 댓글 갯수 조회\n        - 데이터 후처리\n            - 키워드 검색 필터를 위한 키워드 하이라이트 설정\n            - …\n    - “후기 조회” 에서 조회된 후기 목록 중 API 요청 고객이 숨김/차단한 회원을 조회하여 후기 필터링\n    - 내 댓글 여부 표시\n- 상품의 전체 후기 사진 조회\n    - 동일/연관 상품 정보 조회\n    - 동일/연관 상품 사진 조회\n    - 조회된 사진 목록 중 API 요청 고객이 숨김/차단한 회원을 조회하여 후기 필터링\n- 연관 상품 목록의 품절 상품에 품절 표시\n(…더보기)\n호출 위치도 파악해 봅니다.\n상품상세 페이지에서 상품 일련번호를 기준으로 호출되고 있었습니다.\n/api/goods/v3/review-html?상품번호=4146845&유사상품번호=0&선택유사상품번호=4146845&정렬=new\nnginx access log로 유입되는 url 패턴을 확인해본 결과, 상품번호, 선택유사상품번호가 상품번호로 동일하게 들어오고 있었으며, 다른 query parameter는 의미 없는 것으로 파악 되었습니다.\n이를 통해, 상품 단위의 캐싱 전략을 사용해 볼 수 있겠다는 생각이 들었습니다.\n개선 방향\n개선방향은 크게 아래 세가지에 집중했습니다.\n\n상품 단위의 캐싱 전략 사용을 위해, 개인화 호출 부분과 상품 정보 조회 및 후기 목록 조회 부분을 식별하여 분리\n중복 로직 개선\n불필요 로직 제거\n\n개선 후의 처리 내용을 한번 봅시다.\n- 후기 데이터 조회\n    - 상품 정보 조회\n    - 동일/연관 상품 정보 조회\n    - 검색 필터 정보 생성\n    - 상품 옵션 조회\n    - 상품 기준, 작성 가능한 후기 타입 조회\n    - 상품 후기 데이터 조회\n        - 후기 조회\n            - 후기 작성 유저 정보 조회\n            - 후기 이미지 조회\n            - 댓글 갯수 조회\n    - 상품의 전체 후기 사진 조회\n        - 동일/연관 상품 사진 조회\n    - 연관 상품 목록의 품절 상품에 품절 표시\n- 개인화 정보 조회 및 처리\n    - 나의 필터 조건 생성\n        - 마이사이즈 조회\n        - 마이사이즈 기준 필터 생성\n- API 요청 고객이 숨김/차단한 회원을 조회하여 후기 필터링\n    - 사진 목록 필터링\n    - 후기 목록 필터링\n여전히 할 일은 많지만 후기 데이터 조회를 상품번호 키를 기준으로 캐싱 처리를 할 수 있게 되었고, 캐시 처리 된 상태에서는 아래와 같이 줄일 수 있습니다.\n- 후기 데이터 조회 (cache hit)\n- 개인화 정보 조회 및 처리\n    - 나의 필터 조건 생성\n        - 마이사이즈 조회\n        - 마이사이즈 기준 필터 생성\n- API 요청 고객이 숨김/차단한 회원을 조회하여 후기 필터링\n    - 사진 목록 필터링\n    - 후기 목록 필터링\n30개의 pods\n개선 작업 후, k6 부하 테스트를 진행했습니다.\n위에서 언급한 요청량이 제일 많은 top3 API를 대상으로, 테스트를 진행했습니다.\n테스트 결과, pods 1대당 370 req/s 의 요청 수행이 가능했으며, 평균 응답지연속도는 11ms로 측정 되었습니다.\n이를 바탕으로 목표 요청량을 수용하기 위해서는 pods가 15대 필요했으며, autoscaling 을 위해 설정한 KEDA의 CPU 60% 임계 증설 설정, 제한적인 테스트 내용을 감안한 여유있는 증설을 고려하면, 30개의 pods로 목표 트래픽 수용이 가능할 것으로 판단했습니다.\n개선 배포 후 pods 갯수 모니터링 결과, 개선 전 23대 수준에서 운영되던 것이 개선 후 14로 감소하였습니다.\n무진장 여름 블프에도 상시 14대 수준에서 운영이 가능했으며, 전체를 통틀어 최대 피크타임인 마지막날 오후 9시 부터 자정까지 30대 고정 증설하여 운영하였고, 30대를 초과하지 않고 cpu 사용수준을 증설 기준 미만으로 유지하며 안정적인 서버 상황 유지가 가능했습니다.\n개선된 전체 후기 목록 페이지\n배포 후 개선된 API의 평균 응답 속도를 확인해본 결과, (피크 요청량 기준) 아래와 같은 개선 수준을 확인할 수 있었으며,\n\n평균응답지연속도는 기존 300ms > 100ms로 1/3 으로 감소.\np50 250ms > 60ms로 감소\n\n특히 캐싱 구조상, 요청이 많으면 많을 수록 응답지연시간이 줄어드는 구조로, 특가등 요청이 많을 때 더 효과적인 성능을 보여줄 수 있었습니다.\n개선 전/후 지표 비교\n좋은 영향력\n후기 데이터 조회에서 호출하고 있는 여러가지 API들이 있었습니다.\n예를 들면, 상품 정보 조회 API, DB, Elasticsearch, Redis 정도가 있을 것 같네요.\n이 부분이 캐싱 처리 되면서 자연스럽게 호출이 줄어들었고, 지난 무진장에서는 scale-up을 고려했어야 했던 리소스들의 스펙을 유지하면서도 안정적인 서비스가 가능했습니다. 비용을 줄이는 효과를 얻기도 했습니다.\n개선 전/후 지표 비교\n나는 아직 배고프다\n무진장 직전, 일주일 정도의 짧은 시간을 들여 개선된 내용이라, 코드 품질과 유지보수성을 챙기지는 못했습니다. \n다행히도, 무신사 2.0 개편을 앞두고 있던 시점이라 무진장 여름 블프만을 위한 개선이었던 터라 단기적인 개선에 집중한 결과입니다.\n캐시를 이용하여 응답속도를 개선하긴 했으나 여전히 비즈니스 로직 자체에 비효율성이 존재한다는 느낌이 들었습니다. \n위 개선에서는 캐시 만료 시간을 고정적으로 하여 개선이 진행이 되었는데요. 사실 시간이 좀 더 여유가 있었다면, 캐시 데이터 셋과 캐시 만료를 고정 시간이 아닌, 적절한 만료 정책을 정해서 캐시를 좀 더 효율적으로 사용할 수 있었을텐데 하는 아쉬움이 남았습니다.\n맺음말\n개인적으로는, 무진장이라는 회사에서 가장 중요한 이벤트의 원활한 진행에 기여하여 매우 보람 있는 경험이었습니다. \n무진장 기간에는 모든 백엔드 개발자들이 본인이 담당한 서비스가 안정적으로 운영되고 있는지 예의주시하는 기간인데요. 저희 팀 모두가 장애 걱정을 덜고, 무진장 기간동안 긴장도를 줄여서 지낼 수 있게 되었습니다.\n많은 트래픽도 여유롭게 넘길 수 있게 된 평화로운 회원개발팀\n개선에서 제일 중요한 로직은 캐시 적용이었습니다. 당시 후기의 상황에서 적용해서 좋은 효과를 볼 수 있었는데요. 다만, 캐시 적용이 모든 걸 해결해주지는 않습니다. \n캐시는 빠른 만큼 비쌉니다. 빠른 성능을 유지하기 위해서는 꼭 필요한 곳에 잘 사용해야 합니다.\n마지막으로, 서비스 개선을 위해서는 내가 개발/운영하는 서비스에 지속적인 관심이 필요합니다. \n서비스의 개선은, 어느 날 갑자기 ‘개선 좀 해볼까?’ 마음먹는다고 해서 실행할 수 있는 일이 아닙니다. 계속해서 코드와 APM등의 모니터링 을 들여다보면서, 어떤 곳에서 비효율이 발생하는지 알아야 개선도 경험할 수 있습니다.\n자기의 성과는 자기가 스스로 만듭니다.\nMUSINSA CAREER\n함께할 동료를 찾습니다.\n무신사 회원개발팀은 1500만 명의 회원과 MAU 500만 명을 보유한 플랫폼의 핵심 서비스를 안정적으로 운영하고 있습니다. \n회원 관리, 멤버십, 인증, 메시지, 적립금 등의 서비스를 책임지며, 제품의 모든 요구사항을 신속하고 효율적으로 해결하여 안정성과 확장성을 보장합니다.\n전국민이 사용하는 1위 패션 플랫폼 무신사에서 기술로 비즈니스를 성장시키는 경험을 함께하고 싶으시다면 아래 채용 페이지를 통해 지원해 주세요!\n🚀 팀 무신사 채용 페이지 (무신사/29CM 전체 포지션 확인이 가능해요)\n🚀 팀 무신사 테크 소식을 받아보는 링크드인\n🚀 무신사 테크 블로그\n🚀 29CM 테크 블로그\n🚀 무신사 테크 유튜브 채널\n채용이 완료되면 공고가 닫힐 수 있으니 빠르게 지원해 주세요!\n\n무진장을 맞아, 후기 응답속도를 개선해보자 was originally published in MUSINSA tech on Medium, where people are continuing the conversation by highlighting and responding to this story.",
    "dc:creator": "Soyoun Kim",
    "guid": "https://medium.com/p/c92e0ae60f1e",
    "categories": [
      "refactoring",
      "cache",
      "backend",
      "improvement",
      "engineering"
    ],
    "isoDate": "2024-09-25T04:41:52.000Z"
  },
  {
    "creator": "Sangjin Lim",
    "title": "어떻게 3개월 만에 MUSINSA WMS를 개발할 수 있었을까?",
    "link": "https://medium.com/musinsa-tech/%EC%96%B4%EB%96%BB%EA%B2%8C-3%EA%B0%9C%EC%9B%94-%EB%A7%8C%EC%97%90-musinsa-wms%EB%A5%BC-%EA%B0%9C%EB%B0%9C%ED%95%A0-%EC%88%98-%EC%9E%88%EC%97%88%EC%9D%84%EA%B9%8C-993938419cde?source=rss----f107b03c406e---4",
    "pubDate": "Wed, 11 Sep 2024 03:45:13 GMT",
    "content:encoded": "<h3>들어가며</h3><p>안녕하세요, PBO(Platform Business Operation)에서 MUSINA WMS(이하 MWMS)를 개발하고 있는 백엔드 엔지니어 임상진입니다.</p><p>무신사는 빠른 배송과 반품, 오프라인과 글로벌 시장 확대를 위해 물류 시스템 개선에 몰두하고 있습니다.<br>과거 2023년까지 총 4개 물류센터에서 2가지의 상이한 외주 솔루션을 사용해 운영되고 있었습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Riplir7ca0oL9AQuT0pJ6g.jpeg\" /><figcaption>그림1. 3D 소터(자동 분류 설비)</figcaption></figure><p>물류센터를 운영하기 위해서는 WMS라는 시스템이 필수적인데요. 우리가 사용하던 외주 솔루션은 내재화된 시스템이 아니었기 때문에 시시각각 변경되는 요구사항에 즉각 대응하거나 비즈니스를 확장하기에 매우 불리했습니다. 특히 일부 기능은 MUSINSA OMS(이하 MOMS)에 개발되어야만 했습니다.</p><blockquote>잠깐, WMS와 OMS가 뭘까요?</blockquote><blockquote>WMS는 <strong>Warehouse Management System</strong>(창고 관리 시스템)의 약자로, 창고나 물류센터에서 재고 관리, 입출고, 작업 계획, 자동화 설비, 자재 배치 등을 효율적으로 관리하기 위해 사용하는 시스템입니다.</blockquote><blockquote>OMS는 <strong>Order Management System</strong>(주문 관리 시스템)의 약자로, 주문을 관리하고 처리하는 시스템입니다. 주로 커머스나 물류, 유통 등에서 사용되며, 제품 주문의 수집부터 출하, 배송, 재고 관리까지 모든 단계를 추적하고 최적화하는 역할을 합니다.</blockquote><p>따라서, 무신사는 빠르고 유연한 신규 비즈니스 확대를 목적으로 작년부터 MWMS 개발을 시작했고, 현재 무신사의 모든 first-mile 인프라를 MWMS로 전환했습니다.</p><h3>MWMS 개발 배경</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/990/1*_TNZmTx6B5i9Up-1SuSrHQ.png\" /><figcaption>그림2. MWMS 로고</figcaption></figure><p>MWMS 프로젝트는 구체적으로 아래 문제를 해결하기 위해 시작되었습니다.</p><p><strong>하나, 빠른 비즈니스 확장 불가.</strong></p><ul><li>외주 솔루션은 필요한 기능을 적시에 지원하지 못하고, 성능 문제로 작업 효율이 매우 떨어졌습니다.</li><li>신규 비즈니스나 자동화 설비 도입 시 개발 및 일정 조정에 시간이 오래 걸리며, 데이터 분석 및 의사결정에 필요한 정보를 적시에 제공받지 못했습니다.</li></ul><p><strong>둘, 비용 절감.</strong></p><ul><li>무신사의 자체 WMS가 없어 운영사를 중간에 거쳐야 했으나, 자체 시스템이 있다면 비용을 큰 폭으로 절감할 수 있습니다.</li></ul><h3>3개월의 남은 작업 일정</h3><p>신규 WMS 개발 프로젝트가 기획된 이후, 수개월간 개발을 진행했으나 담당 개발자의 잦은 업무 변경과 불명확한 작업공수 산정 등의 이유로 개발 완료 일정이 여러 차례 지연되었습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/344/1*3XAh-qLFRD5v5VH1PKRmyw.jpeg\" /></figure><p>어떤 이유로든 시간은 우리를 기다려주지 않습니다. 기존 운영사의 계약 만료 시점이 벌써 임박했습니다. 이에 따라 개발 완료일정 또한 단 3개월 앞으로 다가왔으나, 진척도는 20% 수준이었습니다.<br>급한 대로 개발자 채용을 시작했지만, 당장 필요한 인원을 빠르게 모으기는 어려웠기에 TF를 구성했고 저 또한 이때 투입되었습니다.</p><p>프로젝트의 성공을 위한 액션 아이템에 앞서, 팀에서 의지를 다지며 얘기했던 내용을 원칙으로 정리하면 아래와 같습니다.</p><blockquote><strong><em>1. 단기 목표 중심적으로 일하자.</em></strong></blockquote><blockquote><strong><em>2. 할 수 있는 만큼 해내자.</em></strong></blockquote><blockquote><strong><em>3. 오버 커뮤니케이션하자.</em></strong></blockquote><p>그리고 실천한 액션.</p><p>먼저 첫 번째 원칙에 따라 우리는 <strong>스프린트 방식을 도입</strong>하고, 매 <strong>스프린트 마지막 날에 데모를 진행</strong>했습니다. 이는 두 가지 이유가 있습니다.</p><p>첫 번째 이유, 일감의 크기 산정과 개발 속도를 더욱 정확하게 진단하고 우리가 프로젝트를 완수할 시점이 언제인지를 파악할 수 있습니다.</p><p>복잡한 프로젝트일수록 납기일을 정확하게 예상하는 것은 매우 어렵지만, MWMS의 경우는 (여러 이유로, 계약 관계 때문에) 이미 작업 완료 일정이 결정되었던 프로젝트로, 정말 우리가 프로젝트를 성공시킬 수 있을 것인지 반드시 가늠해야만 했습니다. 불가능하다면 하루라도 빨리 적색 신호등을 켜기 위해서요.<br>전체 일감의 크기를 재산정하고 두 번의 스프린트를 진행한 이후에는 모두가 할 수 있다는 자신감을 가질 수 있었습니다.</p><p>두 번째 이유, “동작 가능한 코드”를 보장하기 위해 스프린트 단위로 데모를 진행했습니다. 스프린트로 일감을 나누어 데모의 범위를 한정할 수 있었습니다.</p><p>이로써 스프린트 데모는 우리의 0순위 목표가 되었습니다. 입고부터 출고까지 사용자의 사용 흐름에 따라 스프린트 작업 우선순위를 결정했기 때문에, 이전 데모를 성공적으로 마무리하지 못한다면 다음 개발은 없었습니다. 따라서, 단 한 가지 문제에 집중할 수 있었습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/306/1*QQqzahZqLr4px6VCq9BMFQ.jpeg\" /></figure><p>두 번째 원칙에 따라 <strong>“무자비한 우선순위 정하기”</strong>를 실천했습니다.</p><p>저는 가장 중요한 프로젝트인 MWMS 외 다른 작업에 눈곱만큼도 관심을 두지 않았는데요. 업무 집중도를 높이기 위해 반드시 필요한 미팅만 참석하고, 이 프로젝트에 투입되기 전 8명의 조직을 이끌었던 저는 모든 의사결정과 책임을 팀원에게 위임했습니다. 결과적으로 제 빈자리는 동료들이 완벽하게 채워줬고, 저와 팀 모두 한 단계 성장하는 기회가 되었습니다. 이는 생각보다 큰 용기가 필요합니다. 조직이 흔들릴 것이라는 불안과 책임감을 떨쳐내야 합니다. 동료를 믿으세요.</p><p>마지막으로 세 번째 원칙을 위해 <strong>모든 논의를 하나의 프로젝트 채널에서 진행</strong>했습니다. 이로써 모든 진행 상황과 변경 사항은 단일 채널만으로 추적할 수 있게 되었습니다. 매일 각자의 작업 진행 상황을 공유하고, 요구사항에 대한 질문은 즉시 채널에 공유하고, 변경이 발생한 경우 인지가 필요한 멤버를 모두 멘션했습니다. 물류 도메인의 특성상 변경된 제약사항에 대해 모두가 인지해야만 전후 작업끼리 데이터 정합성이 흔들리지 않기 때문입니다. 상당한 비용이 따르는 업무 방식이었지만, 문제 해결에 집착하는 PM과 엔지니어들로 구성되었던 우리는 그야말로 완벽한 호흡을 보여줬습니다.</p><p>여담으로 저는 퇴근할 때마다 인사말 대신 <strong>“파이팅!”</strong>을 외치며 전의를 다졌습니다 🙂</p><h3>너무나도 생소한 물류 도메인</h3><p>또 다른 난관은 생전 처음 보는 물류 용어들과 씨름하는 것이었습니다. 물류 는 팀 내 엔지니어 모두가 처음 경험하는 분야였습니다.<br>ASN, DO, Wave 등의 생소한 물류 용어와 더불어 물류센터에서는 입고부터 출고까지 수많은 단계(그림3 참고)를 거칩니다.<br>각 단계 전후에 생성되는 데이터는 강하게 결합되어 있기 때문에 특정한 하나의 단계만 이해하고 코드를 작성하기 어려웠습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*LB4yUkLrDAqpGSoJmMVdUA.png\" /><figcaption>그림3. 입고, 출고 단계</figcaption></figure><p>특히 재고할당(그림4 참고)은 WMS 시스템에서 가장 비용이 큰 작업이며, 충분한 성능이 보장되지 않을 경우 현장에서 작업을 대기하는 불상사가 발생할 수 있기에 가장 고민이 많이 필요한 작업이었습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*W0vLcV8yCtw4agcHBWWjcQ.png\" /><figcaption>그림4. 상품별 재고할당 프로세스</figcaption></figure><p>이처럼 복잡한 요구사항은 오로지 텍스트만으로 정확한 사용 사례를 이해하기 어려워, 개발팀 모두 센터에 방문(그림5 참고)해서 전체적인 입출고 흐름을 직접 경험하며 현장 작업자의 요구 사항을 파악했습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*r3zjCzgxuKoCfK3VK8oIYw.jpeg\" /><figcaption>그림5. 현장 방문 (패킹 작업)</figcaption></figure><p>예를 들면, 바코드를 스캔하는 작업은 숙련자일수록 스캔 속도가 매우 빨랐는데, 단 수십 ms의 API 응답 지연이 발생하더라도 하루 수십, 수백만 건이 스캔되는 것을 가정하면 생산성이 크게 저하될 수 있음을 깨닫고 높은 응답 퍼포먼스를 보장하도록 구현했습니다.</p><h3>2주간의 통합테스트, 롤아웃, 그리고 성능 문제</h3><p>폭풍 같은 3개월간의 개발 후, 두 달 동안 총 800개 이상의 테스트 케이스에 대한 통합테스트를 진행했습니다.<br>무신사에서 가장 물동량이 많은 블랙프라이데이 기간의 실제 출고 데이터를 기반으로 대량 출고 스트레스 테스트까지 마쳤습니다.</p><p>통합테스트 기간 동안에는 최대한 작업 현장과 동일하게 테스트하기 위해 개발팀이 물류센터에 상주하면서 운영팀과 지속적으로 소통했습니다. 일부 PM과 엔지니어는 물류센터 근처에 숙소를 잡고 통근하면서 문제해결에 집중했습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/600/1*HrtVh_xOK81_ZKPIjefWIA.jpeg\" /></figure><p>첫 센터 롤아웃 이후 바로 성능 문제가 대두되었습니다.<br>재고할당, 피킹지시와 같이 크고 굵직한 기능은 수많은 테스트를 거쳐 자신 있었던 반면에, 데이터 조회 영역에서 여러 성능 문제가 발견되었습니다.<br>통합테스트에서는 사용할 데이터에만 집중했지, 사용하지 않는 (혹은 soft-delete된) 데이터가 합쳐지니 스캔 범위가 기하급수적으로 늘어났습니다. 물류 흐름에 따라서 재고 테이블에만 매일 수백만 개의 레코드가 삽입되기 때문이기도 합니다.<br>그래서, 롤아웃 첫 주는 필요한 인덱스를 생성하고 N+1 쿼리를 줄이는 패치, 불필요한 데이터를 분리 보관하는 작업을 빠르게 진행했습니다. 이 과정에서 Datadog Trace를 적극 활용했습니다. 데이터 멍멍이 만세!</p><h3>성과</h3><p>폭풍 같은 개발과 QA를 거쳐 엄청난 성과를 이뤄냈습니다.</p><ul><li>TF 구성 3개월 만에 WMS 개발을 완료했습니다.</li><li>가장 많은 물량을 점유하는 여주 1센터에 우선적으로 배포, 첫 사용 시 장애 한 번 없이 원활하게 런치했습니다. 이후 3개월 만에 나머지 3개 센터에 성공적으로 확장 배포했습니다.</li><li>작업지시 성능은 9배 향상, 피킹동선 최적화 알고리즘 적용으로 이동동선은 절반으로 단축했습니다.</li><li>출력된 종이를 사용한 기존 수기피킹을 PDA 바코드 스캔으로 전환해 오류 발생을 최소화했습니다.</li><li>데이터 파이프라인을 구성해 Databricks, Amazon QuickSight 등의 BI 도구를 사용한 빠른 데이터 기반 의사결정이 가능해졌습니다.</li></ul><h3>마치며</h3><p>혼자 할 수 있는 일은 생각보다 많지 않습니다. 그래서 큰 목표를 실현하기 위해서는 항상 팀으로 고민해야 합니다. 우리는 주어진 목표를 달성하기 위해 달성할 수 있는 목표인지 빠르게 검증했고, 그 가능성을 보았을 때 원칙을 세우고 모두가 한 마음으로 문제를 해결하기 시작했습니다. 그리고 불가능해 보였던 목표를 달성했습니다.</p><p>현재의 MWMS는 물류 입출고를 위한 기본 기능만을 지원하고 있습니다. 우리는 앞으로도 생산성 향상을 위해서 끊임없이 함께 고민하고 개선할 것입니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/400/1*ckxXubLaGttZVZBnWDIUXw.png\" /></figure><p>화려하지 않아도 팀의 승리를 위해 물불 안 가리고 뛰어드는 모습은 언제나 아름답습니다.</p><h3>Musinsa CAREER</h3><p>무신사는 문제 해결에 집착하는 분들을 기다리고 있습니다.</p><blockquote><strong><em>함께할 동료를 찾습니다.<br></em></strong><em>PBO(Platform Business Operation) 조직은 국내/외 물류 서비스, 재고관리, 매장 운영을 위한 제품을 구축하고 운영하는 물류 프로덕트와 다양한 오프라인 비즈니스 모델에서 고객경험을 향상시킬 수 있는 매장 관리 시스템을 개발하고 고도화합니다. 더불어 플러스 배송서비스를 비롯한 무신사의 차별화된 고객 경험을 브랜딩하고 더 많은 고객에게 제공될 수 있는 멤버쉽 구조를 설계합니다.</em></blockquote><blockquote><em>전국민이 사용하는 1위 패션 플랫폼 무신사에서 기술로 비즈니스를 성장시키는 경험을 함께하고 싶으시다면 아래 채용 페이지를 통해 지원해 주세요!</em></blockquote><blockquote><em>🚀 </em><a href=\"https://corp.musinsa.com/ko/career/\"><em>팀 무신사 채용 페이지</em></a><em> (무신사/29CM 전체 포지션 확인이 가능해요)</em></blockquote><blockquote><em>🚀 </em><a href=\"https://kr.linkedin.com/company/musinsacom\"><em>팀 무신사 테크 소식을 받아보는 링크드인</em></a></blockquote><blockquote><em>🚀 </em><a href=\"https://medium.com/musinsa-tech\"><em>무신사 테크 블로그</em></a></blockquote><blockquote><em>🚀 </em><a href=\"https://medium.com/29cm\"><em>29CM 테크 블로그</em></a></blockquote><blockquote><em>🚀 </em><a href=\"https://www.youtube.com/@MUSINSATECH\"><em>무신사 테크 유튜브 채널</em></a></blockquote><blockquote><em>채용이 완료되면 공고가 닫힐 수 있으니 빠르게 지원해 주세요!</em></blockquote><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=993938419cde\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/musinsa-tech/%EC%96%B4%EB%96%BB%EA%B2%8C-3%EA%B0%9C%EC%9B%94-%EB%A7%8C%EC%97%90-musinsa-wms%EB%A5%BC-%EA%B0%9C%EB%B0%9C%ED%95%A0-%EC%88%98-%EC%9E%88%EC%97%88%EC%9D%84%EA%B9%8C-993938419cde\">어떻게 3개월 만에 MUSINSA WMS를 개발할 수 있었을까?</a> was originally published in <a href=\"https://medium.com/musinsa-tech\">MUSINSA tech</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "content:encodedSnippet": "들어가며\n안녕하세요, PBO(Platform Business Operation)에서 MUSINA WMS(이하 MWMS)를 개발하고 있는 백엔드 엔지니어 임상진입니다.\n무신사는 빠른 배송과 반품, 오프라인과 글로벌 시장 확대를 위해 물류 시스템 개선에 몰두하고 있습니다.\n과거 2023년까지 총 4개 물류센터에서 2가지의 상이한 외주 솔루션을 사용해 운영되고 있었습니다.\n그림1. 3D 소터(자동 분류 설비)\n물류센터를 운영하기 위해서는 WMS라는 시스템이 필수적인데요. 우리가 사용하던 외주 솔루션은 내재화된 시스템이 아니었기 때문에 시시각각 변경되는 요구사항에 즉각 대응하거나 비즈니스를 확장하기에 매우 불리했습니다. 특히 일부 기능은 MUSINSA OMS(이하 MOMS)에 개발되어야만 했습니다.\n잠깐, WMS와 OMS가 뭘까요?\nWMS는 Warehouse Management System(창고 관리 시스템)의 약자로, 창고나 물류센터에서 재고 관리, 입출고, 작업 계획, 자동화 설비, 자재 배치 등을 효율적으로 관리하기 위해 사용하는 시스템입니다.\nOMS는 Order Management System(주문 관리 시스템)의 약자로, 주문을 관리하고 처리하는 시스템입니다. 주로 커머스나 물류, 유통 등에서 사용되며, 제품 주문의 수집부터 출하, 배송, 재고 관리까지 모든 단계를 추적하고 최적화하는 역할을 합니다.\n따라서, 무신사는 빠르고 유연한 신규 비즈니스 확대를 목적으로 작년부터 MWMS 개발을 시작했고, 현재 무신사의 모든 first-mile 인프라를 MWMS로 전환했습니다.\nMWMS 개발 배경\n그림2. MWMS 로고\nMWMS 프로젝트는 구체적으로 아래 문제를 해결하기 위해 시작되었습니다.\n하나, 빠른 비즈니스 확장 불가.\n\n외주 솔루션은 필요한 기능을 적시에 지원하지 못하고, 성능 문제로 작업 효율이 매우 떨어졌습니다.\n신규 비즈니스나 자동화 설비 도입 시 개발 및 일정 조정에 시간이 오래 걸리며, 데이터 분석 및 의사결정에 필요한 정보를 적시에 제공받지 못했습니다.\n\n둘, 비용 절감.\n\n무신사의 자체 WMS가 없어 운영사를 중간에 거쳐야 했으나, 자체 시스템이 있다면 비용을 큰 폭으로 절감할 수 있습니다.\n\n3개월의 남은 작업 일정\n신규 WMS 개발 프로젝트가 기획된 이후, 수개월간 개발을 진행했으나 담당 개발자의 잦은 업무 변경과 불명확한 작업공수 산정 등의 이유로 개발 완료 일정이 여러 차례 지연되었습니다.\n\n어떤 이유로든 시간은 우리를 기다려주지 않습니다. 기존 운영사의 계약 만료 시점이 벌써 임박했습니다. 이에 따라 개발 완료일정 또한 단 3개월 앞으로 다가왔으나, 진척도는 20% 수준이었습니다.\n급한 대로 개발자 채용을 시작했지만, 당장 필요한 인원을 빠르게 모으기는 어려웠기에 TF를 구성했고 저 또한 이때 투입되었습니다.\n프로젝트의 성공을 위한 액션 아이템에 앞서, 팀에서 의지를 다지며 얘기했던 내용을 원칙으로 정리하면 아래와 같습니다.\n1. 단기 목표 중심적으로 일하자.\n2. 할 수 있는 만큼 해내자.\n3. 오버 커뮤니케이션하자.\n그리고 실천한 액션.\n먼저 첫 번째 원칙에 따라 우리는 스프린트 방식을 도입하고, 매 스프린트 마지막 날에 데모를 진행했습니다. 이는 두 가지 이유가 있습니다.\n첫 번째 이유, 일감의 크기 산정과 개발 속도를 더욱 정확하게 진단하고 우리가 프로젝트를 완수할 시점이 언제인지를 파악할 수 있습니다.\n복잡한 프로젝트일수록 납기일을 정확하게 예상하는 것은 매우 어렵지만, MWMS의 경우는 (여러 이유로, 계약 관계 때문에) 이미 작업 완료 일정이 결정되었던 프로젝트로, 정말 우리가 프로젝트를 성공시킬 수 있을 것인지 반드시 가늠해야만 했습니다. 불가능하다면 하루라도 빨리 적색 신호등을 켜기 위해서요.\n전체 일감의 크기를 재산정하고 두 번의 스프린트를 진행한 이후에는 모두가 할 수 있다는 자신감을 가질 수 있었습니다.\n두 번째 이유, “동작 가능한 코드”를 보장하기 위해 스프린트 단위로 데모를 진행했습니다. 스프린트로 일감을 나누어 데모의 범위를 한정할 수 있었습니다.\n이로써 스프린트 데모는 우리의 0순위 목표가 되었습니다. 입고부터 출고까지 사용자의 사용 흐름에 따라 스프린트 작업 우선순위를 결정했기 때문에, 이전 데모를 성공적으로 마무리하지 못한다면 다음 개발은 없었습니다. 따라서, 단 한 가지 문제에 집중할 수 있었습니다.\n\n두 번째 원칙에 따라 “무자비한 우선순위 정하기”를 실천했습니다.\n저는 가장 중요한 프로젝트인 MWMS 외 다른 작업에 눈곱만큼도 관심을 두지 않았는데요. 업무 집중도를 높이기 위해 반드시 필요한 미팅만 참석하고, 이 프로젝트에 투입되기 전 8명의 조직을 이끌었던 저는 모든 의사결정과 책임을 팀원에게 위임했습니다. 결과적으로 제 빈자리는 동료들이 완벽하게 채워줬고, 저와 팀 모두 한 단계 성장하는 기회가 되었습니다. 이는 생각보다 큰 용기가 필요합니다. 조직이 흔들릴 것이라는 불안과 책임감을 떨쳐내야 합니다. 동료를 믿으세요.\n마지막으로 세 번째 원칙을 위해 모든 논의를 하나의 프로젝트 채널에서 진행했습니다. 이로써 모든 진행 상황과 변경 사항은 단일 채널만으로 추적할 수 있게 되었습니다. 매일 각자의 작업 진행 상황을 공유하고, 요구사항에 대한 질문은 즉시 채널에 공유하고, 변경이 발생한 경우 인지가 필요한 멤버를 모두 멘션했습니다. 물류 도메인의 특성상 변경된 제약사항에 대해 모두가 인지해야만 전후 작업끼리 데이터 정합성이 흔들리지 않기 때문입니다. 상당한 비용이 따르는 업무 방식이었지만, 문제 해결에 집착하는 PM과 엔지니어들로 구성되었던 우리는 그야말로 완벽한 호흡을 보여줬습니다.\n여담으로 저는 퇴근할 때마다 인사말 대신 “파이팅!”을 외치며 전의를 다졌습니다 🙂\n너무나도 생소한 물류 도메인\n또 다른 난관은 생전 처음 보는 물류 용어들과 씨름하는 것이었습니다. 물류 는 팀 내 엔지니어 모두가 처음 경험하는 분야였습니다.\nASN, DO, Wave 등의 생소한 물류 용어와 더불어 물류센터에서는 입고부터 출고까지 수많은 단계(그림3 참고)를 거칩니다.\n각 단계 전후에 생성되는 데이터는 강하게 결합되어 있기 때문에 특정한 하나의 단계만 이해하고 코드를 작성하기 어려웠습니다.\n그림3. 입고, 출고 단계\n특히 재고할당(그림4 참고)은 WMS 시스템에서 가장 비용이 큰 작업이며, 충분한 성능이 보장되지 않을 경우 현장에서 작업을 대기하는 불상사가 발생할 수 있기에 가장 고민이 많이 필요한 작업이었습니다.\n그림4. 상품별 재고할당 프로세스\n이처럼 복잡한 요구사항은 오로지 텍스트만으로 정확한 사용 사례를 이해하기 어려워, 개발팀 모두 센터에 방문(그림5 참고)해서 전체적인 입출고 흐름을 직접 경험하며 현장 작업자의 요구 사항을 파악했습니다.\n그림5. 현장 방문 (패킹 작업)\n예를 들면, 바코드를 스캔하는 작업은 숙련자일수록 스캔 속도가 매우 빨랐는데, 단 수십 ms의 API 응답 지연이 발생하더라도 하루 수십, 수백만 건이 스캔되는 것을 가정하면 생산성이 크게 저하될 수 있음을 깨닫고 높은 응답 퍼포먼스를 보장하도록 구현했습니다.\n2주간의 통합테스트, 롤아웃, 그리고 성능 문제\n폭풍 같은 3개월간의 개발 후, 두 달 동안 총 800개 이상의 테스트 케이스에 대한 통합테스트를 진행했습니다.\n무신사에서 가장 물동량이 많은 블랙프라이데이 기간의 실제 출고 데이터를 기반으로 대량 출고 스트레스 테스트까지 마쳤습니다.\n통합테스트 기간 동안에는 최대한 작업 현장과 동일하게 테스트하기 위해 개발팀이 물류센터에 상주하면서 운영팀과 지속적으로 소통했습니다. 일부 PM과 엔지니어는 물류센터 근처에 숙소를 잡고 통근하면서 문제해결에 집중했습니다.\n\n첫 센터 롤아웃 이후 바로 성능 문제가 대두되었습니다.\n재고할당, 피킹지시와 같이 크고 굵직한 기능은 수많은 테스트를 거쳐 자신 있었던 반면에, 데이터 조회 영역에서 여러 성능 문제가 발견되었습니다.\n통합테스트에서는 사용할 데이터에만 집중했지, 사용하지 않는 (혹은 soft-delete된) 데이터가 합쳐지니 스캔 범위가 기하급수적으로 늘어났습니다. 물류 흐름에 따라서 재고 테이블에만 매일 수백만 개의 레코드가 삽입되기 때문이기도 합니다.\n그래서, 롤아웃 첫 주는 필요한 인덱스를 생성하고 N+1 쿼리를 줄이는 패치, 불필요한 데이터를 분리 보관하는 작업을 빠르게 진행했습니다. 이 과정에서 Datadog Trace를 적극 활용했습니다. 데이터 멍멍이 만세!\n성과\n폭풍 같은 개발과 QA를 거쳐 엄청난 성과를 이뤄냈습니다.\n\nTF 구성 3개월 만에 WMS 개발을 완료했습니다.\n가장 많은 물량을 점유하는 여주 1센터에 우선적으로 배포, 첫 사용 시 장애 한 번 없이 원활하게 런치했습니다. 이후 3개월 만에 나머지 3개 센터에 성공적으로 확장 배포했습니다.\n작업지시 성능은 9배 향상, 피킹동선 최적화 알고리즘 적용으로 이동동선은 절반으로 단축했습니다.\n출력된 종이를 사용한 기존 수기피킹을 PDA 바코드 스캔으로 전환해 오류 발생을 최소화했습니다.\n데이터 파이프라인을 구성해 Databricks, Amazon QuickSight 등의 BI 도구를 사용한 빠른 데이터 기반 의사결정이 가능해졌습니다.\n\n마치며\n혼자 할 수 있는 일은 생각보다 많지 않습니다. 그래서 큰 목표를 실현하기 위해서는 항상 팀으로 고민해야 합니다. 우리는 주어진 목표를 달성하기 위해 달성할 수 있는 목표인지 빠르게 검증했고, 그 가능성을 보았을 때 원칙을 세우고 모두가 한 마음으로 문제를 해결하기 시작했습니다. 그리고 불가능해 보였던 목표를 달성했습니다.\n현재의 MWMS는 물류 입출고를 위한 기본 기능만을 지원하고 있습니다. 우리는 앞으로도 생산성 향상을 위해서 끊임없이 함께 고민하고 개선할 것입니다.\n\n화려하지 않아도 팀의 승리를 위해 물불 안 가리고 뛰어드는 모습은 언제나 아름답습니다.\nMusinsa CAREER\n무신사는 문제 해결에 집착하는 분들을 기다리고 있습니다.\n함께할 동료를 찾습니다.\nPBO(Platform Business Operation) 조직은 국내/외 물류 서비스, 재고관리, 매장 운영을 위한 제품을 구축하고 운영하는 물류 프로덕트와 다양한 오프라인 비즈니스 모델에서 고객경험을 향상시킬 수 있는 매장 관리 시스템을 개발하고 고도화합니다. 더불어 플러스 배송서비스를 비롯한 무신사의 차별화된 고객 경험을 브랜딩하고 더 많은 고객에게 제공될 수 있는 멤버쉽 구조를 설계합니다.\n전국민이 사용하는 1위 패션 플랫폼 무신사에서 기술로 비즈니스를 성장시키는 경험을 함께하고 싶으시다면 아래 채용 페이지를 통해 지원해 주세요!\n🚀 팀 무신사 채용 페이지 (무신사/29CM 전체 포지션 확인이 가능해요)\n🚀 팀 무신사 테크 소식을 받아보는 링크드인\n🚀 무신사 테크 블로그\n🚀 29CM 테크 블로그\n🚀 무신사 테크 유튜브 채널\n채용이 완료되면 공고가 닫힐 수 있으니 빠르게 지원해 주세요!\n\n어떻게 3개월 만에 MUSINSA WMS를 개발할 수 있었을까? was originally published in MUSINSA tech on Medium, where people are continuing the conversation by highlighting and responding to this story.",
    "dc:creator": "Sangjin Lim",
    "guid": "https://medium.com/p/993938419cde",
    "categories": [
      "fulfillment",
      "logistics",
      "product",
      "wms",
      "musinsa"
    ],
    "isoDate": "2024-09-11T03:45:13.000Z"
  },
  {
    "creator": "Jiwoong Han",
    "title": "무진장 블랙 프라이데이 가격 할인은 어떻게 할까?",
    "link": "https://medium.com/musinsa-tech/%EB%AC%B4%EC%A7%84%EC%9E%A5-%EB%B8%94%EB%9E%99-%ED%94%84%EB%9D%BC%EC%9D%B4%EB%8D%B0%EC%9D%B4-%EA%B0%80%EA%B2%A9-%ED%95%A0%EC%9D%B8%EC%9D%80-%EC%96%B4%EB%96%BB%EA%B2%8C-%ED%95%A0%EA%B9%8C-6f8517b5795a?source=rss----f107b03c406e---4",
    "pubDate": "Mon, 02 Sep 2024 00:40:29 GMT",
    "content:encoded": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*E4dAl3avDQECvy-Xhx1usg.png\" /></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*p6Aw8AHP92ldyOWmw3Bkgw.jpeg\" /><figcaption>사진출처: 무신사 뉴스룸</figcaption></figure><p>안녕하세요. 무신사에서 세일(할인)을 담당하고 있는 한지웅입니다. 이번 겨울 “무진장 블랙 프라이데이”(이하 블프)를 앞두고 2023년 겨울 시즌 거래액 3,000억 원을 달성했던 ‘무진장 블프’의 준비 과정을 공유하고자 합니다. 또한, 무신사의 가격 체계가 어떻게 구성되어 있으며, 40만 개에 달하는 상품에 세일(할인)을 어떻게 적용했었는지<strong> </strong>알려 드리려 합니다</p><p>2022년까지의 무진장 블프에서 무신사는 하나의 상품에 하나의 가격을 적용하는 1:1 가격 정책을 운영해왔습니다. 이 정책은 오랜 기간 동안 큰 문제 없이 서비스를 제공하는 데 기여했지만, 무신사가 빠르게 성장하면서 여러 가지 문제에 직면하게 되었습니다.</p><p>특히, 무진장 캠페인에 참여하는 브랜드 수가 증가함에 따라 세일 적용 상품도 지속적으로 늘어났고, 이로 인해 세일을 적용하는 데 많은 운영 리소스가 필요 했으며 시스템적으로도 한계에 부딪히게 되었습니다.</p><p>이러한 문제를 해결 하기 위해 문제 정의 부터 해결 과정 그리고 결과까지 이야기 하려 합니다.</p><p><strong>목차</strong></p><ol><li>문제 정의</li><li>운영 프로세스 개선</li><li>세일(할인) 구조 개선</li><li>파트너 참여(ing)</li><li>결과</li></ol><h3>문제 정의</h3><h4>기존 시스템에 한계와 문제점\u001e</h4><ul><li>10만 개의 상품의 가격을 변경하는데 1시간이나 소요됨</li><li>수수료 정책이 산발적으로 관리되고 있었고, 시스템에 연동되지 않았음</li><li>세일 상품을 취합하고 등록하는 데에 MD와 운영팀의 리소스가 많이 소모됨</li></ul><h3>해결 과정</h3><h4>운영 프로세스 개선</h4><p>먼저 오랫동안 사람의 손으로 진행되거나 입에서 입으로 전해지던 작업들을 문서화하고 업무 프로세스를 먼저 정립했습니다. 시스템을 구축한다는 것은 단순히 최신 기술을 사용하거나 프로그래밍을 잘한다고 해서 좋은 시스템이 나오는 것이 아니라 현재 어떤 문제가 있는지 그리고 사람이 처리하는 것보다 소프트웨어가 처리하는 것이 어디까지 더 효율적인지 판단해야 한다고 생각합니다.</p><p>이에 따라, 시스템을 새롭게 만들기 전에 다음과 같은 작업을 우선적으로 진행했습니다.</p><ol><li>할인(프로모션) 정책 문서화</li><li>팀마다 다른 엑셀 템플릿을 구글 스프레드시트로 통일</li><li>AppsScript를 이용해 수수료 계산, 데이터 불러오기, 유효성 검사 진행</li></ol><p>무신사는 복잡한 수수료 정책을 가지고 있으며, 제휴 및 할인율에 따라 할인 혜택도 다양하게 변경되고 있습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*UEKmDFkDvY8uHA-EGZjhWg.png\" /></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*SVBn3ZZTczbAVsSYdWdOyg.png\" /><figcaption>출처: 무신사앱 PDP 화면, 다양한 할인 혜택</figcaption></figure><p>MD들은 기획전이나 캠페인, 혹은 파트너사의 요청에 따라 상품 할인 설정을 할 수 있습니다. 이때 무신사의 할인 정책에 맞춰 설정을 해야 하는 번거로움이 있었습니다. 그 결과 경험이 많고 오래 다닌 직원은 정확하게 회사의 할인 정책에 맞춰 설정할 수 있었지만 신규 입사자나 회사의 할인 정책을 잘 모르는 직원은 더 많은 할인이나 적은 할인을 설정하게 되어 파트너사에 혜택을 덜 주거나 더 많이 주는 일이 빈번하게 발생했습니다.</p><p>이러한 근본적인 문제를 해결하기 위해 모든 할인 정책을 문서화하여 신규 입사자들이 언제든지 동일한 정책을 확인할 수 있도록 했습니다. 또한 세일 템플릿을 구글 스프레드시트로 만들어 버전 관리를 통해 항상 최신 상태로 유지했습니다.</p><p>구글 스프레드시트를 이용하면 AppsScript를 활용할 수 있는데 이를 통해 상품 번호와 가격만 입력하면 모든 계산이 자동으로 이루어지도록 구현해 업무의 효율을 극대화 하였습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*IqxMpgJKZflVu7Yx7YhL5g.png\" /><figcaption>Apps Script 를 이용한 자동화</figcaption></figure><blockquote>“<strong>무신사 할인 정책이 정리되고, MD들이 사용하는 템플릿도 마련되었으니, 새롭게시스템을 구축하기</strong>”</blockquote><h4>세일(할인) 구조 개선</h4><p>무신사의 상품 가격은 상품 하나당 하나의 가격만 존재했습니다. 이로 인해 기획전이나 캠페인을 진행할 때마다 매번 가격을 변경해야 했으며, 이 가격 변경 작업은 배치(batch) 작업으로 진행되었습니다. 기존 세일 적용 프로세스는 아무리 스케줄링이 되어 있어도 10분 간격으로 동작하는 배치의 특성상 즉시 적용이 어려웠으며, 노후화된 시스템으로 인해 상품 가격도 1분에 3,000개 이상 대량으로 수정하는 것이 불가능했습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*1yVyCpw9BujYlZJaqv36LQ.png\" /><figcaption>기존 세일 적용 프로세스</figcaption></figure><p>상품 가격은 다양한 방법으로 가격이 변경될 수 있었고 때문에 데이터가 오염되고 수수료가 잘못 적용되는 일이 빈번했습니다. 잘못된 수수료가 적용되어 판매가 이루어지면 정산 시마다 보정 작업을 해야 하는 번거로움이 있었습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*UWLG-6UwsYwbnqQbx_uK1A.png\" /></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*_gwgOxmHLhw9K5z8KjetEA.png\" /><figcaption>데이터 오염 시뮬레이션</figcaption></figure><p>배치(batch)로 인해 가격이 변경되면 종료시 이전 가격으로 환원하게 되는데 이때 중간에 가격이 변경하게 되면 데이터가 오염 되어 환원된 데이터를 못 찾는 경우가 발생 했습니다.</p><p>이러한 기존 시스템에는 다음과 같은 문제점이 있었습니다:</p><ol><li>배치 시스템의 특성상 다양한 경로로 직접 가격이 변경될 경우, 히스토리를 파악하고 가격을 복원하는 데 어려움이 있음</li><li>상품 테이블을 직접 업데이트해야 하므로 대량의 데이터를 수정할 때 DB 부하가 발생하여 다른 서비스의 지연이 발생</li><li>세일 적용 상품이 많아 물리적인 시간의 한계로 인해 가격 검수 불가능</li></ol><p>이런 문제점을 해결 하기 위해 세일 구조 개선 및 시스템 재구축을 결정 하였습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*JbHPVBa_nB1ISoZxCuakMw.png\" /></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*FGxnV8UBa5W_RnDA_kk5YQ.png\" /><figcaption>세일 관리자 페이지</figcaption></figure><p><strong>새로운 세일 시스템은 아래와 같은 특징을 가지고 있습니다.</strong></p><ol><li>상품과 가격 데이터 분리 하여 1:1구조에서 1:N으로 설정 가능</li><li><strong>실시간 가격</strong></li><li>날짜/재고별 가격 데이터 관리</li><li>편리한 UI/UX 툴 제공</li><li>대용량 트래픽 대응</li><li>수수료 및 할인 정책 자동 반영</li><li>이벤트 기반 아키텍처</li></ol><p>이번에는 이 중에서 <strong>실시간 가격</strong>에 대해 자세히 이야기해보려 합니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*wvyCqzTFFWWae1a0xqKxlw.png\" /></figure><p>REAL TIME Dynamic Pricing 이라고 거창하게 이름을 붙여보았습니다. 앞서 말씀드린 것처럼, 기존에는 무진장 블프와 같은 전사 캠페인을 진행할 때마다 상품 테이블에 가격을 계속 업데이트해야만 했습니다. 이 과정에서 <strong>“업데이트를 하지 않고 가격 할인을 할 수 있을까?” </strong>라는 고민을 많이 했고, 결과적으로 업데이트를 하지 않고 <strong><em>새로운 세일 정보를 등록</em></strong>만 하여 구현 하기로 결정했습니다.</p><p>무진장 블프 캠페인이 00:00에 시작한다고 가정하면 00:00에 맞춰 상품 가격을 모두 업데이트를 해야만 했습니다. 이 경우 가격이 한 번에 변경되어야 하기 때문에 마스터와 슬레이브 DB 서버 모두 부하를 겪게 되었습니다. (오픈과 동시에 유저 트래픽이 몰리는데 가격도 변경되기 때문)</p><p>하지만 바뀐 구조에서는 업데이트를 하지 않고 새로운 세일을 미리 등록을 할수 있기에 00:00 오픈 시간에 업데이트를 부하를 피할수 있었고 2주 전부터 미리 미리 입력 함으로 마스터 DB 로드를 분산시킬 수 있었습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*X2gAGb55rKlc3jyv6Ctfmg.png\" /><figcaption>세일 정보는 여러개가 존재</figcaption></figure><p>쉽게 이야기 하면 기존 상품 정보를 업데이트를 하는것이 아니라 매번 새로운 세일 설정(할인/수수료/적립금/제한 사항…. 등등)을 입력 하게 됩니다.</p><p>가격 API는 <strong>현재 시간</strong>에 맞춰 설정된 세일 정보 중 최저가를 찾아 응답합니다. 다시 말해, 가격을 매번 변경하는 대신 가격을 새로 설정하며, 여러 개의 세일 정보가 있을 경우 해당 기간 동안의 최저가를 찾고 가격이 같은 경우 최신 등록순으로 응답합니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/788/1*sDulJL8iWgXah68I-NM3XA.png\" /></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*aX9yIBdKrImxV4Lkkbjmqw.png\" /><figcaption>가격 레이어 정책</figcaption></figure><p>중복으로 등록된 세일 정보들은 가격 API 를 통해서 전달되며 <strong>현재 시간</strong>에 따라 가격이 변하게 됩니다. 이유는 세일(할인) 설정이 레이어되어 설정 되어 있더라도 최저가는 시간에 따라 변할 수 있기 때문입니다. 위에 그림 처럼 시간은 계속 흘러가고 가격은 계속 변경 될 겁니다.</p><p>이러한 구조로 덕분에 무진장 블프 캠페인이 시작 하는 00:00 에 모든 가격이 제시간에 전부 노출이 될수 있었고 빠른 응답 속도로 대량 트래픽도 받아 낼수 있게 되었습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*JeF8sPz_4rWOIl90\" /></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*omj34U60i_kQYcgjJ2V8pA.png\" /><figcaption>세일 아키텍처와 가격 API P99 응답</figcaption></figure><blockquote>“안정적인 세일 시스템이 구성 됐으니 파트너사가 직접 세일 설정을 할수 있게 만들기”</blockquote><h4>파트너 참여(ing)</h4><ul><li>기획전/캠페인 파트너 직접 참여 기능 제공</li><li>파트너 직접 세일 설정</li></ul><p>이제 세일 시스템은 한 단계 나아가 파트너(브랜드)에게도 기능을 제공 하기 위해 준비중에 있습니다.</p><h3>결과</h3><p>이런 개선 작업을 통해 평균 1,000여 개의 상품 할인이 있는 세일 시트를 운영팀에서 등록하는 데 소요되는 시간이 10분에서 1분으로 단축되어 <strong>1,000%</strong> 개선되었습니다. 또한 시스템의 세일 가격 적용 건수는 분당 3,000건에서 <strong>45,000건</strong>으로 증가했습니다. 업무 프로세스의 변화 역시 운영 효율화를 가져왔습니다. 캠페인 설정 후 세일 적용과 진열이 한 번에 처리되는 방식 덕분에 MD의 업무 요청과 운영팀의 업무가 <strong>50%</strong> 감소했으며, 전사 캠페인 시 발생하던 세일 적용 지연과 종료 지연 등의 문제도 근본적으로 해결되었습니다. 그 결과 수수료/가격 미반영, 환원 오류 발생률이<strong> 0건</strong>으로 줄어들었습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*E4dAl3avDQECvy-Xhx1usg.png\" /><figcaption>사진출처: 무신사 뉴스룸</figcaption></figure><p>이런한 노력으로 무진장 블프때 많은 상품을 할인 할 수 있었고 시스템 성능 개선과 근본적인 시스템 오류를 줄여 운영 효율을 극대화 하였습니다. 이제 무신사의 세일 시스템은 매우 안정적으로 운영 되고 있으며 다음에는 시스템 개발시 어떤 시행 착오를 겪고 문제를 해결 하였는지 이야기 하도록 하겠습니다. 지금까지 긴글 읽어주셔서 감사합니다.</p><h3>Musinsa CAREER</h3><blockquote><strong><em>함께할 동료를 찾습니다.<br></em></strong>세일개발팀은 무신사 전체 상품의 가격 할인과 쿠폰 서비스를 담당하고 있어, 서비스 곳곳에 걸쳐 다양한 영역에서 보여집니다. 또한, 할인된 가격이나 쿠폰을 사용하는 다른 팀과도 긴밀히 협력해야 하므로, 커뮤니케이션이 매우 중요한 조직입니다.</blockquote><blockquote><em>전국민이 사용하는 1위 패션 플랫폼 무신사에서 기술로 비즈니스를 성장시키는 경험을 함께하고 싶으시다면 아래 채용 페이지를 통해 지원해 주세요!</em></blockquote><blockquote><em>🚀 </em><a href=\"https://corp.musinsa.com/ko/career/\"><em>팀 무신사 채용 페이지</em></a><em> (무신사/29CM 전체 포지션 확인이 가능해요)</em></blockquote><blockquote><em>🚀 </em><a href=\"https://kr.linkedin.com/company/musinsacom\"><em>팀 무신사 테크 소식을 받아보는 링크드인</em></a></blockquote><blockquote><em>🚀 </em><a href=\"https://medium.com/musinsa-tech\"><em>무신사 테크 블로그</em></a></blockquote><blockquote><em>🚀 </em><a href=\"https://medium.com/29cm\"><em>29CM 테크 블로그</em></a></blockquote><blockquote><em>🚀 </em><a href=\"https://www.youtube.com/@MUSINSATECH\"><em>무신사 테크 유튜브 채널</em></a></blockquote><blockquote><em>채용이 완료되면 공고가 닫힐 수 있으니 빠르게 지원해 주세요!</em></blockquote><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*z3J4iwZeiJRoSARK-e8Mzw.png\" /></figure><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=6f8517b5795a\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/musinsa-tech/%EB%AC%B4%EC%A7%84%EC%9E%A5-%EB%B8%94%EB%9E%99-%ED%94%84%EB%9D%BC%EC%9D%B4%EB%8D%B0%EC%9D%B4-%EA%B0%80%EA%B2%A9-%ED%95%A0%EC%9D%B8%EC%9D%80-%EC%96%B4%EB%96%BB%EA%B2%8C-%ED%95%A0%EA%B9%8C-6f8517b5795a\">무진장 블랙 프라이데이 가격 할인은 어떻게 할까?</a> was originally published in <a href=\"https://medium.com/musinsa-tech\">MUSINSA tech</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "content:encodedSnippet": "사진출처: 무신사 뉴스룸\n안녕하세요. 무신사에서 세일(할인)을 담당하고 있는 한지웅입니다. 이번 겨울 “무진장 블랙 프라이데이”(이하 블프)를 앞두고 2023년 겨울 시즌 거래액 3,000억 원을 달성했던 ‘무진장 블프’의 준비 과정을 공유하고자 합니다. 또한, 무신사의 가격 체계가 어떻게 구성되어 있으며, 40만 개에 달하는 상품에 세일(할인)을 어떻게 적용했었는지 알려 드리려 합니다\n2022년까지의 무진장 블프에서 무신사는 하나의 상품에 하나의 가격을 적용하는 1:1 가격 정책을 운영해왔습니다. 이 정책은 오랜 기간 동안 큰 문제 없이 서비스를 제공하는 데 기여했지만, 무신사가 빠르게 성장하면서 여러 가지 문제에 직면하게 되었습니다.\n특히, 무진장 캠페인에 참여하는 브랜드 수가 증가함에 따라 세일 적용 상품도 지속적으로 늘어났고, 이로 인해 세일을 적용하는 데 많은 운영 리소스가 필요 했으며 시스템적으로도 한계에 부딪히게 되었습니다.\n이러한 문제를 해결 하기 위해 문제 정의 부터 해결 과정 그리고 결과까지 이야기 하려 합니다.\n목차\n\n문제 정의\n운영 프로세스 개선\n세일(할인) 구조 개선\n파트너 참여(ing)\n결과\n\n문제 정의\n기존 시스템에 한계와 문제점\u001e\n\n10만 개의 상품의 가격을 변경하는데 1시간이나 소요됨\n수수료 정책이 산발적으로 관리되고 있었고, 시스템에 연동되지 않았음\n세일 상품을 취합하고 등록하는 데에 MD와 운영팀의 리소스가 많이 소모됨\n\n해결 과정\n운영 프로세스 개선\n먼저 오랫동안 사람의 손으로 진행되거나 입에서 입으로 전해지던 작업들을 문서화하고 업무 프로세스를 먼저 정립했습니다. 시스템을 구축한다는 것은 단순히 최신 기술을 사용하거나 프로그래밍을 잘한다고 해서 좋은 시스템이 나오는 것이 아니라 현재 어떤 문제가 있는지 그리고 사람이 처리하는 것보다 소프트웨어가 처리하는 것이 어디까지 더 효율적인지 판단해야 한다고 생각합니다.\n이에 따라, 시스템을 새롭게 만들기 전에 다음과 같은 작업을 우선적으로 진행했습니다.\n\n할인(프로모션) 정책 문서화\n팀마다 다른 엑셀 템플릿을 구글 스프레드시트로 통일\nAppsScript를 이용해 수수료 계산, 데이터 불러오기, 유효성 검사 진행\n\n무신사는 복잡한 수수료 정책을 가지고 있으며, 제휴 및 할인율에 따라 할인 혜택도 다양하게 변경되고 있습니다.\n출처: 무신사앱 PDP 화면, 다양한 할인 혜택\nMD들은 기획전이나 캠페인, 혹은 파트너사의 요청에 따라 상품 할인 설정을 할 수 있습니다. 이때 무신사의 할인 정책에 맞춰 설정을 해야 하는 번거로움이 있었습니다. 그 결과 경험이 많고 오래 다닌 직원은 정확하게 회사의 할인 정책에 맞춰 설정할 수 있었지만 신규 입사자나 회사의 할인 정책을 잘 모르는 직원은 더 많은 할인이나 적은 할인을 설정하게 되어 파트너사에 혜택을 덜 주거나 더 많이 주는 일이 빈번하게 발생했습니다.\n이러한 근본적인 문제를 해결하기 위해 모든 할인 정책을 문서화하여 신규 입사자들이 언제든지 동일한 정책을 확인할 수 있도록 했습니다. 또한 세일 템플릿을 구글 스프레드시트로 만들어 버전 관리를 통해 항상 최신 상태로 유지했습니다.\n구글 스프레드시트를 이용하면 AppsScript를 활용할 수 있는데 이를 통해 상품 번호와 가격만 입력하면 모든 계산이 자동으로 이루어지도록 구현해 업무의 효율을 극대화 하였습니다.\nApps Script 를 이용한 자동화\n“무신사 할인 정책이 정리되고, MD들이 사용하는 템플릿도 마련되었으니, 새롭게시스템을 구축하기”\n세일(할인) 구조 개선\n무신사의 상품 가격은 상품 하나당 하나의 가격만 존재했습니다. 이로 인해 기획전이나 캠페인을 진행할 때마다 매번 가격을 변경해야 했으며, 이 가격 변경 작업은 배치(batch) 작업으로 진행되었습니다. 기존 세일 적용 프로세스는 아무리 스케줄링이 되어 있어도 10분 간격으로 동작하는 배치의 특성상 즉시 적용이 어려웠으며, 노후화된 시스템으로 인해 상품 가격도 1분에 3,000개 이상 대량으로 수정하는 것이 불가능했습니다.\n기존 세일 적용 프로세스\n상품 가격은 다양한 방법으로 가격이 변경될 수 있었고 때문에 데이터가 오염되고 수수료가 잘못 적용되는 일이 빈번했습니다. 잘못된 수수료가 적용되어 판매가 이루어지면 정산 시마다 보정 작업을 해야 하는 번거로움이 있었습니다.\n데이터 오염 시뮬레이션\n배치(batch)로 인해 가격이 변경되면 종료시 이전 가격으로 환원하게 되는데 이때 중간에 가격이 변경하게 되면 데이터가 오염 되어 환원된 데이터를 못 찾는 경우가 발생 했습니다.\n이러한 기존 시스템에는 다음과 같은 문제점이 있었습니다:\n\n배치 시스템의 특성상 다양한 경로로 직접 가격이 변경될 경우, 히스토리를 파악하고 가격을 복원하는 데 어려움이 있음\n상품 테이블을 직접 업데이트해야 하므로 대량의 데이터를 수정할 때 DB 부하가 발생하여 다른 서비스의 지연이 발생\n세일 적용 상품이 많아 물리적인 시간의 한계로 인해 가격 검수 불가능\n\n이런 문제점을 해결 하기 위해 세일 구조 개선 및 시스템 재구축을 결정 하였습니다.\n세일 관리자 페이지\n새로운 세일 시스템은 아래와 같은 특징을 가지고 있습니다.\n\n상품과 가격 데이터 분리 하여 1:1구조에서 1:N으로 설정 가능\n실시간 가격\n날짜/재고별 가격 데이터 관리\n편리한 UI/UX 툴 제공\n대용량 트래픽 대응\n수수료 및 할인 정책 자동 반영\n이벤트 기반 아키텍처\n\n이번에는 이 중에서 실시간 가격에 대해 자세히 이야기해보려 합니다.\n\nREAL TIME Dynamic Pricing 이라고 거창하게 이름을 붙여보았습니다. 앞서 말씀드린 것처럼, 기존에는 무진장 블프와 같은 전사 캠페인을 진행할 때마다 상품 테이블에 가격을 계속 업데이트해야만 했습니다. 이 과정에서 “업데이트를 하지 않고 가격 할인을 할 수 있을까?” 라는 고민을 많이 했고, 결과적으로 업데이트를 하지 않고 새로운 세일 정보를 등록만 하여 구현 하기로 결정했습니다.\n무진장 블프 캠페인이 00:00에 시작한다고 가정하면 00:00에 맞춰 상품 가격을 모두 업데이트를 해야만 했습니다. 이 경우 가격이 한 번에 변경되어야 하기 때문에 마스터와 슬레이브 DB 서버 모두 부하를 겪게 되었습니다. (오픈과 동시에 유저 트래픽이 몰리는데 가격도 변경되기 때문)\n하지만 바뀐 구조에서는 업데이트를 하지 않고 새로운 세일을 미리 등록을 할수 있기에 00:00 오픈 시간에 업데이트를 부하를 피할수 있었고 2주 전부터 미리 미리 입력 함으로 마스터 DB 로드를 분산시킬 수 있었습니다.\n세일 정보는 여러개가 존재\n쉽게 이야기 하면 기존 상품 정보를 업데이트를 하는것이 아니라 매번 새로운 세일 설정(할인/수수료/적립금/제한 사항…. 등등)을 입력 하게 됩니다.\n가격 API는 현재 시간에 맞춰 설정된 세일 정보 중 최저가를 찾아 응답합니다. 다시 말해, 가격을 매번 변경하는 대신 가격을 새로 설정하며, 여러 개의 세일 정보가 있을 경우 해당 기간 동안의 최저가를 찾고 가격이 같은 경우 최신 등록순으로 응답합니다.\n가격 레이어 정책\n중복으로 등록된 세일 정보들은 가격 API 를 통해서 전달되며 현재 시간에 따라 가격이 변하게 됩니다. 이유는 세일(할인) 설정이 레이어되어 설정 되어 있더라도 최저가는 시간에 따라 변할 수 있기 때문입니다. 위에 그림 처럼 시간은 계속 흘러가고 가격은 계속 변경 될 겁니다.\n이러한 구조로 덕분에 무진장 블프 캠페인이 시작 하는 00:00 에 모든 가격이 제시간에 전부 노출이 될수 있었고 빠른 응답 속도로 대량 트래픽도 받아 낼수 있게 되었습니다.\n세일 아키텍처와 가격 API P99 응답\n“안정적인 세일 시스템이 구성 됐으니 파트너사가 직접 세일 설정을 할수 있게 만들기”\n파트너 참여(ing)\n\n기획전/캠페인 파트너 직접 참여 기능 제공\n파트너 직접 세일 설정\n\n이제 세일 시스템은 한 단계 나아가 파트너(브랜드)에게도 기능을 제공 하기 위해 준비중에 있습니다.\n결과\n이런 개선 작업을 통해 평균 1,000여 개의 상품 할인이 있는 세일 시트를 운영팀에서 등록하는 데 소요되는 시간이 10분에서 1분으로 단축되어 1,000% 개선되었습니다. 또한 시스템의 세일 가격 적용 건수는 분당 3,000건에서 45,000건으로 증가했습니다. 업무 프로세스의 변화 역시 운영 효율화를 가져왔습니다. 캠페인 설정 후 세일 적용과 진열이 한 번에 처리되는 방식 덕분에 MD의 업무 요청과 운영팀의 업무가 50% 감소했으며, 전사 캠페인 시 발생하던 세일 적용 지연과 종료 지연 등의 문제도 근본적으로 해결되었습니다. 그 결과 수수료/가격 미반영, 환원 오류 발생률이 0건으로 줄어들었습니다.\n사진출처: 무신사 뉴스룸\n이런한 노력으로 무진장 블프때 많은 상품을 할인 할 수 있었고 시스템 성능 개선과 근본적인 시스템 오류를 줄여 운영 효율을 극대화 하였습니다. 이제 무신사의 세일 시스템은 매우 안정적으로 운영 되고 있으며 다음에는 시스템 개발시 어떤 시행 착오를 겪고 문제를 해결 하였는지 이야기 하도록 하겠습니다. 지금까지 긴글 읽어주셔서 감사합니다.\nMusinsa CAREER\n함께할 동료를 찾습니다.\n세일개발팀은 무신사 전체 상품의 가격 할인과 쿠폰 서비스를 담당하고 있어, 서비스 곳곳에 걸쳐 다양한 영역에서 보여집니다. 또한, 할인된 가격이나 쿠폰을 사용하는 다른 팀과도 긴밀히 협력해야 하므로, 커뮤니케이션이 매우 중요한 조직입니다.\n전국민이 사용하는 1위 패션 플랫폼 무신사에서 기술로 비즈니스를 성장시키는 경험을 함께하고 싶으시다면 아래 채용 페이지를 통해 지원해 주세요!\n🚀 팀 무신사 채용 페이지 (무신사/29CM 전체 포지션 확인이 가능해요)\n🚀 팀 무신사 테크 소식을 받아보는 링크드인\n🚀 무신사 테크 블로그\n🚀 29CM 테크 블로그\n🚀 무신사 테크 유튜브 채널\n채용이 완료되면 공고가 닫힐 수 있으니 빠르게 지원해 주세요!\n\n무진장 블랙 프라이데이 가격 할인은 어떻게 할까? was originally published in MUSINSA tech on Medium, where people are continuing the conversation by highlighting and responding to this story.",
    "dc:creator": "Jiwoong Han",
    "guid": "https://medium.com/p/6f8517b5795a",
    "categories": [
      "backend",
      "real-time-analytics",
      "api",
      "musinsa",
      "tech"
    ],
    "isoDate": "2024-09-02T00:40:29.000Z"
  }
]